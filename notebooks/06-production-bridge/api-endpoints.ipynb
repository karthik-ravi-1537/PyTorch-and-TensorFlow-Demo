{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Endpoints: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Create REST API endpoints for model serving\n",
    "- Compare Flask and FastAPI integration patterns\n",
    "- Learn request/response handling and validation\n",
    "- Understand error management and monitoring\n",
    "\n",
    "**Prerequisites:** Model serialization, inference patterns\n",
    "\n",
    "**Estimated Time:** 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import logging\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from foundations.data_utils import get_tutorial_tabular_data\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import web frameworks\n",
    "try:\n",
    "    from flask import Flask, request, jsonify\n",
    "    FLASK_AVAILABLE = True\n",
    "    print(\"‚úÖ Flask available\")\n",
    "except ImportError:\n",
    "    FLASK_AVAILABLE = False\n",
    "    print(\"‚ùå Flask not available (install with: pip install flask)\")\n",
    "\n",
    "try:\n",
    "    from fastapi import FastAPI, HTTPException\n",
    "    from pydantic import BaseModel, validator\n",
    "    import uvicorn\n",
    "    FASTAPI_AVAILABLE = True\n",
    "    print(\"‚úÖ FastAPI available\")\n",
    "except ImportError:\n",
    "    FASTAPI_AVAILABLE = False\n",
    "    print(\"‚ùå FastAPI not available (install with: pip install fastapi uvicorn pydantic)\")\n",
    "\n",
    "# Try to import ML frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get sample data for API testing\n",
    "data = get_tutorial_tabular_data()\n",
    "X_test = data['X_test']\n",
    "input_dim = X_test.shape[1]\n",
    "num_classes = len(np.unique(data['y_test']))\n",
    "\n",
    "print(f\"\\nData info: {input_dim} features, {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Flask API with PyTorch\n",
    "\n",
    "Creating a simple REST API using Flask and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FLASK API WITH PYTORCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if FLASK_AVAILABLE and PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Flask API Implementation:\")\n",
    "    \n",
    "    # Define model architecture\n",
    "    class SimpleClassifier(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "            self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "            self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    # Model service class\n",
    "    class PyTorchModelService:\n",
    "        def __init__(self, model_path=None, device='cpu'):\n",
    "            self.device = torch.device(device)\n",
    "            self.model = self._load_model(model_path)\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Statistics tracking\n",
    "            self.request_count = 0\n",
    "            self.total_inference_time = 0.0\n",
    "            self.lock = threading.Lock()\n",
    "        \n",
    "        def _load_model(self, model_path):\n",
    "            \"\"\"Load model (create dummy for demo)\"\"\"\n",
    "            model = SimpleClassifier(input_dim, 64, num_classes).to(self.device)\n",
    "            # In practice: model.load_state_dict(torch.load(model_path))\n",
    "            return model\n",
    "        \n",
    "        def predict(self, features: List[float]) -> Dict[str, Any]:\n",
    "            \"\"\"Make prediction with error handling\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Validate input\n",
    "                if len(features) != input_dim:\n",
    "                    raise ValueError(f\"Expected {input_dim} features, got {len(features)}\")\n",
    "                \n",
    "                # Convert to tensor\n",
    "                input_tensor = torch.FloatTensor([features]).to(self.device)\n",
    "                \n",
    "                # Inference\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                # Update statistics\n",
    "                with self.lock:\n",
    "                    self.request_count += 1\n",
    "                    self.total_inference_time += inference_time\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'predicted_class': int(predicted_class.item()),\n",
    "                    'probabilities': probabilities.cpu().numpy()[0].tolist(),\n",
    "                    'confidence': float(torch.max(probabilities).item()),\n",
    "                    'inference_time_ms': inference_time * 1000,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Prediction error: {str(e)}\")\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "        \n",
    "        def get_stats(self) -> Dict[str, Any]:\n",
    "            \"\"\"Get service statistics\"\"\"\n",
    "            with self.lock:\n",
    "                avg_time = (self.total_inference_time / self.request_count \n",
    "                           if self.request_count > 0 else 0)\n",
    "                \n",
    "                return {\n",
    "                    'total_requests': self.request_count,\n",
    "                    'average_inference_time_ms': avg_time * 1000,\n",
    "                    'model_device': str(self.device),\n",
    "                    'model_parameters': sum(p.numel() for p in self.model.parameters())\n",
    "                }\n",
    "    \n",
    "    # Create Flask app\n",
    "    def create_pytorch_flask_app():\n",
    "        app = Flask(__name__)\n",
    "        model_service = PyTorchModelService()\n",
    "        \n",
    "        @app.route('/health', methods=['GET'])\n",
    "        def health_check():\n",
    "            \"\"\"Health check endpoint\"\"\"\n",
    "            return jsonify({\n",
    "                'status': 'healthy',\n",
    "                'framework': 'pytorch',\n",
    "                'version': torch.__version__,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        @app.route('/predict', methods=['POST'])\n",
    "        def predict():\n",
    "            \"\"\"Prediction endpoint\"\"\"\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                \n",
    "                if not data or 'features' not in data:\n",
    "                    return jsonify({\n",
    "                        'success': False,\n",
    "                        'error': 'Missing features in request body'\n",
    "                    }), 400\n",
    "                \n",
    "                features = data['features']\n",
    "                result = model_service.predict(features)\n",
    "                \n",
    "                status_code = 200 if result['success'] else 400\n",
    "                return jsonify(result), status_code\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Request processing error: {str(e)}\")\n",
    "                return jsonify({\n",
    "                    'success': False,\n",
    "                    'error': 'Internal server error'\n",
    "                }), 500\n",
    "        \n",
    "        @app.route('/batch_predict', methods=['POST'])\n",
    "        def batch_predict():\n",
    "            \"\"\"Batch prediction endpoint\"\"\"\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                \n",
    "                if not data or 'samples' not in data:\n",
    "                    return jsonify({\n",
    "                        'success': False,\n",
    "                        'error': 'Missing samples in request body'\n",
    "                    }), 400\n",
    "                \n",
    "                samples = data['samples']\n",
    "                results = []\n",
    "                \n",
    "                for i, features in enumerate(samples):\n",
    "                    result = model_service.predict(features)\n",
    "                    result['sample_id'] = i\n",
    "                    results.append(result)\n",
    "                \n",
    "                return jsonify({\n",
    "                    'success': True,\n",
    "                    'results': results,\n",
    "                    'total_samples': len(results)\n",
    "                })\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch processing error: {str(e)}\")\n",
    "                return jsonify({\n",
    "                    'success': False,\n",
    "                    'error': 'Internal server error'\n",
    "                }), 500\n",
    "        \n",
    "        @app.route('/stats', methods=['GET'])\n",
    "        def get_stats():\n",
    "            \"\"\"Statistics endpoint\"\"\"\n",
    "            return jsonify(model_service.get_stats())\n",
    "        \n",
    "        return app\n",
    "    \n",
    "    # Create app instance\n",
    "    pytorch_app = create_pytorch_flask_app()\n",
    "    \n",
    "    print(\"‚úÖ PyTorch Flask API created\")\n",
    "    print(\"Available endpoints:\")\n",
    "    print(\"  GET  /health - Health check\")\n",
    "    print(\"  POST /predict - Single prediction\")\n",
    "    print(\"  POST /batch_predict - Batch predictions\")\n",
    "    print(\"  GET  /stats - Service statistics\")\n",
    "    \n",
    "    # Test the API endpoints (simulate requests)\n",
    "    print(\"\\nüß™ Testing API endpoints:\")\n",
    "    \n",
    "    # Simulate health check\n",
    "    with pytorch_app.test_client() as client:\n",
    "        # Health check\n",
    "        response = client.get('/health')\n",
    "        print(f\"Health check: {response.status_code} - {response.get_json()['status']}\")\n",
    "        \n",
    "        # Single prediction\n",
    "        test_features = X_test[0].tolist()\n",
    "        response = client.post('/predict', \n",
    "                             json={'features': test_features},\n",
    "                             content_type='application/json')\n",
    "        result = response.get_json()\n",
    "        print(f\"Single prediction: {response.status_code} - Class {result.get('predicted_class')} \"\n",
    "              f\"(confidence: {result.get('confidence', 0):.3f})\")\n",
    "        \n",
    "        # Batch prediction\n",
    "        batch_features = [X_test[i].tolist() for i in range(3)]\n",
    "        response = client.post('/batch_predict',\n",
    "                             json={'samples': batch_features},\n",
    "                             content_type='application/json')\n",
    "        batch_result = response.get_json()\n",
    "        print(f\"Batch prediction: {response.status_code} - {batch_result.get('total_samples')} samples processed\")\n",
    "        \n",
    "        # Statistics\n",
    "        response = client.get('/stats')\n",
    "        stats = response.get_json()\n",
    "        print(f\"Statistics: {stats['total_requests']} requests, \"\n",
    "              f\"avg time: {stats['average_inference_time_ms']:.2f}ms\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Flask or PyTorch not available - skipping Flask API demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FastAPI with TensorFlow\n",
    "\n",
    "Creating a modern async API using FastAPI and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FASTAPI WITH TENSORFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if FASTAPI_AVAILABLE and TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow FastAPI Implementation:\")\n",
    "    \n",
    "    # Pydantic models for request/response validation\n",
    "    class PredictionRequest(BaseModel):\n",
    "        features: List[float]\n",
    "        \n",
    "        @validator('features')\n",
    "        def validate_features(cls, v):\n",
    "            if len(v) != input_dim:\n",
    "                raise ValueError(f'Expected {input_dim} features, got {len(v)}')\n",
    "            return v\n",
    "    \n",
    "    class BatchPredictionRequest(BaseModel):\n",
    "        samples: List[List[float]]\n",
    "        \n",
    "        @validator('samples')\n",
    "        def validate_samples(cls, v):\n",
    "            for i, sample in enumerate(v):\n",
    "                if len(sample) != input_dim:\n",
    "                    raise ValueError(f'Sample {i}: expected {input_dim} features, got {len(sample)}')\n",
    "            return v\n",
    "    \n",
    "    class PredictionResponse(BaseModel):\n",
    "        success: bool\n",
    "        predicted_class: Optional[int] = None\n",
    "        probabilities: Optional[List[float]] = None\n",
    "        confidence: Optional[float] = None\n",
    "        inference_time_ms: Optional[float] = None\n",
    "        timestamp: str\n",
    "        error: Optional[str] = None\n",
    "    \n",
    "    class BatchPredictionResponse(BaseModel):\n",
    "        success: bool\n",
    "        results: Optional[List[PredictionResponse]] = None\n",
    "        total_samples: Optional[int] = None\n",
    "        error: Optional[str] = None\n",
    "    \n",
    "    class HealthResponse(BaseModel):\n",
    "        status: str\n",
    "        framework: str\n",
    "        version: str\n",
    "        timestamp: str\n",
    "    \n",
    "    class StatsResponse(BaseModel):\n",
    "        total_requests: int\n",
    "        average_inference_time_ms: float\n",
    "        model_parameters: int\n",
    "        gpu_available: bool\n",
    "    \n",
    "    # Model service class\n",
    "    class TensorFlowModelService:\n",
    "        def __init__(self, model_path=None):\n",
    "            self.model = self._load_model(model_path)\n",
    "            \n",
    "            # Create optimized inference function\n",
    "            self.inference_func = tf.function(self.model.call)\n",
    "            \n",
    "            # Statistics tracking\n",
    "            self.request_count = 0\n",
    "            self.total_inference_time = 0.0\n",
    "            self.lock = threading.Lock()\n",
    "        \n",
    "        def _load_model(self, model_path):\n",
    "            \"\"\"Load model (create dummy for demo)\"\"\"\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "            # In practice: model = tf.keras.models.load_model(model_path)\n",
    "            return model\n",
    "        \n",
    "        async def predict(self, features: List[float]) -> PredictionResponse:\n",
    "            \"\"\"Async prediction with error handling\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Convert to tensor\n",
    "                input_tensor = tf.constant([features], dtype=tf.float32)\n",
    "                \n",
    "                # Inference\n",
    "                probabilities = self.inference_func(input_tensor, training=False)\n",
    "                predicted_class = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                # Update statistics\n",
    "                with self.lock:\n",
    "                    self.request_count += 1\n",
    "                    self.total_inference_time += inference_time\n",
    "                \n",
    "                return PredictionResponse(\n",
    "                    success=True,\n",
    "                    predicted_class=int(predicted_class.numpy()[0]),\n",
    "                    probabilities=probabilities.numpy()[0].tolist(),\n",
    "                    confidence=float(tf.reduce_max(probabilities).numpy()),\n",
    "                    inference_time_ms=inference_time * 1000,\n",
    "                    timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Prediction error: {str(e)}\")\n",
    "                return PredictionResponse(\n",
    "                    success=False,\n",
    "                    error=str(e),\n",
    "                    timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "        \n",
    "        async def batch_predict(self, samples: List[List[float]]) -> BatchPredictionResponse:\n",
    "            \"\"\"Async batch prediction\"\"\"\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Convert to tensor\n",
    "                input_tensor = tf.constant(samples, dtype=tf.float32)\n",
    "                \n",
    "                # Batch inference\n",
    "                probabilities = self.inference_func(input_tensor, training=False)\n",
    "                predicted_classes = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                # Update statistics\n",
    "                with self.lock:\n",
    "                    self.request_count += len(samples)\n",
    "                    self.total_inference_time += batch_time\n",
    "                \n",
    "                # Create individual results\n",
    "                results = []\n",
    "                for i in range(len(samples)):\n",
    "                    results.append(PredictionResponse(\n",
    "                        success=True,\n",
    "                        predicted_class=int(predicted_classes[i].numpy()),\n",
    "                        probabilities=probabilities[i].numpy().tolist(),\n",
    "                        confidence=float(probabilities[i].numpy().max()),\n",
    "                        inference_time_ms=(batch_time / len(samples)) * 1000,\n",
    "                        timestamp=datetime.now().isoformat()\n",
    "                    ))\n",
    "                \n",
    "                return BatchPredictionResponse(\n",
    "                    success=True,\n",
    "                    results=results,\n",
    "                    total_samples=len(results)\n",
    "                )\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch prediction error: {str(e)}\")\n",
    "                return BatchPredictionResponse(\n",
    "                    success=False,\n",
    "                    error=str(e)\n",
    "                )\n",
    "        \n",
    "        def get_stats(self) -> StatsResponse:\n",
    "            \"\"\"Get service statistics\"\"\"\n",
    "            with self.lock:\n",
    "                avg_time = (self.total_inference_time / self.request_count \n",
    "                           if self.request_count > 0 else 0)\n",
    "                \n",
    "                return StatsResponse(\n",
    "                    total_requests=self.request_count,\n",
    "                    average_inference_time_ms=avg_time * 1000,\n",
    "                    model_parameters=self.model.count_params(),\n",
    "                    gpu_available=len(tf.config.list_physical_devices('GPU')) > 0\n",
    "                )\n",
    "    \n",
    "    # Create FastAPI app\n",
    "    def create_tensorflow_fastapi_app():\n",
    "        app = FastAPI(\n",
    "            title=\"TensorFlow Model API\",\n",
    "            description=\"REST API for TensorFlow model inference\",\n",
    "            version=\"1.0.0\"\n",
    "        )\n",
    "        \n",
    "        model_service = TensorFlowModelService()\n",
    "        \n",
    "        @app.get(\"/health\", response_model=HealthResponse)\n",
    "        async def health_check():\n",
    "            \"\"\"Health check endpoint\"\"\"\n",
    "            return HealthResponse(\n",
    "                status=\"healthy\",\n",
    "                framework=\"tensorflow\",\n",
    "                version=tf.__version__,\n",
    "                timestamp=datetime.now().isoformat()\n",
    "            )\n",
    "        \n",
    "        @app.post(\"/predict\", response_model=PredictionResponse)\n",
    "        async def predict(request: PredictionRequest):\n",
    "            \"\"\"Single prediction endpoint\"\"\"\n",
    "            return await model_service.predict(request.features)\n",
    "        \n",
    "        @app.post(\"/batch_predict\", response_model=BatchPredictionResponse)\n",
    "        async def batch_predict(request: BatchPredictionRequest):\n",
    "            \"\"\"Batch prediction endpoint\"\"\"\n",
    "            return await model_service.batch_predict(request.samples)\n",
    "        \n",
    "        @app.get(\"/stats\", response_model=StatsResponse)\n",
    "        async def get_stats():\n",
    "            \"\"\"Statistics endpoint\"\"\"\n",
    "            return model_service.get_stats()\n",
    "        \n",
    "        return app\n",
    "    \n",
    "    # Create app instance\n",
    "    tensorflow_app = create_tensorflow_fastapi_app()\n",
    "    \n",
    "    print(\"‚úÖ TensorFlow FastAPI created\")\n",
    "    print(\"Available endpoints:\")\n",
    "    print(\"  GET  /health - Health check\")\n",
    "    print(\"  POST /predict - Single prediction\")\n",
    "    print(\"  POST /batch_predict - Batch predictions\")\n",
    "    print(\"  GET  /stats - Service statistics\")\n",
    "    print(\"  GET  /docs - Interactive API documentation\")\n",
    "    \n",
    "    # Test the API endpoints (simulate requests)\n",
    "    print(\"\\nüß™ Testing FastAPI endpoints:\")\n",
    "    \n",
    "    # Note: FastAPI testing requires async context, so we'll simulate the responses\n",
    "    import asyncio\n",
    "    \n",
    "    async def test_fastapi_endpoints():\n",
    "        service = TensorFlowModelService()\n",
    "        \n",
    "        # Single prediction\n",
    "        test_features = X_test[0].tolist()\n",
    "        result = await service.predict(test_features)\n",
    "        print(f\"Single prediction: Success={result.success}, Class={result.predicted_class}, \"\n",
    "              f\"Confidence={result.confidence:.3f}\")\n",
    "        \n",
    "        # Batch prediction\n",
    "        batch_features = [X_test[i].tolist() for i in range(3)]\n",
    "        batch_result = await service.batch_predict(batch_features)\n",
    "        print(f\"Batch prediction: Success={batch_result.success}, \"\n",
    "              f\"Samples={batch_result.total_samples}\")\n",
    "        \n",
    "        # Statistics\n",
    "        stats = service.get_stats()\n",
    "        print(f\"Statistics: {stats.total_requests} requests, \"\n",
    "              f\"avg time: {stats.average_inference_time_ms:.2f}ms\")\n",
    "    \n",
    "    # Run async tests\n",
    "    asyncio.run(test_fastapi_endpoints())\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è FastAPI or TensorFlow not available - skipping FastAPI demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Comparison and Best Practices\n",
    "\n",
    "Comparing different API approaches and deployment considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"API COMPARISON AND BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Framework comparison\n",
    "print(\"\\nüìä Framework Comparison:\")\n",
    "\n",
    "comparison_data = {\n",
    "    'Feature': [\n",
    "        'Performance',\n",
    "        'Async Support',\n",
    "        'Type Validation',\n",
    "        'Auto Documentation',\n",
    "        'Learning Curve',\n",
    "        'Ecosystem',\n",
    "        'Production Ready'\n",
    "    ],\n",
    "    'Flask': [\n",
    "        'Good (sync)',\n",
    "        'Limited',\n",
    "        'Manual',\n",
    "        'Manual',\n",
    "        'Easy',\n",
    "        'Mature',\n",
    "        'Yes'\n",
    "    ],\n",
    "    'FastAPI': [\n",
    "        'Excellent (async)',\n",
    "        'Native',\n",
    "        'Automatic (Pydantic)',\n",
    "        'Automatic (OpenAPI)',\n",
    "        'Moderate',\n",
    "        'Growing',\n",
    "        'Yes'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"{'Feature':<20} {'Flask':<20} {'FastAPI':<25}\")\n",
    "print(\"-\" * 65)\n",
    "for i, feature in enumerate(comparison_data['Feature']):\n",
    "    flask_val = comparison_data['Flask'][i]\n",
    "    fastapi_val = comparison_data['FastAPI'][i]\n",
    "    print(f\"{feature:<20} {flask_val:<20} {fastapi_val:<25}\")\n",
    "\n",
    "# Error handling patterns\n",
    "print(\"\\nüõ°Ô∏è Error Handling Best Practices:\")\n",
    "\n",
    "error_handling_examples = {\n",
    "    'Input Validation': {\n",
    "        'Flask': '''\n",
    "# Manual validation\n",
    "data = request.get_json()\n",
    "if not data or 'features' not in data:\n",
    "    return jsonify({'error': 'Missing features'}), 400\n",
    "\n",
    "features = data['features']\n",
    "if len(features) != expected_dim:\n",
    "    return jsonify({'error': 'Invalid feature count'}), 400\n",
    "        ''',\n",
    "        'FastAPI': '''\n",
    "# Automatic validation with Pydantic\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: List[float]\n",
    "    \n",
    "    @validator('features')\n",
    "    def validate_features(cls, v):\n",
    "        if len(v) != expected_dim:\n",
    "            raise ValueError('Invalid feature count')\n",
    "        return v\n",
    "        '''\n",
    "    },\n",
    "    'Exception Handling': {\n",
    "        'Flask': '''\n",
    "# Manual exception handling\n",
    "try:\n",
    "    result = model.predict(features)\n",
    "    return jsonify(result)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Prediction error: {e}\")\n",
    "    return jsonify({'error': 'Internal error'}), 500\n",
    "        ''',\n",
    "        'FastAPI': '''\n",
    "# Exception handlers\n",
    "@app.exception_handler(ValueError)\n",
    "async def validation_exception_handler(request, exc):\n",
    "    return JSONResponse(\n",
    "        status_code=400,\n",
    "        content={\"error\": str(exc)}\n",
    "    )\n",
    "        '''\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, examples in error_handling_examples.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(f\"Flask approach: {examples['Flask'].strip()}\")\n",
    "    print(f\"FastAPI approach: {examples['FastAPI'].strip()}\")\n",
    "\n",
    "# Performance optimization tips\n",
    "print(\"\\n‚ö° Performance Optimization Tips:\")\n",
    "\n",
    "performance_tips = [\n",
    "    \"Use batch processing for multiple predictions\",\n",
    "    \"Implement model warming during startup\",\n",
    "    \"Cache frequently used models in memory\",\n",
    "    \"Use connection pooling for database operations\",\n",
    "    \"Implement request queuing for high load\",\n",
    "    \"Monitor memory usage and implement cleanup\",\n",
    "    \"Use async endpoints for I/O-bound operations\",\n",
    "    \"Implement proper logging and metrics collection\"\n",
    "]\n",
    "\n",
    "for i, tip in enumerate(performance_tips, 1):\n",
    "    print(f\"  {i}. {tip}\")\n",
    "\n",
    "# Deployment considerations\n",
    "print(\"\\nüöÄ Deployment Considerations:\")\n",
    "\n",
    "deployment_checklist = {\n",
    "    'Security': [\n",
    "        \"Implement authentication and authorization\",\n",
    "        \"Use HTTPS in production\",\n",
    "        \"Validate and sanitize all inputs\",\n",
    "        \"Implement rate limiting\",\n",
    "        \"Use environment variables for secrets\"\n",
    "    ],\n",
    "    'Monitoring': [\n",
    "        \"Log all requests and responses\",\n",
    "        \"Monitor inference latency and throughput\",\n",
    "        \"Track model accuracy over time\",\n",
    "        \"Set up health checks and alerts\",\n",
    "        \"Monitor resource usage (CPU, memory, GPU)\"\n",
    "    ],\n",
    "    'Scalability': [\n",
    "        \"Use load balancers for multiple instances\",\n",
    "        \"Implement horizontal scaling\",\n",
    "        \"Consider using container orchestration\",\n",
    "        \"Implement caching strategies\",\n",
    "        \"Use async processing for heavy workloads\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in deployment_checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  ‚Ä¢ {item}\")\n",
    "\n",
    "# Sample Docker configurations\n",
    "print(\"\\nüê≥ Sample Docker Configuration:\")\n",
    "\n",
    "dockerfile_pytorch = '''\n",
    "# PyTorch Flask API Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 5000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:5000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"--workers\", \"4\", \"app:app\"]\n",
    "'''\n",
    "\n",
    "dockerfile_tensorflow = '''\n",
    "# TensorFlow FastAPI Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n",
    "'''\n",
    "\n",
    "print(\"PyTorch Flask Dockerfile:\")\n",
    "print(dockerfile_pytorch.strip())\n",
    "\n",
    "print(\"\\nTensorFlow FastAPI Dockerfile:\")\n",
    "print(dockerfile_tensorflow.strip())\n",
    "\n",
    "# Side-by-side API comparison\n",
    "flask_api_code = \"\"\"\n",
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = load_pytorch_model()\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        features = data['features']\n",
    "        \n",
    "        # Manual validation\n",
    "        if len(features) != expected_dim:\n",
    "            return jsonify({'error': 'Invalid input'}), 400\n",
    "        \n",
    "        # Prediction\n",
    "        with torch.no_grad():\n",
    "            result = model(torch.FloatTensor([features]))\n",
    "            prediction = torch.argmax(result).item()\n",
    "        \n",
    "        return jsonify({\n",
    "            'prediction': prediction,\n",
    "            'confidence': torch.max(result).item()\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\"\"\"\n",
    "\n",
    "fastapi_api_code = \"\"\"\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, validator\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "\n",
    "app = FastAPI()\n",
    "model = tf.keras.models.load_model('model_path')\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: List[float]\n",
    "    \n",
    "    @validator('features')\n",
    "    def validate_features(cls, v):\n",
    "        if len(v) != expected_dim:\n",
    "            raise ValueError('Invalid input dimension')\n",
    "        return v\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: int\n",
    "    confidence: float\n",
    "\n",
    "@app.post('/predict', response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    try:\n",
    "        # Automatic validation by Pydantic\n",
    "        input_tensor = tf.constant([request.features])\n",
    "        \n",
    "        # Prediction\n",
    "        result = model(input_tensor, training=False)\n",
    "        prediction = int(tf.argmax(result, axis=1)[0])\n",
    "        confidence = float(tf.reduce_max(result))\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            prediction=prediction,\n",
    "            confidence=confidence\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host='0.0.0.0', port=8000)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + create_side_by_side_comparison(\n",
    "    flask_api_code, fastapi_api_code, \"API Implementation Comparison\"\n",
    "))\n",
    "\n",
    "print(\"\\nüéØ Key Takeaways:\")\n",
    "print(\"‚Ä¢ Choose Flask for simple APIs and existing Flask expertise\")\n",
    "print(\"‚Ä¢ Choose FastAPI for modern async APIs with automatic documentation\")\n",
    "print(\"‚Ä¢ Implement proper error handling and input validation\")\n",
    "print(\"‚Ä¢ Use appropriate deployment strategies (Docker, load balancing)\")\n",
    "print(\"‚Ä¢ Monitor performance and implement proper logging\")\n",
    "print(\"‚Ä¢ Consider security, scalability, and maintenance from the start\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
