{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Endpoints: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Create REST API endpoints for model serving\n",
    "- Compare Flask and FastAPI integration patterns\n",
    "- Learn request/response handling and validation\n",
    "- Understand error management and monitoring\n",
    "\n",
    "**Prerequisites:** Model serialization, inference patterns\n",
    "\n",
    "**Estimated Time:** 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import logging\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from foundations.data_utils import get_tutorial_tabular_data\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import web frameworks\n",
    "try:\n",
    "    from flask import Flask, request, jsonify\n",
    "    FLASK_AVAILABLE = True\n",
    "    print(\"✅ Flask available\")\n",
    "except ImportError:\n",
    "    FLASK_AVAILABLE = False\n",
    "    print(\"❌ Flask not available (install with: pip install flask)\")\n",
    "\n",
    "try:\n",
    "    from fastapi import FastAPI, HTTPException\n",
    "    from pydantic import BaseModel, validator\n",
    "    import uvicorn\n",
    "    FASTAPI_AVAILABLE = True\n",
    "    print(\"✅ FastAPI available\")\n",
    "except ImportError:\n",
    "    FASTAPI_AVAILABLE = False\n",
    "    print(\"❌ FastAPI not available (install with: pip install fastapi uvicorn pydantic)\")\n",
    "\n",
    "# Try to import ML frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"✅ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"❌ PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"✅ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"❌ TensorFlow not available\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get sample data for API testing\n",
    "data = get_tutorial_tabular_data()\n",
    "X_test = data['X_test']\n",
    "input_dim = X_test.shape[1]\n",
    "num_classes = len(np.unique(data['y_test']))\n",
    "\n",
    "print(f\"\\nData info: {input_dim} features, {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Flask API with PyTorch\n",
    "\n",
    "Creating a simple REST API using Flask and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FLASK API WITH PYTORCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if FLASK_AVAILABLE and PYTORCH_AVAILABLE:\n",
    "    print(\"\\n🔥 PyTorch Flask API Implementation:\")\n",
    "    \n",
    "    # Define model architecture\n",
    "    class SimpleClassifier(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "            self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "            self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    # Model service class\n",
    "    class PyTorchModelService:\n",
    "        def __init__(self, model_path=None, device='cpu'):\n",
    "            self.device = torch.device(device)\n",
    "            self.model = self._load_model(model_path)\n",
    "            self.model.eval()\n",
    "            \n",
    "            # Statistics tracking\n",
    "            self.request_count = 0\n",
    "            self.total_inference_time = 0.0\n",
    "            self.lock = threading.Lock()\n",
    "        \n",
    "        def _load_model(self, model_path):\n",
    "            \"\"\"Load model (create dummy for demo)\"\"\"\n",
    "            model = SimpleClassifier(input_dim, 64, num_classes).to(self.device)\n",
    "            # In practice: model.load_state_dict(torch.load(model_path))\n",
    "            return model\n",
    "        \n",
    "        def predict(self, features: List[float]) -> Dict[str, Any]:\n",
    "            \"\"\"Make prediction with error handling\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Validate input\n",
    "                if len(features) != input_dim:\n",
    "                    raise ValueError(f\"Expected {input_dim} features, got {len(features)}\")\n",
    "                \n",
    "                # Convert to tensor\n",
    "                input_tensor = torch.FloatTensor([features]).to(self.device)\n",
    "                \n",
    "                # Inference\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                # Update statistics\n",
    "                with self.lock:\n",
    "                    self.request_count += 1\n",
    "                    self.total_inference_time += inference_time\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'predicted_class': int(predicted_class.item()),\n",
    "                    'probabilities': probabilities.cpu().numpy()[0].tolist(),\n",
    "                    'confidence': float(torch.max(probabilities).item()),\n",
    "                    'inference_time_ms': inference_time * 1000,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Prediction error: {str(e)}\")\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "        \n",
    "        def get_stats(self) -> Dict[str, Any]:\n",
    "            \"\"\"Get service statistics\"\"\"\n",
    "            with self.lock:\n",
    "                avg_time = (self.total_inference_time / self.request_count \n",
    "                           if self.request_count > 0 else 0)\n",
    "                \n",
    "                return {\n",
    "                    'total_requests': self.request_count,\n",
    "                    'average_inference_time_ms': avg_time * 1000,\n",
    "                    'model_device': str(self.device),\n",
    "                    'model_parameters': sum(p.numel() for p in self.model.parameters())\n",
    "                }\n",
    "    \n",
    "    # Create Flask app\n",
    "    def create_pytorch_flask_app():\n",
    "        app = Flask(__name__)\n",
    "        model_service = PyTorchModelService()\n",
    "        \n",
    "        @app.route('/health', methods=['GET'])\n",
    "        def health_check():\n",
    "            \"\"\"Health check endpoint\"\"\"\n",
    "            return jsonify({\n",
    "                'status': 'healthy',\n",
    "                'framework': 'pytorch',\n",
    "                'version': torch.__version__,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        @app.route('/predict', methods=['POST'])\n",
    "        def predict():\n",
    "            \"\"\"Prediction endpoint\"\"\"\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                \n",
    "                if not data or 'features' not in data:\n",
    "                    return jsonify({\n",
    "                        'success': False,\n",
    "                        'error': 'Missing features in request body'\n",
    "                    }), 400\n",
    "                \n",
    "                features = data['features']\n",
    "                result = model_service.predict(features)\n",
    "                \n",
    "                status_code = 200 if result['success'] else 400\n",
    "                return jsonify(result), status_code\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Request processing error: {str(e)}\")\n",
    "                return jsonify({\n",
    "                    'success': False,\n",
    "                    'error': 'Internal server error'\n",
    "                }), 500\n",
    "        \n",
    "        @app.route('/batch_predict', methods=['POST'])\n",
    "        def batch_predict():\n",
    "            \"\"\"Batch prediction endpoint\"\"\"\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                \n",
    "                if not data or 'samples' not in data:\n",
    "                    return jsonify({\n",
    "                        'success': False,\n",
    "                        'error': 'Missing samples in request body'\n",
    "                    }), 400\n",
    "                \n",
    "                samples = data['samples']\n",
    "                results = []\n",
    "                \n",
    "                for i, features in enumerate(samples):\n",
    "                    result = model_service.predict(features)\n",
    "                    result['sample_id'] = i\n",
    "                    results.append(result)\n",
    "                \n",
    "                return jsonify({\n",
    "                    'success': True,\n",
    "                    'results': results,\n",
    "                    'total_samples': len(results)\n",
    "                })\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch processing error: {str(e)}\")\n",
    "                return jsonify({\n",
    "                    'success': False,\n",
    "                    'error': 'Internal server error'\n",
    "                }), 500\n",
    "        \n",
    "        @app.route('/stats', methods=['GET'])\n",
    "        def get_stats():\n",
    "            \"\"\"Statistics endpoint\"\"\"\n",
    "            return jsonify(model_service.get_stats())\n",
    "        \n",
    "        return app\n",
    "    \n",
    "    # Create app instance\n",
    "    pytorch_app = create_pytorch_flask_app()\n",
    "    \n",
    "    print(\"✅ PyTorch Flask API created\")\n",
    "    print(\"Available endpoints:\")\n",
    "    print(\"  GET  /health - Health check\")\n",
    "    print(\"  POST /predict - Single prediction\")\n",
    "    print(\"  POST /batch_predict - Batch predictions\")\n",
    "    print(\"  GET  /stats - Service statistics\")\n",
    "    \n",
    "    # Test the API endpoints (simulate requests)\n",
    "    print(\"\\n🧪 Testing API endpoints:\")\n",
    "    \n",
    "    # Simulate health check\n",
    "    with pytorch_app.test_client() as client:\n",
    "        # Health check\n",
    "        response = client.get('/health')\n",
    "        print(f\"Health check: {response.status_code} - {response.get_json()['status']}\")\n",
    "        \n",
    "        # Single prediction\n",
    "        test_features = X_test[0].tolist()\n",
    "        response = client.post('/predict', \n",
    "                             json={'features': test_features},\n",
    "                             content_type='application/json')\n",
    "        result = response.get_json()\n",
    "        print(f\"Single prediction: {response.status_code} - Class {result.get('predicted_class')} \"\n",
    "              f\"(confidence: {result.get('confidence', 0):.3f})\")\n",
    "        \n",
    "        # Batch prediction\n",
    "        batch_features = [X_test[i].tolist() for i in range(3)]\n",
    "        response = client.post('/batch_predict',\n",
    "                             json={'samples': batch_features},\n",
    "                             content_type='application/json')\n",
    "        batch_result = response.get_json()\n",
    "        print(f\"Batch prediction: {response.status_code} - {batch_result.get('total_samples')} samples processed\")\n",
    "        \n",
    "        # Statistics\n",
    "        response = client.get('/stats')\n",
    "        stats = response.get_json()\n",
    "        print(f\"Statistics: {stats['total_requests']} requests, \"\n",
    "              f\"avg time: {stats['average_inference_time_ms']:.2f}ms\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Flask or PyTorch not available - skipping Flask API demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FastAPI with TensorFlow\n",
    "\n",
    "Creating a modern async API using FastAPI and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FASTAPI WITH TENSORFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if FASTAPI_AVAILABLE and TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n🟠 TensorFlow FastAPI Implementation:\")\n",
    "    \n",
    "    # Pydantic models for request/response validation\n",
    "    class PredictionRequest(BaseModel):\n",
    "        features: List[float]\n",
    "        \n",
    "        @validator('features')\n",
    "        def validate_features(cls, v):\n",
    "            if len(v) != input_dim:\n",
    "                raise ValueError(f'Expected {input_dim} features, got {len(v)}')\n",
    "            return v\n",
    "    \n",
    "    class BatchPredictionRequest(BaseModel):\n",
    "        samples: List[List[float]]\n",
    "        \n",
    "        @validator('samples')\n",
    "        def validate_samples(cls, v):\n",
    "            for i, sample in enumerate(v):\n",
    "                if len(sample) != input_dim:\n",
    "                    raise ValueError(f'Sample {i}: expected {input_dim} features, got {len(sample)}')\n",
    "            return v\n",
    "    \n",
    "    class PredictionResponse(BaseModel):\n",
    "        success: bool\n",
    "        predicted_class: Optional[int] = None\n",
    "        probabilities: Optional[List[float]] = None\n",
    "        confidence: Optional[float] = None\n",
    "        inference_time_ms: Optional[float] = None\n",
    "        timestamp: str\n",
    "        error: Optional[str] = None\n",
    "    \n",
    "    class BatchPredictionResponse(BaseModel):\n",
    "        success: bool\n",
    "        results: Optional[List[PredictionResponse]] = None\n",
    "        total_samples: Optional[int] = None\n",
    "        error: Optional[str] = None\n",
    "    \n",
    "    class HealthResponse(BaseModel):\n",
    "        status: str\n",
    "        framework: str\n",
    "        version: str\n",
    "        timestamp: str\n",
    "    \n",
    "    class StatsResponse(BaseModel):\n",
    "        total_requests: int\n",
    "        average_inference_time_ms: float\n",
    "        model_parameters: int\n",
    "        gpu_available: bool\n",
    "    \n",
    "    # Model service class\n",
    "    class TensorFlowModelService:\n",
    "        def __init__(self, model_path=None):\n",
    "            self.model = self._load_model(model_path)\n",
    "            \n",
    "            # Create optimized inference function\n",
    "            self.inference_func = tf.function(self.model.call)\n",
    "            \n",
    "            # Statistics tracking\n",
    "            self.request_count = 0\n",
    "            self.total_inference_time = 0.0\n",
    "            self.lock = threading.Lock()\n",
    "        \n",
    "        def _load_model(self, model_path):\n",
    "            \"\"\"Load model (create dummy for demo)\"\"\"\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "            # In practice: model = tf.keras.models.load_model(model_path)\n",
    "            return model\n",
    "        \n",
    "        async def predict(self, features: List[float]) -> PredictionResponse:\n",
    "            \"\"\"Async prediction with error handling\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Convert to tensor\n",
    "                input_tensor = tf.constant([features], dtype=tf.float32)\n",
    "                \n",
    "                # Inference\n",
    "                probabilities = self.inference_func(input_tensor, training=False)\n",
    "                predicted_class = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                # Update statistics\n",
    "                with self.lock:\n",
    "                    self.request_count += 1\n",
    "                    self.total_inference_time += inference_time\n",
    "                \n",
    "                return PredictionResponse(\n",
    "                    success=True,\n",
    "                    predicted_class=int(predicted_class.numpy()[0]),\n",
    "                    probabilities=probabilities.numpy()[0].tolist(),\n",
    "                    confidence=float(tf.reduce_max(probabilities).numpy()),\n",
    "                    inference_time_ms=inference_time * 1000,\n",
    "                    timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Prediction error: {str(e)}\")\n",
    "                return PredictionResponse(\n",
    "                    success=False,\n",
    "                    error=str(e),\n",
    "                    timestamp=datetime.now().isoformat()\n",
    "                )\n",
    "        \n",
    "        async def batch_predict(self, samples: List[List[float]]) -> BatchPredictionResponse:\n",
    "            \"\"\"Async batch prediction\"\"\"\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Convert to tensor\n",
    "                input_tensor = tf.constant(samples, dtype=tf.float32)\n",
    "                \n",
    "                # Batch inference\n",
    "                probabilities = self.inference_func(input_tensor, training=False)\n",
    "                predicted_classes = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                # Update statistics\n",
    "                with self.lock:\n",
    "                    self.request_count += len(samples)\n",
    "                    self.total_inference_time += batch_time\n",
    "                \n",
    "                # Create individual results\n",
    "                results = []\n",
    "                for i in range(len(samples)):\n",
    "                    results.append(PredictionResponse(\n",
    "                        success=True,\n",
    "                        predicted_class=int(predicted_classes[i].numpy()),\n",
    "                        probabilities=probabilities[i].numpy().tolist(),\n",
    "                        confidence=float(probabilities[i].numpy().max()),\n",
    "                        inference_time_ms=(batch_time / len(samples)) * 1000,\n",
    "                        timestamp=datetime.now().isoformat()\n",
    "                    ))\n",
    "                \n",
    "                return BatchPredictionResponse(\n",
    "                    success=True,\n",
    "                    results=results,\n",
    "                    total_samples=len(results)\n",
    "                )\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch prediction error: {str(e)}\")\n",
    "                return BatchPredictionResponse(\n",
    "                    success=False,\n",
    "                    error=str(e)\n",
    "                )\n",
    "        \n",
    "        def get_stats(self) -> StatsResponse:\n",
    "            \"\"\"Get service statistics\"\"\"\n",
    "            with self.lock:\n",
    "                avg_time = (self.total_inference_time / self.request_count \n",
    "                           if self.request_count > 0 else 0)\n",
    "                \n",
    "                return StatsResponse(\n",
    "                    total_requests=self.request_count,\n",
    "                    average_inference_time_ms=avg_time * 1000,\n",
    "                    model_parameters=self.model.count_params(),\n",
    "                    gpu_available=len(tf.config.list_physical_devices('GPU')) > 0\n",
    "                )\n",
    "    \n",
    "    # Create FastAPI app\n",
    "    def create_tensorflow_fastapi_app():\n",
    "        app = FastAPI(\n",
    "            title=\"TensorFlow Model API\",\n",
    "            description=\"REST API for TensorFlow model inference\",\n",
    "            version=\"1.0.0\"\n",
    "        )\n",
    "        \n",
    "        model_service = TensorFlowModelService()\n",
    "        \n",
    "        @app.get(\"/health\", response_model=HealthResponse)\n",
    "        async def health_check():\n",
    "            \"\"\"Health check endpoint\"\"\"\n",
    "            return HealthResponse(\n",
    "                status=\"healthy\",\n",
    "                framework=\"tensorflow\",\n",
    "                version=tf.__version__,\n",
    "                timestamp=datetime.now().isoformat()\n",
    "            )\n",
    "        \n",
    "        @app.post(\"/predict\", response_model=PredictionResponse)\n",
    "        async def predict(request: PredictionRequest):\n",
    "            \"\"\"Single prediction endpoint\"\"\"\n",
    "            return await model_service.predict(request.features)\n",
    "        \n",
    "        @app.post(\"/batch_predict\", response_model=BatchPredictionResponse)\n",
    "        async def batch_predict(request: BatchPredictionRequest):\n",
    "            \"\"\"Batch prediction endpoint\"\"\"\n",
    "            return await model_service.batch_predict(request.samples)\n",
    "        \n",
    "        @app.get(\"/stats\", response_model=StatsResponse)\n",
    "        async def get_stats():\n",
    "            \"\"\"Statistics endpoint\"\"\"\n",
    "            return model_service.get_stats()\n",
    "        \n",
    "        return app\n",
    "    \n",
    "    # Create app instance\n",
    "    tensorflow_app = create_tensorflow_fastapi_app()\n",
    "    \n",
    "    print(\"✅ TensorFlow FastAPI created\")\n",
    "    print(\"Available endpoints:\")\n",
    "    print(\"  GET  /health - Health check\")\n",
    "    print(\"  POST /predict - Single prediction\")\n",
    "    print(\"  POST /batch_predict - Batch predictions\")\n",
    "    print(\"  GET  /stats - Service statistics\")\n",
    "    print(\"  GET  /docs - Interactive API documentation\")\n",
    "    \n",
    "    # Test the API endpoints (simulate requests)\n",
    "    print(\"\\n🧪 Testing FastAPI endpoints:\")\n",
    "    \n",
    "    # Note: FastAPI testing requires async context, so we'll simulate the responses\n",
    "    import asyncio\n",
    "    \n",
    "    async def test_fastapi_endpoints():\n",
    "        service = TensorFlowModelService()\n",
    "        \n",
    "        # Single prediction\n",
    "        test_features = X_test[0].tolist()\n",
    "        result = await service.predict(test_features)\n",
    "        print(f\"Single prediction: Success={result.success}, Class={result.predicted_class}, \"\n",
    "              f\"Confidence={result.confidence:.3f}\")\n",
    "        \n",
    "        # Batch prediction\n",
    "        batch_features = [X_test[i].tolist() for i in range(3)]\n",
    "        batch_result = await service.batch_predict(batch_features)\n",
    "        print(f\"Batch prediction: Success={batch_result.success}, \"\n",
    "              f\"Samples={batch_result.total_samples}\")\n",
    "        \n",
    "        # Statistics\n",
    "        stats = service.get_stats()\n",
    "        print(f\"Statistics: {stats.total_requests} requests, \"\n",
    "              f\"avg time: {stats.average_inference_time_ms:.2f}ms\")\n",
    "    \n",
    "    # Run async tests\n",
    "    asyncio.run(test_fastapi_endpoints())\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ FastAPI or TensorFlow not available - skipping FastAPI demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Comparison and Best Practices\n",
    "\n",
    "Comparing different API approaches and deployment considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"API COMPARISON AND BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Framework comparison\n",
    "print(\"\\n📊 Framework Comparison:\")\n",
    "\n",
    "comparison_data = {\n",
    "    'Feature': [\n",
    "        'Performance',\n",
    "        'Async Support',\n",
    "        'Type Validation',\n",
    "        'Auto Documentation',\n",
    "        'Learning Curve',\n",
    "        'Ecosystem',\n",
    "        'Production Ready'\n",
    "    ],\n",
    "    'Flask': [\n",
    "        'Good (sync)',\n",
    "        'Limited',\n",
    "        'Manual',\n",
    "        'Manual',\n",
    "        'Easy',\n",
    "        'Mature',\n",
    "        'Yes'\n",
    "    ],\n",
    "    'FastAPI': [\n",
    "        'Excellent (async)',\n",
    "        'Native',\n",
    "        'Automatic (Pydantic)',\n",
    "        'Automatic (OpenAPI)',\n",
    "        'Moderate',\n",
    "        'Growing',\n",
    "        'Yes'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"{'Feature':<20} {'Flask':<20} {'FastAPI':<25}\")\n",
    "print(\"-\" * 65)\n",
    "for i, feature in enumerate(comparison_data['Feature']):\n",
    "    flask_val = comparison_data['Flask'][i]\n",
    "    fastapi_val = comparison_data['FastAPI'][i]\n",
    "    print(f\"{feature:<20} {flask_val:<20} {fastapi_val:<25}\")\n",
    "\n",
    "# Error handling patterns\n",
    "print(\"\\n🛡️ Error Handling Best Practices:\")\n",
    "\n",
    "error_handling_examples = {\n",
    "    'Input Validation': {\n",
    "        'Flask': '''\n",
    "# Manual validation\n",
    "data = request.get_json()\n",
    "if not data or 'features' not in data:\n",
    "    return jsonify({'error': 'Missing features'}), 400\n",
    "\n",
    "features = data['features']\n",
    "if len(features) != expected_dim:\n",
    "    return jsonify({'error': 'Invalid feature count'}), 400\n",
    "        ''',\n",
    "        'FastAPI': '''\n",
    "# Automatic validation with Pydantic\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: List[float]\n",
    "    \n",
    "    @validator('features')\n",
    "    def validate_features(cls, v):\n",
    "        if len(v) != expected_dim:\n",
    "            raise ValueError('Invalid feature count')\n",
    "        return v\n",
    "        '''\n",
    "    },\n",
    "    'Exception Handling': {\n",
    "        'Flask': '''\n",
    "# Manual exception handling\n",
    "try:\n",
    "    result = model.predict(features)\n",
    "    return jsonify(result)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Prediction error: {e}\")\n",
    "    return jsonify({'error': 'Internal error'}), 500\n",
    "        ''',\n",
    "        'FastAPI': '''\n",
    "# Exception handlers\n",
    "@app.exception_handler(ValueError)\n",
    "async def validation_exception_handler(request, exc):\n",
    "    return JSONResponse(\n",
    "        status_code=400,\n",
    "        content={\"error\": str(exc)}\n",
    "    )\n",
    "        '''\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, examples in error_handling_examples.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(f\"Flask approach: {examples['Flask'].strip()}\")\n",
    "    print(f\"FastAPI approach: {examples['FastAPI'].strip()}\")\n",
    "\n",
    "# Performance optimization tips\n",
    "print(\"\\n⚡ Performance Optimization Tips:\")\n",
    "\n",
    "performance_tips = [\n",
    "    \"Use batch processing for multiple predictions\",\n",
    "    \"Implement model warming during startup\",\n",
    "    \"Cache frequently used models in memory\",\n",
    "    \"Use connection pooling for database operations\",\n",
    "    \"Implement request queuing for high load\",\n",
    "    \"Monitor memory usage and implement cleanup\",\n",
    "    \"Use async endpoints for I/O-bound operations\",\n",
    "    \"Implement proper logging and metrics collection\"\n",
    "]\n",
    "\n",
    "for i, tip in enumerate(performance_tips, 1):\n",
    "    print(f\"  {i}. {tip}\")\n",
    "\n",
    "# Deployment considerations\n",
    "print(\"\\n🚀 Deployment Considerations:\")\n",
    "\n",
    "deployment_checklist = {\n",
    "    'Security': [\n",
    "        \"Implement authentication and authorization\",\n",
    "        \"Use HTTPS in production\",\n",
    "        \"Validate and sanitize all inputs\",\n",
    "        \"Implement rate limiting\",\n",
    "        \"Use environment variables for secrets\"\n",
    "    ],\n",
    "    'Monitoring': [\n",
    "        \"Log all requests and responses\",\n",
    "        \"Monitor inference latency and throughput\",\n",
    "        \"Track model accuracy over time\",\n",
    "        \"Set up health checks and alerts\",\n",
    "        \"Monitor resource usage (CPU, memory, GPU)\"\n",
    "    ],\n",
    "    'Scalability': [\n",
    "        \"Use load balancers for multiple instances\",\n",
    "        \"Implement horizontal scaling\",\n",
    "        \"Consider using container orchestration\",\n",
    "        \"Implement caching strategies\",\n",
    "        \"Use async processing for heavy workloads\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in deployment_checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  • {item}\")\n",
    "\n",
    "# Sample Docker configurations\n",
    "print(\"\\n🐳 Sample Docker Configuration:\")\n",
    "\n",
    "dockerfile_pytorch = '''\n",
    "# PyTorch Flask API Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 5000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:5000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"--workers\", \"4\", \"app:app\"]\n",
    "'''\n",
    "\n",
    "dockerfile_tensorflow = '''\n",
    "# TensorFlow FastAPI Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n",
    "'''\n",
    "\n",
    "print(\"PyTorch Flask Dockerfile:\")\n",
    "print(dockerfile_pytorch.strip())\n",
    "\n",
    "print(\"\\nTensorFlow FastAPI Dockerfile:\")\n",
    "print(dockerfile_tensorflow.strip())\n",
    "\n",
    "# Side-by-side API comparison\n",
    "flask_api_code = \"\"\"\n",
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = load_pytorch_model()\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        features = data['features']\n",
    "        \n",
    "        # Manual validation\n",
    "        if len(features) != expected_dim:\n",
    "            return jsonify({'error': 'Invalid input'}), 400\n",
    "        \n",
    "        # Prediction\n",
    "        with torch.no_grad():\n",
    "            result = model(torch.FloatTensor([features]))\n",
    "            prediction = torch.argmax(result).item()\n",
    "        \n",
    "        return jsonify({\n",
    "            'prediction': prediction,\n",
    "            'confidence': torch.max(result).item()\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\"\"\"\n",
    "\n",
    "fastapi_api_code = \"\"\"\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, validator\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "\n",
    "app = FastAPI()\n",
    "model = tf.keras.models.load_model('model_path')\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: List[float]\n",
    "    \n",
    "    @validator('features')\n",
    "    def validate_features(cls, v):\n",
    "        if len(v) != expected_dim:\n",
    "            raise ValueError('Invalid input dimension')\n",
    "        return v\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: int\n",
    "    confidence: float\n",
    "\n",
    "@app.post('/predict', response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    try:\n",
    "        # Automatic validation by Pydantic\n",
    "        input_tensor = tf.constant([request.features])\n",
    "        \n",
    "        # Prediction\n",
    "        result = model(input_tensor, training=False)\n",
    "        prediction = int(tf.argmax(result, axis=1)[0])\n",
    "        confidence = float(tf.reduce_max(result))\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            prediction=prediction,\n",
    "            confidence=confidence\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host='0.0.0.0', port=8000)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + create_side_by_side_comparison(\n",
    "    flask_api_code, fastapi_api_code, \"API Implementation Comparison\"\n",
    "))\n",
    "\n",
    "print(\"\\n🎯 Key Takeaways:\")\n",
    "print(\"• Choose Flask for simple APIs and existing Flask expertise\")\n",
    "print(\"• Choose FastAPI for modern async APIs with automatic documentation\")\n",
    "print(\"• Implement proper error handling and input validation\")\n",
    "print(\"• Use appropriate deployment strategies (Docker, load balancing)\")\n",
    "print(\"• Monitor performance and implement proper logging\")\n",
    "print(\"• Consider security, scalability, and maintenance from the start\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
