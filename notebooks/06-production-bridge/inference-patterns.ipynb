{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Patterns: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master batch and real-time inference patterns\n",
    "- Learn optimization techniques for inference performance\n",
    "- Compare preprocessing and postprocessing approaches\n",
    "- Understand memory management during inference\n",
    "\n",
    "**Prerequisites:** Model serialization, framework fundamentals\n",
    "\n",
    "**Estimated Time:** 35 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Dict, Any, Union\n",
    "import threading\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from foundations.data_utils import get_tutorial_tabular_data, get_tutorial_text_data\n",
    "from foundations.preprocessing import TextPreprocessor\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"✅ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"❌ PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"✅ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"❌ TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Sample Inference\n",
    "\n",
    "Optimized patterns for real-time, single-sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SINGLE SAMPLE INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data\n",
    "tabular_data = get_tutorial_tabular_data()\n",
    "X_test = tabular_data['X_test']\n",
    "y_test = tabular_data['y_test']\n",
    "\n",
    "input_dim = X_test.shape[1]\n",
    "num_classes = len(np.unique(y_test))\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# PyTorch single sample inference\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\n🔥 PyTorch Single Sample Inference:\")\n",
    "    \n",
    "    class PyTorchInferenceEngine:\n",
    "        def __init__(self, model_path: str, device: str = 'cpu'):\n",
    "            self.device = torch.device(device)\n",
    "            self.model = self._load_model(model_path)\n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "            \n",
    "            # Warm up the model\n",
    "            self._warmup()\n",
    "        \n",
    "        def _load_model(self, model_path: str):\n",
    "            \"\"\"Load model from checkpoint\"\"\"\n",
    "            # Define model architecture (in practice, this would be imported)\n",
    "            class SimpleClassifier(nn.Module):\n",
    "                def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "                    super().__init__()\n",
    "                    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "                    self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "                    self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "                    self.relu = nn.ReLU()\n",
    "                    self.dropout = nn.Dropout(0.2)\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    x = self.relu(self.fc1(x))\n",
    "                    x = self.dropout(x)\n",
    "                    x = self.relu(self.fc2(x))\n",
    "                    x = self.dropout(x)\n",
    "                    x = self.fc3(x)\n",
    "                    return x\n",
    "            \n",
    "            # Create and load model\n",
    "            model = SimpleClassifier(input_dim, 64, num_classes).to(self.device)\n",
    "            \n",
    "            # For demo, create a trained model\n",
    "            # In practice, you'd load from saved checkpoint\n",
    "            return model\n",
    "        \n",
    "        def _warmup(self, num_warmup: int = 5):\n",
    "            \"\"\"Warm up the model with dummy inputs\"\"\"\n",
    "            dummy_input = torch.randn(1, input_dim).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(num_warmup):\n",
    "                    _ = self.model(dummy_input)\n",
    "        \n",
    "        def predict_single(self, sample: np.ndarray) -> Dict[str, Any]:\n",
    "            \"\"\"Predict single sample with timing\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Preprocessing\n",
    "            input_tensor = torch.FloatTensor(sample.reshape(1, -1)).to(self.device)\n",
    "            \n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(input_tensor)\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                predicted_class = torch.argmax(probabilities, dim=1)\n",
    "            \n",
    "            # Postprocessing\n",
    "            result = {\n",
    "                'predicted_class': predicted_class.cpu().item(),\n",
    "                'probabilities': probabilities.cpu().numpy()[0],\n",
    "                'confidence': torch.max(probabilities).cpu().item(),\n",
    "                'inference_time_ms': (time.time() - start_time) * 1000\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def predict_batch_optimized(self, samples: np.ndarray, batch_size: int = 32) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Optimized batch prediction\"\"\"\n",
    "            results = []\n",
    "            \n",
    "            for i in range(0, len(samples), batch_size):\n",
    "                batch = samples[i:i + batch_size]\n",
    "                batch_tensor = torch.FloatTensor(batch).to(self.device)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(batch_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predicted_classes = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                # Convert to individual results\n",
    "                for j in range(len(batch)):\n",
    "                    results.append({\n",
    "                        'predicted_class': predicted_classes[j].cpu().item(),\n",
    "                        'probabilities': probabilities[j].cpu().numpy(),\n",
    "                        'confidence': probabilities[j].max().cpu().item(),\n",
    "                        'inference_time_ms': (batch_time / len(batch)) * 1000\n",
    "                    })\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    # Create inference engine\n",
    "    pt_engine = PyTorchInferenceEngine('dummy_path')\n",
    "    \n",
    "    # Test single sample inference\n",
    "    sample = X_test[0]\n",
    "    result = pt_engine.predict_single(sample)\n",
    "    \n",
    "    print(f\"Single sample prediction:\")\n",
    "    print(f\"  Predicted class: {result['predicted_class']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"  Inference time: {result['inference_time_ms']:.2f} ms\")\n",
    "    \n",
    "    # Benchmark single vs batch inference\n",
    "    test_samples = X_test[:100]\n",
    "    \n",
    "    # Single sample approach\n",
    "    start_time = time.time()\n",
    "    single_results = [pt_engine.predict_single(sample) for sample in test_samples]\n",
    "    single_total_time = time.time() - start_time\n",
    "    \n",
    "    # Batch approach\n",
    "    start_time = time.time()\n",
    "    batch_results = pt_engine.predict_batch_optimized(test_samples, batch_size=32)\n",
    "    batch_total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n📊 Performance Comparison (100 samples):\")\n",
    "    print(f\"  Single inference: {single_total_time:.3f}s ({single_total_time/len(test_samples)*1000:.2f} ms/sample)\")\n",
    "    print(f\"  Batch inference: {batch_total_time:.3f}s ({batch_total_time/len(test_samples)*1000:.2f} ms/sample)\")\n",
    "    print(f\"  Speedup: {single_total_time/batch_total_time:.1f}x\")\n",
    "\n",
    "# TensorFlow single sample inference\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n🟠 TensorFlow Single Sample Inference:\")\n",
    "    \n",
    "    class TensorFlowInferenceEngine:\n",
    "        def __init__(self, model_path: str = None):\n",
    "            self.model = self._create_model()  # In practice, load from saved model\n",
    "            \n",
    "            # Compile for inference optimization\n",
    "            self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "            \n",
    "            # Create inference function for better performance\n",
    "            self.inference_func = tf.function(self.model.call)\n",
    "            \n",
    "            # Warm up\n",
    "            self._warmup()\n",
    "        \n",
    "        def _create_model(self):\n",
    "            \"\"\"Create model (in practice, load from saved model)\"\"\"\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "            return model\n",
    "        \n",
    "        def _warmup(self, num_warmup: int = 5):\n",
    "            \"\"\"Warm up the model\"\"\"\n",
    "            dummy_input = tf.random.normal((1, input_dim))\n",
    "            \n",
    "            for _ in range(num_warmup):\n",
    "                _ = self.inference_func(dummy_input, training=False)\n",
    "        \n",
    "        def predict_single(self, sample: np.ndarray) -> Dict[str, Any]:\n",
    "            \"\"\"Predict single sample with timing\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Preprocessing\n",
    "            input_tensor = tf.constant(sample.reshape(1, -1), dtype=tf.float32)\n",
    "            \n",
    "            # Inference\n",
    "            probabilities = self.inference_func(input_tensor, training=False)\n",
    "            predicted_class = tf.argmax(probabilities, axis=1)\n",
    "            \n",
    "            # Postprocessing\n",
    "            result = {\n",
    "                'predicted_class': int(predicted_class.numpy()[0]),\n",
    "                'probabilities': probabilities.numpy()[0],\n",
    "                'confidence': float(tf.reduce_max(probabilities).numpy()),\n",
    "                'inference_time_ms': (time.time() - start_time) * 1000\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def predict_batch_optimized(self, samples: np.ndarray, batch_size: int = 32) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Optimized batch prediction\"\"\"\n",
    "            results = []\n",
    "            \n",
    "            for i in range(0, len(samples), batch_size):\n",
    "                batch = samples[i:i + batch_size]\n",
    "                batch_tensor = tf.constant(batch, dtype=tf.float32)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                probabilities = self.inference_func(batch_tensor, training=False)\n",
    "                predicted_classes = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                # Convert to individual results\n",
    "                for j in range(len(batch)):\n",
    "                    results.append({\n",
    "                        'predicted_class': int(predicted_classes[j].numpy()),\n",
    "                        'probabilities': probabilities[j].numpy(),\n",
    "                        'confidence': float(probabilities[j].numpy().max()),\n",
    "                        'inference_time_ms': (batch_time / len(batch)) * 1000\n",
    "                    })\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    # Create inference engine\n",
    "    tf_engine = TensorFlowInferenceEngine()\n",
    "    \n",
    "    # Test single sample inference\n",
    "    sample = X_test[0]\n",
    "    result = tf_engine.predict_single(sample)\n",
    "    \n",
    "    print(f\"Single sample prediction:\")\n",
    "    print(f\"  Predicted class: {result['predicted_class']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"  Inference time: {result['inference_time_ms']:.2f} ms\")\n",
    "    \n",
    "    # Benchmark single vs batch inference\n",
    "    test_samples = X_test[:100]\n",
    "    \n",
    "    # Single sample approach\n",
    "    start_time = time.time()\n",
    "    single_results = [tf_engine.predict_single(sample) for sample in test_samples]\n",
    "    single_total_time = time.time() - start_time\n",
    "    \n",
    "    # Batch approach\n",
    "    start_time = time.time()\n",
    "    batch_results = tf_engine.predict_batch_optimized(test_samples, batch_size=32)\n",
    "    batch_total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n📊 Performance Comparison (100 samples):\")\n",
    "    print(f\"  Single inference: {single_total_time:.3f}s ({single_total_time/len(test_samples)*1000:.2f} ms/sample)\")\n",
    "    print(f\"  Batch inference: {batch_total_time:.3f}s ({batch_total_time/len(test_samples)*1000:.2f} ms/sample)\")\n",
    "    print(f\"  Speedup: {single_total_time/batch_total_time:.1f}x\")\n",
    "\n",
    "# Side-by-side comparison\n",
    "pytorch_inference_code = \"\"\"\n",
    "import torch\n",
    "\n",
    "class PyTorchInferenceEngine:\n",
    "    def __init__(self, model_path, device='cpu'):\n",
    "        self.device = torch.device(device)\n",
    "        self.model = torch.load(model_path, map_location=device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Warm up model\n",
    "        dummy_input = torch.randn(1, input_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(dummy_input)\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        input_tensor = torch.FloatTensor(sample.reshape(1, -1))\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_tensor)\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1)\n",
    "        \n",
    "        return {\n",
    "            'class': predicted_class.cpu().item(),\n",
    "            'probabilities': probabilities.cpu().numpy()[0],\n",
    "            'confidence': torch.max(probabilities).cpu().item()\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_inference_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class TensorFlowInferenceEngine:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Create optimized inference function\n",
    "        self.inference_func = tf.function(self.model.call)\n",
    "        \n",
    "        # Warm up model\n",
    "        dummy_input = tf.random.normal((1, input_dim))\n",
    "        _ = self.inference_func(dummy_input, training=False)\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        input_tensor = tf.constant(sample.reshape(1, -1), \n",
    "                                 dtype=tf.float32)\n",
    "        \n",
    "        probabilities = self.inference_func(input_tensor, \n",
    "                                          training=False)\n",
    "        predicted_class = tf.argmax(probabilities, axis=1)\n",
    "        \n",
    "        return {\n",
    "            'class': int(predicted_class.numpy()[0]),\n",
    "            'probabilities': probabilities.numpy()[0],\n",
    "            'confidence': float(tf.reduce_max(probabilities))\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_inference_code, tensorflow_inference_code, \"Single Sample Inference\"\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Processing Patterns\n",
    "\n",
    "Efficient patterns for processing large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BATCH PROCESSING PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate larger dataset for batch processing demo\n",
    "large_dataset = np.random.randn(1000, input_dim)\n",
    "print(f\"Large dataset shape: {large_dataset.shape}\")\n",
    "\n",
    "# PyTorch batch processing\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\n🔥 PyTorch Batch Processing:\")\n",
    "    \n",
    "    class PyTorchBatchProcessor:\n",
    "        def __init__(self, model, device='cpu', batch_size=64):\n",
    "            self.model = model\n",
    "            self.device = torch.device(device)\n",
    "            self.batch_size = batch_size\n",
    "            self.model.eval()\n",
    "        \n",
    "        def process_dataset(self, dataset: np.ndarray, \n",
    "                          progress_callback=None) -> Dict[str, Any]:\n",
    "            \"\"\"Process entire dataset in batches\"\"\"\n",
    "            all_predictions = []\n",
    "            all_probabilities = []\n",
    "            processing_times = []\n",
    "            \n",
    "            total_batches = (len(dataset) + self.batch_size - 1) // self.batch_size\n",
    "            \n",
    "            for i, batch_start in enumerate(range(0, len(dataset), self.batch_size)):\n",
    "                batch_end = min(batch_start + self.batch_size, len(dataset))\n",
    "                batch = dataset[batch_start:batch_end]\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Convert to tensor\n",
    "                batch_tensor = torch.FloatTensor(batch).to(self.device)\n",
    "                \n",
    "                # Inference\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(batch_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predictions = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                # Store results\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities.cpu().numpy())\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                processing_times.append(batch_time)\n",
    "                \n",
    "                # Progress callback\n",
    "                if progress_callback:\n",
    "                    progress_callback(i + 1, total_batches, batch_time)\n",
    "            \n",
    "            return {\n",
    "                'predictions': np.array(all_predictions),\n",
    "                'probabilities': np.array(all_probabilities),\n",
    "                'total_time': sum(processing_times),\n",
    "                'avg_batch_time': np.mean(processing_times),\n",
    "                'throughput_samples_per_sec': len(dataset) / sum(processing_times)\n",
    "            }\n",
    "        \n",
    "        def process_streaming(self, data_generator, max_samples=None):\n",
    "            \"\"\"Process streaming data\"\"\"\n",
    "            batch_buffer = []\n",
    "            processed_count = 0\n",
    "            \n",
    "            for sample in data_generator:\n",
    "                batch_buffer.append(sample)\n",
    "                \n",
    "                # Process when batch is full\n",
    "                if len(batch_buffer) >= self.batch_size:\n",
    "                    batch = np.array(batch_buffer)\n",
    "                    batch_tensor = torch.FloatTensor(batch).to(self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        logits = self.model(batch_tensor)\n",
    "                        probabilities = torch.softmax(logits, dim=1)\n",
    "                        predictions = torch.argmax(probabilities, dim=1)\n",
    "                    \n",
    "                    # Yield results\n",
    "                    for i, (pred, prob) in enumerate(zip(predictions.cpu().numpy(), \n",
    "                                                       probabilities.cpu().numpy())):\n",
    "                        yield {\n",
    "                            'prediction': pred,\n",
    "                            'probabilities': prob,\n",
    "                            'sample_index': processed_count + i\n",
    "                        }\n",
    "                    \n",
    "                    processed_count += len(batch_buffer)\n",
    "                    batch_buffer = []\n",
    "                    \n",
    "                    if max_samples and processed_count >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            # Process remaining samples\n",
    "            if batch_buffer:\n",
    "                batch = np.array(batch_buffer)\n",
    "                batch_tensor = torch.FloatTensor(batch).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(batch_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predictions = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                for i, (pred, prob) in enumerate(zip(predictions.cpu().numpy(), \n",
    "                                                   probabilities.cpu().numpy())):\n",
    "                    yield {\n",
    "                        'prediction': pred,\n",
    "                        'probabilities': prob,\n",
    "                        'sample_index': processed_count + i\n",
    "                    }\n",
    "    \n",
    "    # Create batch processor\n",
    "    pt_processor = PyTorchBatchProcessor(pt_engine.model, batch_size=64)\n",
    "    \n",
    "    # Process dataset with progress tracking\n",
    "    def progress_callback(batch_num, total_batches, batch_time):\n",
    "        if batch_num % 5 == 0 or batch_num == total_batches:\n",
    "            print(f\"  Batch {batch_num}/{total_batches} - {batch_time:.3f}s\")\n",
    "    \n",
    "    print(\"Processing large dataset...\")\n",
    "    results = pt_processor.process_dataset(large_dataset, progress_callback)\n",
    "    \n",
    "    print(f\"\\n📊 Batch Processing Results:\")\n",
    "    print(f\"  Total samples: {len(results['predictions'])}\")\n",
    "    print(f\"  Total time: {results['total_time']:.3f}s\")\n",
    "    print(f\"  Average batch time: {results['avg_batch_time']:.3f}s\")\n",
    "    print(f\"  Throughput: {results['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "    \n",
    "    # Demonstrate streaming processing\n",
    "    def data_generator():\n",
    "        \"\"\"Simulate streaming data\"\"\"\n",
    "        for i in range(200):\n",
    "            yield np.random.randn(input_dim)\n",
    "    \n",
    "    print(\"\\n🌊 Streaming Processing (first 10 results):\")\n",
    "    stream_results = list(pt_processor.process_streaming(data_generator(), max_samples=100))\n",
    "    \n",
    "    for i, result in enumerate(stream_results[:10]):\n",
    "        print(f\"  Sample {result['sample_index']}: Class {result['prediction']} \"\n",
    "              f\"(confidence: {result['probabilities'].max():.3f})\")\n",
    "    \n",
    "    print(f\"  ... processed {len(stream_results)} total samples\")\n",
    "\n",
    "# TensorFlow batch processing\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n🟠 TensorFlow Batch Processing:\")\n",
    "    \n",
    "    class TensorFlowBatchProcessor:\n",
    "        def __init__(self, model, batch_size=64):\n",
    "            self.model = model\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "            # Create optimized inference function\n",
    "            self.inference_func = tf.function(self.model.call)\n",
    "        \n",
    "        def process_dataset(self, dataset: np.ndarray, \n",
    "                          progress_callback=None) -> Dict[str, Any]:\n",
    "            \"\"\"Process entire dataset in batches\"\"\"\n",
    "            all_predictions = []\n",
    "            all_probabilities = []\n",
    "            processing_times = []\n",
    "            \n",
    "            total_batches = (len(dataset) + self.batch_size - 1) // self.batch_size\n",
    "            \n",
    "            for i, batch_start in enumerate(range(0, len(dataset), self.batch_size)):\n",
    "                batch_end = min(batch_start + self.batch_size, len(dataset))\n",
    "                batch = dataset[batch_start:batch_end]\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Convert to tensor\n",
    "                batch_tensor = tf.constant(batch, dtype=tf.float32)\n",
    "                \n",
    "                # Inference\n",
    "                probabilities = self.inference_func(batch_tensor, training=False)\n",
    "                predictions = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                # Store results\n",
    "                all_predictions.extend(predictions.numpy())\n",
    "                all_probabilities.extend(probabilities.numpy())\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                processing_times.append(batch_time)\n",
    "                \n",
    "                # Progress callback\n",
    "                if progress_callback:\n",
    "                    progress_callback(i + 1, total_batches, batch_time)\n",
    "            \n",
    "            return {\n",
    "                'predictions': np.array(all_predictions),\n",
    "                'probabilities': np.array(all_probabilities),\n",
    "                'total_time': sum(processing_times),\n",
    "                'avg_batch_time': np.mean(processing_times),\n",
    "                'throughput_samples_per_sec': len(dataset) / sum(processing_times)\n",
    "            }\n",
    "        \n",
    "        def process_tf_dataset(self, tf_dataset):\n",
    "            \"\"\"Process TensorFlow dataset efficiently\"\"\"\n",
    "            all_predictions = []\n",
    "            all_probabilities = []\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            for batch in tf_dataset:\n",
    "                probabilities = self.inference_func(batch, training=False)\n",
    "                predictions = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                all_predictions.extend(predictions.numpy())\n",
    "                all_probabilities.extend(probabilities.numpy())\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'predictions': np.array(all_predictions),\n",
    "                'probabilities': np.array(all_probabilities),\n",
    "                'total_time': total_time,\n",
    "                'throughput_samples_per_sec': len(all_predictions) / total_time\n",
    "            }\n",
    "    \n",
    "    # Create batch processor\n",
    "    tf_processor = TensorFlowBatchProcessor(tf_engine.model, batch_size=64)\n",
    "    \n",
    "    # Process dataset with progress tracking\n",
    "    print(\"Processing large dataset...\")\n",
    "    results = tf_processor.process_dataset(large_dataset, progress_callback)\n",
    "    \n",
    "    print(f\"\\n📊 Batch Processing Results:\")\n",
    "    print(f\"  Total samples: {len(results['predictions'])}\")\n",
    "    print(f\"  Total time: {results['total_time']:.3f}s\")\n",
    "    print(f\"  Average batch time: {results['avg_batch_time']:.3f}s\")\n",
    "    print(f\"  Throughput: {results['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "    \n",
    "    # Demonstrate TensorFlow dataset processing\n",
    "    print(\"\\n🗂️ TensorFlow Dataset Processing:\")\n",
    "    \n",
    "    # Create TensorFlow dataset\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(large_dataset[:500])\n",
    "    tf_dataset = tf_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset_results = tf_processor.process_tf_dataset(tf_dataset)\n",
    "    \n",
    "    print(f\"  Processed {len(dataset_results['predictions'])} samples\")\n",
    "    print(f\"  Total time: {dataset_results['total_time']:.3f}s\")\n",
    "    print(f\"  Throughput: {dataset_results['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "\n",
    "print(\"\\n💡 Batch Processing Best Practices:\")\n",
    "print(\"• Use appropriate batch sizes (32-128 typically optimal)\")\n",
    "print(\"• Implement progress tracking for long-running jobs\")\n",
    "print(\"• Consider memory constraints when setting batch size\")\n",
    "print(\"• Use streaming processing for very large datasets\")\n",
    "print(\"• Optimize data loading and preprocessing pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concurrent Inference Patterns\n",
    "\n",
    "Handling multiple concurrent inference requests efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONCURRENT INFERENCE PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch concurrent inference\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\n🔥 PyTorch Concurrent Inference:\")\n",
    "    \n",
    "    class PyTorchConcurrentInference:\n",
    "        def __init__(self, model, device='cpu', max_workers=4):\n",
    "            self.device = torch.device(device)\n",
    "            self.max_workers = max_workers\n",
    "            \n",
    "            # Create multiple model instances for thread safety\n",
    "            self.models = []\n",
    "            for _ in range(max_workers):\n",
    "                model_copy = type(model)(input_dim, 64, num_classes).to(device)\n",
    "                model_copy.load_state_dict(model.state_dict())\n",
    "                model_copy.eval()\n",
    "                self.models.append(model_copy)\n",
    "            \n",
    "            self.model_queue = queue.Queue()\n",
    "            for model in self.models:\n",
    "                self.model_queue.put(model)\n",
    "        \n",
    "        def predict_single_threadsafe(self, sample: np.ndarray, request_id: str = None) -> Dict[str, Any]:\n",
    "            \"\"\"Thread-safe single prediction\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Get model from queue\n",
    "            model = self.model_queue.get()\n",
    "            \n",
    "            try:\n",
    "                # Preprocessing\n",
    "                input_tensor = torch.FloatTensor(sample.reshape(1, -1)).to(self.device)\n",
    "                \n",
    "                # Inference\n",
    "                with torch.no_grad():\n",
    "                    logits = model(input_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                result = {\n",
    "                    'request_id': request_id,\n",
    "                    'predicted_class': predicted_class.cpu().item(),\n",
    "                    'probabilities': probabilities.cpu().numpy()[0],\n",
    "                    'confidence': torch.max(probabilities).cpu().item(),\n",
    "                    'inference_time_ms': (time.time() - start_time) * 1000,\n",
    "                    'thread_id': threading.current_thread().ident\n",
    "                }\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            finally:\n",
    "                # Return model to queue\n",
    "                self.model_queue.put(model)\n",
    "        \n",
    "        def process_concurrent_requests(self, samples: List[np.ndarray]) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Process multiple requests concurrently\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                # Submit all requests\n",
    "                futures = []\n",
    "                for i, sample in enumerate(samples):\n",
    "                    future = executor.submit(self.predict_single_threadsafe, sample, f\"req_{i}\")\n",
    "                    futures.append(future)\n",
    "                \n",
    "                # Collect results\n",
    "                results = []\n",
    "                for future in futures:\n",
    "                    results.append(future.result())\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Add summary statistics\n",
    "            for result in results:\n",
    "                result['total_processing_time'] = total_time\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    # Create concurrent inference engine\n",
    "    pt_concurrent = PyTorchConcurrentInference(pt_engine.model, max_workers=4)\n",
    "    \n",
    "    # Test concurrent processing\n",
    "    test_samples = [X_test[i] for i in range(20)]\n",
    "    \n",
    "    # Sequential processing (baseline)\n",
    "    start_time = time.time()\n",
    "    sequential_results = [pt_engine.predict_single(sample) for sample in test_samples]\n",
    "    sequential_time = time.time() - start_time\n",
    "    \n",
    "    # Concurrent processing\n",
    "    concurrent_results = pt_concurrent.process_concurrent_requests(test_samples)\n",
    "    concurrent_time = concurrent_results[0]['total_processing_time']\n",
    "    \n",
    "    print(f\"📊 Concurrent Processing Comparison ({len(test_samples)} samples):\")\n",
    "    print(f\"  Sequential: {sequential_time:.3f}s\")\n",
    "    print(f\"  Concurrent: {concurrent_time:.3f}s\")\n",
    "    print(f\"  Speedup: {sequential_time/concurrent_time:.1f}x\")\n",
    "    \n",
    "    # Show thread distribution\n",
    "    thread_ids = [result['thread_id'] for result in concurrent_results]\n",
    "    unique_threads = len(set(thread_ids))\n",
    "    print(f\"  Used {unique_threads} different threads\")\n",
    "    \n",
    "    # Show first few results\n",
    "    print(\"\\nFirst 5 concurrent results:\")\n",
    "    for result in concurrent_results[:5]:\n",
    "        print(f\"  {result['request_id']}: Class {result['predicted_class']} \"\n",
    "              f\"({result['inference_time_ms']:.1f}ms, thread {result['thread_id']})\")\n",
    "\n",
    "# TensorFlow concurrent inference\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n🟠 TensorFlow Concurrent Inference:\")\n",
    "    \n",
    "    class TensorFlowConcurrentInference:\n",
    "        def __init__(self, model, max_workers=4):\n",
    "            self.model = model\n",
    "            self.max_workers = max_workers\n",
    "            \n",
    "            # TensorFlow models are generally thread-safe for inference\n",
    "            # Create optimized inference function\n",
    "            self.inference_func = tf.function(self.model.call)\n",
    "            \n",
    "            # Thread-local storage for any thread-specific data\n",
    "            self.thread_local = threading.local()\n",
    "        \n",
    "        def predict_single_threadsafe(self, sample: np.ndarray, request_id: str = None) -> Dict[str, Any]:\n",
    "            \"\"\"Thread-safe single prediction\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Preprocessing\n",
    "            input_tensor = tf.constant(sample.reshape(1, -1), dtype=tf.float32)\n",
    "            \n",
    "            # Inference (TensorFlow handles thread safety)\n",
    "            probabilities = self.inference_func(input_tensor, training=False)\n",
    "            predicted_class = tf.argmax(probabilities, axis=1)\n",
    "            \n",
    "            result = {\n",
    "                'request_id': request_id,\n",
    "                'predicted_class': int(predicted_class.numpy()[0]),\n",
    "                'probabilities': probabilities.numpy()[0],\n",
    "                'confidence': float(tf.reduce_max(probabilities).numpy()),\n",
    "                'inference_time_ms': (time.time() - start_time) * 1000,\n",
    "                'thread_id': threading.current_thread().ident\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def process_concurrent_requests(self, samples: List[np.ndarray]) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Process multiple requests concurrently\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                # Submit all requests\n",
    "                futures = []\n",
    "                for i, sample in enumerate(samples):\n",
    "                    future = executor.submit(self.predict_single_threadsafe, sample, f\"req_{i}\")\n",
    "                    futures.append(future)\n",
    "                \n",
    "                # Collect results\n",
    "                results = []\n",
    "                for future in futures:\n",
    "                    results.append(future.result())\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Add summary statistics\n",
    "            for result in results:\n",
    "                result['total_processing_time'] = total_time\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        def process_batch_concurrent(self, samples: List[np.ndarray], batch_size: int = 8) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Process samples in concurrent batches\"\"\"\n",
    "            def process_batch(batch_samples, batch_id):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Stack samples into batch\n",
    "                batch_array = np.array(batch_samples)\n",
    "                batch_tensor = tf.constant(batch_array, dtype=tf.float32)\n",
    "                \n",
    "                # Batch inference\n",
    "                probabilities = self.inference_func(batch_tensor, training=False)\n",
    "                predictions = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                # Convert to individual results\n",
    "                results = []\n",
    "                for i in range(len(batch_samples)):\n",
    "                    results.append({\n",
    "                        'request_id': f\"batch_{batch_id}_sample_{i}\",\n",
    "                        'predicted_class': int(predictions[i].numpy()),\n",
    "                        'probabilities': probabilities[i].numpy(),\n",
    "                        'confidence': float(probabilities[i].numpy().max()),\n",
    "                        'batch_time_ms': batch_time * 1000,\n",
    "                        'thread_id': threading.current_thread().ident\n",
    "                    })\n",
    "                \n",
    "                return results\n",
    "            \n",
    "            # Split into batches\n",
    "            batches = [samples[i:i + batch_size] for i in range(0, len(samples), batch_size)]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                # Submit batch processing tasks\n",
    "                futures = []\n",
    "                for i, batch in enumerate(batches):\n",
    "                    future = executor.submit(process_batch, batch, i)\n",
    "                    futures.append(future)\n",
    "                \n",
    "                # Collect results\n",
    "                all_results = []\n",
    "                for future in futures:\n",
    "                    batch_results = future.result()\n",
    "                    all_results.extend(batch_results)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Add total processing time\n",
    "            for result in all_results:\n",
    "                result['total_processing_time'] = total_time\n",
    "            \n",
    "            return all_results\n",
    "    \n",
    "    # Create concurrent inference engine\n",
    "    tf_concurrent = TensorFlowConcurrentInference(tf_engine.model, max_workers=4)\n",
    "    \n",
    "    # Test concurrent processing\n",
    "    test_samples = [X_test[i] for i in range(20)]\n",
    "    \n",
    "    # Sequential processing (baseline)\n",
    "    start_time = time.time()\n",
    "    sequential_results = [tf_engine.predict_single(sample) for sample in test_samples]\n",
    "    sequential_time = time.time() - start_time\n",
    "    \n",
    "    # Concurrent processing\n",
    "    concurrent_results = tf_concurrent.process_concurrent_requests(test_samples)\n",
    "    concurrent_time = concurrent_results[0]['total_processing_time']\n",
    "    \n",
    "    # Batch concurrent processing\n",
    "    batch_concurrent_results = tf_concurrent.process_batch_concurrent(test_samples, batch_size=8)\n",
    "    batch_concurrent_time = batch_concurrent_results[0]['total_processing_time']\n",
    "    \n",
    "    print(f\"📊 Concurrent Processing Comparison ({len(test_samples)} samples):\")\n",
    "    print(f\"  Sequential: {sequential_time:.3f}s\")\n",
    "    print(f\"  Concurrent (single): {concurrent_time:.3f}s (speedup: {sequential_time/concurrent_time:.1f}x)\")\n",
    "    print(f\"  Concurrent (batch): {batch_concurrent_time:.3f}s (speedup: {sequential_time/batch_concurrent_time:.1f}x)\")\n",
    "    \n",
    "    # Show thread distribution\n",
    "    thread_ids = [result['thread_id'] for result in concurrent_results]\n",
    "    unique_threads = len(set(thread_ids))\n",
    "    print(f\"  Used {unique_threads} different threads\")\n",
    "    \n",
    "    # Show first few results\n",
    "    print(\"\\nFirst 5 batch concurrent results:\")\n",
    "    for result in batch_concurrent_results[:5]:\n",
    "        print(f\"  {result['request_id']}: Class {result['predicted_class']} \"\n",
    "              f\"({result['batch_time_ms']:.1f}ms batch time, thread {result['thread_id']})\")\n",
    "\n",
    "print(\"\\n🚀 Concurrent Inference Best Practices:\")\n",
    "print(\"• Use thread pools to manage concurrent requests\")\n",
    "print(\"• Consider model thread safety (PyTorch needs multiple instances)\")\n",
    "print(\"• Batch concurrent requests when possible for better throughput\")\n",
    "print(\"• Monitor resource usage (CPU, memory, GPU)\")\n",
    "print(\"• Implement proper error handling and timeouts\")\n",
    "print(\"• Use async patterns for I/O-bound operations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
