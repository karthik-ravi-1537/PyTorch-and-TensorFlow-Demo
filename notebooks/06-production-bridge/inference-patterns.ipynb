{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Patterns: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master batch and real-time inference patterns\n",
    "- Learn optimization techniques for inference performance\n",
    "- Compare preprocessing and postprocessing approaches\n",
    "- Understand memory management during inference\n",
    "\n",
    "**Prerequisites:** Model serialization, framework fundamentals\n",
    "\n",
    "**Estimated Time:** 35 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Dict, Any, Union\n",
    "import threading\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from foundations.data_utils import get_tutorial_tabular_data, get_tutorial_text_data\n",
    "from foundations.preprocessing import TextPreprocessor\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"âœ… PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"âŒ PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"âœ… TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"âŒ TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Sample Inference\n",
    "\n",
    "Optimized patterns for real-time, single-sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SINGLE SAMPLE INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data\n",
    "tabular_data = get_tutorial_tabular_data()\n",
    "X_test = tabular_data['X_test']\n",
    "y_test = tabular_data['y_test']\n",
    "\n",
    "input_dim = X_test.shape[1]\n",
    "num_classes = len(np.unique(y_test))\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# PyTorch single sample inference\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nðŸ”¥ PyTorch Single Sample Inference:\")\n",
    "    \n",
    "    class PyTorchInferenceEngine:\n",
    "        def __init__(self, model_path: str, device: str = 'cpu'):\n",
    "            self.device = torch.device(device)\n",
    "            self.model = self._load_model(model_path)\n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "            \n",
    "            # Warm up the model\n",
    "            self._warmup()\n",
    "        \n",
    "        def _load_model(self, model_path: str):\n",
    "            \"\"\"Load model from checkpoint\"\"\"\n",
    "            # Define model architecture (in practice, this would be imported)\n",
    "            class SimpleClassifier(nn.Module):\n",
    "                def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "                    super().__init__()\n",
    "                    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "                    self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "                    self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "                    self.relu = nn.ReLU()\n",
    "                    self.dropout = nn.Dropout(0.2)\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    x = self.relu(self.fc1(x))\n",
    "                    x = self.dropout(x)\n",
    "                    x = self.relu(self.fc2(x))\n",
    "                    x = self.dropout(x)\n",
    "                    x = self.fc3(x)\n",
    "                    return x\n",
    "            \n",
    "            # Create and load model\n",
    "            model = SimpleClassifier(input_dim, 64, num_classes).to(self.device)\n",
    "            \n",
    "            # For demo, create a trained model\n",
    "            # In practice, you'd load from saved checkpoint\n",
    "            return model\n",
    "        \n",
    "        def _warmup(self, num_warmup: int = 5):\n",
    "            \"\"\"Warm up the model with dummy inputs\"\"\"\n",
    "            dummy_input = torch.randn(1, input_dim).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(num_warmup):\n",
    "                    _ = self.model(dummy_input)\n",
    "        \n",
    "        def predict_single(self, sample: np.ndarray) -> Dict[str, Any]:\n",
    "            \"\"\"Predict single sample with timing\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Preprocessing\n",
    "            input_tensor = torch.FloatTensor(sample.reshape(1, -1)).to(self.device)\n",
    "            \n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(input_tensor)\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                predicted_class = torch.argmax(probabilities, dim=1)\n",
    "            \n",
    "            # Postprocessing\n",
    "            result = {\n",
    "                'predicted_class': predicted_class.cpu().item(),\n",
    "                'probabilities': probabilities.cpu().numpy()[0],\n",
    "                'confidence': torch.max(probabilities).cpu().item(),\n",
    "                'inference_time_ms': (time.time() - start_time) * 1000\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def predict_batch_optimized(self, samples: np.ndarray, batch_size: int = 32) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Optimized batch prediction\"\"\"\n",
    "            results = []\n",
    "            \n",
    "            for i in range(0, len(samples), batch_size):\n",
    "                batch = samples[i:i + batch_size]\n",
    "                batch_tensor = torch.FloatTensor(batch).to(self.device)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(batch_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predicted_classes = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                # Convert to individual results\n",
    "                for j in range(len(batch)):\n",
    "                    results.append({\n",
    "                        'predicted_class': predicted_classes[j].cpu().item(),\n",
    "                        'probabilities': probabilities[j].cpu().numpy(),\n",
    "                        'confidence': probabilities[j].max().cpu().item(),\n",
    "                        'inference_time_ms': (batch_time / len(batch)) * 1000\n",
    "                    })\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    # Create inference engine\n",
    "    pt_engine = PyTorchInferenceEngine('dummy_path')\n",
    "    \n",
    "    # Test single sample inference\n",
    "    sample = X_test[0]\n",
    "    result = pt_engine.predict_single(sample)\n",
    "    \n",
    "    print(f\"Single sample prediction:\")\n",
    "    print(f\"  Predicted class: {result['predicted_class']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"  Inference time: {result['inference_time_ms']:.2f} ms\")\n",
    "    \n",
    "    # Benchmark single vs batch inference\n",
    "    test_samples = X_test[:100]\n",
    "    \n",
    "    # Single sample approach\n",
    "    start_time = time.time()\n",
    "    single_results = [pt_engine.predict_single(sample) for sample in test_samples]\n",
    "    single_total_time = time.time() - start_time\n",
    "    \n",
    "    # Batch approach\n",
    "    start_time = time.time()\n",
    "    batch_results = pt_engine.predict_batch_optimized(test_samples, batch_size=32)\n",
    "    batch_total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Performance Comparison (100 samples):\")\n",
    "    print(f\"  Single inference: {single_total_time:.3f}s ({single_total_time/len(test_samples)*1000:.2f} ms/sample)\")\n",
    "    print(f\"  Batch inference: {batch_total_time:.3f}s ({batch_total_time/len(test_samples)*1000:.2f} ms/sample)\")\n",
    "    print(f\"  Speedup: {single_total_time/batch_total_time:.1f}x\")\n",
    "\n",
    "# TensorFlow single sample inference\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nðŸŸ  TensorFlow Single Sample Inference:\")\n",
    "    \n",
    "    class TensorFlowInferenceEngine:\n",
    "        def __init__(self, model_path: str = None):\n",
    "            self.model = self._create_model()  # In practice, load from saved model\n",
    "            \n",
    "            # Compile for inference optimization\n",
    "            self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "            \n",
    "            # Create inference function for better performance\n",
    "            self.inference_func = tf.function(self.model.call)\n",
    "            \n",
    "            # Warm up\n",
    "            self._warmup()\n",
    "        \n",
    "        def _create_model(self):\n",
    "            \"\"\"Create model (in practice, load from saved model)\"\"\"\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "            return model\n",
    "        \n",
    "        def _warmup(self, num_warmup: int = 5):\n",
    "            \"\"\"Warm up the model\"\"\"\n",
    "            dummy_input = tf.random.normal((1, input_dim))\n",
    "            \n",
    "            for _ in range(num_warmup):\n",
    "                _ = self.inference_func(dummy_input, training=False)\n",
    "        \n",
    "        def predict_single(self, sample: np.ndarray) -> Dict[str, Any]:\n",
    "            \"\"\"Predict single sample with timing\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Preprocessing\n",
    "            input_tensor = tf.constant(sample.reshape(1, -1), dtype=tf.float32)\n",
    "            \n",
    "            # Inference\n",
    "            probabilities = self.inference_func(input_tensor, training=False)\n",
    "            predicted_class = tf.argmax(probabilities, axis=1)\n",
    "            \n",
    "            # Postprocessing\n",
    "            result = {\n",
    "                'predicted_class': int(predicted_class.numpy()[0]),\n",
    "                'probabilities': probabilities.numpy()[0],\n",
    "                'confidence': float(tf.reduce_max(probabilities).numpy()),\n",
    "                'inference_time_ms': (time.time() - start_time) * 1000\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def predict_batch_optimized(self, samples: np.ndarray, batch_size: int = 32) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Optimized batch prediction\"\"\"\n",
    "            results = []\n",
    "            \n",
    "            for i in range(0, len(samples), batch_size):\n",
    "                batch = samples[i:i + batch_size]\n",
    "                batch_tensor = tf.constant(batch, dtype=tf.float32)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                probabilities = self.inference_func(batch_tensor, training=False)\n",
    "                predicted_classes = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                # Convert to individual results\n",
    "                for j in range(len(batch)):\n",
    "                    results.append({\n",
    "                        'predicted_class': int(predicted_classes[j].numpy()),\n",
    "                        'probabilities': probabilities[j].numpy(),\n",
    "                        'confidence': float(probabilities[j].numpy().max()),\n",
    "                        'inference_time_ms': (batch_time / len(batch)) * 1000\n",
    "                    })\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    # Create inference engine\n",
    "    tf_engine = TensorFlowInferenceEngine()\n",
    "    \n",
    "    # Test single sample inference\n",
    "    sample = X_test[0]\n",
    "    result = tf_engine.predict_single(sample)\n",
    "    \n",
    "    print(f\"Single sample prediction:\")\n",
    "    print(f\"  Predicted class: {result['predicted_class']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"  Inference time: {result['inference_time_ms']:.2f} ms\")\n",
    "    \n",
    "    # Benchmark single vs batch inference\n",
    "    test_samples = X_test[:100]\n",
    "    \n",
    "    # Single sample approach\n",
    "    start_time = time.time()\n",
    "    single_results = [tf_engine.predict_single(sample) for sample in test_samples]\n",
    "    single_total_time = time.time() - start_time\n",
    "    \n",
    "    # Batch approach\n",
    "    start_time = time.time()\n",
    "    batch_results = tf_engine.predict_batch_optimized(test_samples, batch_size=32)\n",
    "    batch_total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Performance Comparison (100 samples):\")\n",
    "    print(f\"  Single inference: {single_total_time:.3f}s ({single_total_time/len(test_samples)*1000:.2f} ms/sample)\")\n",
    "    print(f\"  Batch inference: {batch_total_time:.3f}s ({batch_total_time/len(test_samples)*1000:.2f} ms/sample)\")\n",
    "    print(f\"  Speedup: {single_total_time/batch_total_time:.1f}x\")\n",
    "\n",
    "# Side-by-side comparison\n",
    "pytorch_inference_code = \"\"\"\n",
    "import torch\n",
    "\n",
    "class PyTorchInferenceEngine:\n",
    "    def __init__(self, model_path, device='cpu'):\n",
    "        self.device = torch.device(device)\n",
    "        self.model = torch.load(model_path, map_location=device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Warm up model\n",
    "        dummy_input = torch.randn(1, input_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(dummy_input)\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        input_tensor = torch.FloatTensor(sample.reshape(1, -1))\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_tensor)\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1)\n",
    "        \n",
    "        return {\n",
    "            'class': predicted_class.cpu().item(),\n",
    "            'probabilities': probabilities.cpu().numpy()[0],\n",
    "            'confidence': torch.max(probabilities).cpu().item()\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_inference_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "class TensorFlowInferenceEngine:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Create optimized inference function\n",
    "        self.inference_func = tf.function(self.model.call)\n",
    "        \n",
    "        # Warm up model\n",
    "        dummy_input = tf.random.normal((1, input_dim))\n",
    "        _ = self.inference_func(dummy_input, training=False)\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        input_tensor = tf.constant(sample.reshape(1, -1), \n",
    "                                 dtype=tf.float32)\n",
    "        \n",
    "        probabilities = self.inference_func(input_tensor, \n",
    "                                          training=False)\n",
    "        predicted_class = tf.argmax(probabilities, axis=1)\n",
    "        \n",
    "        return {\n",
    "            'class': int(predicted_class.numpy()[0]),\n",
    "            'probabilities': probabilities.numpy()[0],\n",
    "            'confidence': float(tf.reduce_max(probabilities))\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_inference_code, tensorflow_inference_code, \"Single Sample Inference\"\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Processing Patterns\n",
    "\n",
    "Efficient patterns for processing large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BATCH PROCESSING PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate larger dataset for batch processing demo\n",
    "large_dataset = np.random.randn(1000, input_dim)\n",
    "print(f\"Large dataset shape: {large_dataset.shape}\")\n",
    "\n",
    "# PyTorch batch processing\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nðŸ”¥ PyTorch Batch Processing:\")\n",
    "    \n",
    "    class PyTorchBatchProcessor:\n",
    "        def __init__(self, model, device='cpu', batch_size=64):\n",
    "            self.model = model\n",
    "            self.device = torch.device(device)\n",
    "            self.batch_size = batch_size\n",
    "            self.model.eval()\n",
    "        \n",
    "        def process_dataset(self, dataset: np.ndarray, \n",
    "                          progress_callback=None) -> Dict[str, Any]:\n",
    "            \"\"\"Process entire dataset in batches\"\"\"\n",
    "            all_predictions = []\n",
    "            all_probabilities = []\n",
    "            processing_times = []\n",
    "            \n",
    "            total_batches = (len(dataset) + self.batch_size - 1) // self.batch_size\n",
    "            \n",
    "            for i, batch_start in enumerate(range(0, len(dataset), self.batch_size)):\n",
    "                batch_end = min(batch_start + self.batch_size, len(dataset))\n",
    "                batch = dataset[batch_start:batch_end]\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Convert to tensor\n",
    "                batch_tensor = torch.FloatTensor(batch).to(self.device)\n",
    "                \n",
    "                # Inference\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(batch_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predictions = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                # Store results\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities.cpu().numpy())\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                processing_times.append(batch_time)\n",
    "                \n",
    "                # Progress callback\n",
    "                if progress_callback:\n",
    "                    progress_callback(i + 1, total_batches, batch_time)\n",
    "            \n",
    "            return {\n",
    "                'predictions': np.array(all_predictions),\n",
    "                'probabilities': np.array(all_probabilities),\n",
    "                'total_time': sum(processing_times),\n",
    "                'avg_batch_time': np.mean(processing_times),\n",
    "                'throughput_samples_per_sec': len(dataset) / sum(processing_times)\n",
    "            }\n",
    "        \n",
    "        def process_streaming(self, data_generator, max_samples=None):\n",
    "            \"\"\"Process streaming data\"\"\"\n",
    "            batch_buffer = []\n",
    "            processed_count = 0\n",
    "            \n",
    "            for sample in data_generator:\n",
    "                batch_buffer.append(sample)\n",
    "                \n",
    "                # Process when batch is full\n",
    "                if len(batch_buffer) >= self.batch_size:\n",
    "                    batch = np.array(batch_buffer)\n",
    "                    batch_tensor = torch.FloatTensor(batch).to(self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        logits = self.model(batch_tensor)\n",
    "                        probabilities = torch.softmax(logits, dim=1)\n",
    "                        predictions = torch.argmax(probabilities, dim=1)\n",
    "                    \n",
    "                    # Yield results\n",
    "                    for i, (pred, prob) in enumerate(zip(predictions.cpu().numpy(), \n",
    "                                                       probabilities.cpu().numpy())):\n",
    "                        yield {\n",
    "                            'prediction': pred,\n",
    "                            'probabilities': prob,\n",
    "                            'sample_index': processed_count + i\n",
    "                        }\n",
    "                    \n",
    "                    processed_count += len(batch_buffer)\n",
    "                    batch_buffer = []\n",
    "                    \n",
    "                    if max_samples and processed_count >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            # Process remaining samples\n",
    "            if batch_buffer:\n",
    "                batch = np.array(batch_buffer)\n",
    "                batch_tensor = torch.FloatTensor(batch).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(batch_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predictions = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                for i, (pred, prob) in enumerate(zip(predictions.cpu().numpy(), \n",
    "                                                   probabilities.cpu().numpy())):\n",
    "                    yield {\n",
    "                        'prediction': pred,\n",
    "                        'probabilities': prob,\n",
    "                        'sample_index': processed_count + i\n",
    "                    }\n",
    "    \n",
    "    # Create batch processor\n",
    "    pt_processor = PyTorchBatchProcessor(pt_engine.model, batch_size=64)\n",
    "    \n",
    "    # Process dataset with progress tracking\n",
    "    def progress_callback(batch_num, total_batches, batch_time):\n",
    "        if batch_num % 5 == 0 or batch_num == total_batches:\n",
    "            print(f\"  Batch {batch_num}/{total_batches} - {batch_time:.3f}s\")\n",
    "    \n",
    "    print(\"Processing large dataset...\")\n",
    "    results = pt_processor.process_dataset(large_dataset, progress_callback)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Batch Processing Results:\")\n",
    "    print(f\"  Total samples: {len(results['predictions'])}\")\n",
    "    print(f\"  Total time: {results['total_time']:.3f}s\")\n",
    "    print(f\"  Average batch time: {results['avg_batch_time']:.3f}s\")\n",
    "    print(f\"  Throughput: {results['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "    \n",
    "    # Demonstrate streaming processing\n",
    "    def data_generator():\n",
    "        \"\"\"Simulate streaming data\"\"\"\n",
    "        for i in range(200):\n",
    "            yield np.random.randn(input_dim)\n",
    "    \n",
    "    print(\"\\nðŸŒŠ Streaming Processing (first 10 results):\")\n",
    "    stream_results = list(pt_processor.process_streaming(data_generator(), max_samples=100))\n",
    "    \n",
    "    for i, result in enumerate(stream_results[:10]):\n",
    "        print(f\"  Sample {result['sample_index']}: Class {result['prediction']} \"\n",
    "              f\"(confidence: {result['probabilities'].max():.3f})\")\n",
    "    \n",
    "    print(f\"  ... processed {len(stream_results)} total samples\")\n",
    "\n",
    "# TensorFlow batch processing\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nðŸŸ  TensorFlow Batch Processing:\")\n",
    "    \n",
    "    class TensorFlowBatchProcessor:\n",
    "        def __init__(self, model, batch_size=64):\n",
    "            self.model = model\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "            # Create optimized inference function\n",
    "            self.inference_func = tf.function(self.model.call)\n",
    "        \n",
    "        def process_dataset(self, dataset: np.ndarray, \n",
    "                          progress_callback=None) -> Dict[str, Any]:\n",
    "            \"\"\"Process entire dataset in batches\"\"\"\n",
    "            all_predictions = []\n",
    "            all_probabilities = []\n",
    "            processing_times = []\n",
    "            \n",
    "            total_batches = (len(dataset) + self.batch_size - 1) // self.batch_size\n",
    "            \n",
    "            for i, batch_start in enumerate(range(0, len(dataset), self.batch_size)):\n",
    "                batch_end = min(batch_start + self.batch_size, len(dataset))\n",
    "                batch = dataset[batch_start:batch_end]\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Convert to tensor\n",
    "                batch_tensor = tf.constant(batch, dtype=tf.float32)\n",
    "                \n",
    "                # Inference\n",
    "                probabilities = self.inference_func(batch_tensor, training=False)\n",
    "                predictions = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                # Store results\n",
    "                all_predictions.extend(predictions.numpy())\n",
    "                all_probabilities.extend(probabilities.numpy())\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                processing_times.append(batch_time)\n",
    "                \n",
    "                # Progress callback\n",
    "                if progress_callback:\n",
    "                    progress_callback(i + 1, total_batches, batch_time)\n",
    "            \n",
    "            return {\n",
    "                'predictions': np.array(all_predictions),\n",
    "                'probabilities': np.array(all_probabilities),\n",
    "                'total_time': sum(processing_times),\n",
    "                'avg_batch_time': np.mean(processing_times),\n",
    "                'throughput_samples_per_sec': len(dataset) / sum(processing_times)\n",
    "            }\n",
    "        \n",
    "        def process_tf_dataset(self, tf_dataset):\n",
    "            \"\"\"Process TensorFlow dataset efficiently\"\"\"\n",
    "            all_predictions = []\n",
    "            all_probabilities = []\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            for batch in tf_dataset:\n",
    "                probabilities = self.inference_func(batch, training=False)\n",
    "                predictions = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                all_predictions.extend(predictions.numpy())\n",
    "                all_probabilities.extend(probabilities.numpy())\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'predictions': np.array(all_predictions),\n",
    "                'probabilities': np.array(all_probabilities),\n",
    "                'total_time': total_time,\n",
    "                'throughput_samples_per_sec': len(all_predictions) / total_time\n",
    "            }\n",
    "    \n",
    "    # Create batch processor\n",
    "    tf_processor = TensorFlowBatchProcessor(tf_engine.model, batch_size=64)\n",
    "    \n",
    "    # Process dataset with progress tracking\n",
    "    print(\"Processing large dataset...\")\n",
    "    results = tf_processor.process_dataset(large_dataset, progress_callback)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Batch Processing Results:\")\n",
    "    print(f\"  Total samples: {len(results['predictions'])}\")\n",
    "    print(f\"  Total time: {results['total_time']:.3f}s\")\n",
    "    print(f\"  Average batch time: {results['avg_batch_time']:.3f}s\")\n",
    "    print(f\"  Throughput: {results['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "    \n",
    "    # Demonstrate TensorFlow dataset processing\n",
    "    print(\"\\nðŸ—‚ï¸ TensorFlow Dataset Processing:\")\n",
    "    \n",
    "    # Create TensorFlow dataset\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(large_dataset[:500])\n",
    "    tf_dataset = tf_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset_results = tf_processor.process_tf_dataset(tf_dataset)\n",
    "    \n",
    "    print(f\"  Processed {len(dataset_results['predictions'])} samples\")\n",
    "    print(f\"  Total time: {dataset_results['total_time']:.3f}s\")\n",
    "    print(f\"  Throughput: {dataset_results['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Batch Processing Best Practices:\")\n",
    "print(\"â€¢ Use appropriate batch sizes (32-128 typically optimal)\")\n",
    "print(\"â€¢ Implement progress tracking for long-running jobs\")\n",
    "print(\"â€¢ Consider memory constraints when setting batch size\")\n",
    "print(\"â€¢ Use streaming processing for very large datasets\")\n",
    "print(\"â€¢ Optimize data loading and preprocessing pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concurrent Inference Patterns\n",
    "\n",
    "Handling multiple concurrent inference requests efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONCURRENT INFERENCE PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch concurrent inference\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nðŸ”¥ PyTorch Concurrent Inference:\")\n",
    "    \n",
    "    class PyTorchConcurrentInference:\n",
    "        def __init__(self, model, device='cpu', max_workers=4):\n",
    "            self.device = torch.device(device)\n",
    "            self.max_workers = max_workers\n",
    "            \n",
    "            # Create multiple model instances for thread safety\n",
    "            self.models = []\n",
    "            for _ in range(max_workers):\n",
    "                model_copy = type(model)(input_dim, 64, num_classes).to(device)\n",
    "                model_copy.load_state_dict(model.state_dict())\n",
    "                model_copy.eval()\n",
    "                self.models.append(model_copy)\n",
    "            \n",
    "            self.model_queue = queue.Queue()\n",
    "            for model in self.models:\n",
    "                self.model_queue.put(model)\n",
    "        \n",
    "        def predict_single_threadsafe(self, sample: np.ndarray, request_id: str = None) -> Dict[str, Any]:\n",
    "            \"\"\"Thread-safe single prediction\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Get model from queue\n",
    "            model = self.model_queue.get()\n",
    "            \n",
    "            try:\n",
    "                # Preprocessing\n",
    "                input_tensor = torch.FloatTensor(sample.reshape(1, -1)).to(self.device)\n",
    "                \n",
    "                # Inference\n",
    "                with torch.no_grad():\n",
    "                    logits = model(input_tensor)\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "                \n",
    "                result = {\n",
    "                    'request_id': request_id,\n",
    "                    'predicted_class': predicted_class.cpu().item(),\n",
    "                    'probabilities': probabilities.cpu().numpy()[0],\n",
    "                    'confidence': torch.max(probabilities).cpu().item(),\n",
    "                    'inference_time_ms': (time.time() - start_time) * 1000,\n",
    "                    'thread_id': threading.current_thread().ident\n",
    "                }\n",
    "                \n",
    "                return result\n",
    "            \n",
    "            finally:\n",
    "                # Return model to queue\n",
    "                self.model_queue.put(model)\n",
    "        \n",
    "        def process_concurrent_requests(self, samples: List[np.ndarray]) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Process multiple requests concurrently\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                # Submit all requests\n",
    "                futures = []\n",
    "                for i, sample in enumerate(samples):\n",
    "                    future = executor.submit(self.predict_single_threadsafe, sample, f\"req_{i}\")\n",
    "                    futures.append(future)\n",
    "                \n",
    "                # Collect results\n",
    "                results = []\n",
    "                for future in futures:\n",
    "                    results.append(future.result())\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Add summary statistics\n",
    "            for result in results:\n",
    "                result['total_processing_time'] = total_time\n",
    "            \n",
    "            return results\n",
    "    \n",
    "    # Create concurrent inference engine\n",
    "    pt_concurrent = PyTorchConcurrentInference(pt_engine.model, max_workers=4)\n",
    "    \n",
    "    # Test concurrent processing\n",
    "    test_samples = [X_test[i] for i in range(20)]\n",
    "    \n",
    "    # Sequential processing (baseline)\n",
    "    start_time = time.time()\n",
    "    sequential_results = [pt_engine.predict_single(sample) for sample in test_samples]\n",
    "    sequential_time = time.time() - start_time\n",
    "    \n",
    "    # Concurrent processing\n",
    "    concurrent_results = pt_concurrent.process_concurrent_requests(test_samples)\n",
    "    concurrent_time = concurrent_results[0]['total_processing_time']\n",
    "    \n",
    "    print(f\"ðŸ“Š Concurrent Processing Comparison ({len(test_samples)} samples):\")\n",
    "    print(f\"  Sequential: {sequential_time:.3f}s\")\n",
    "    print(f\"  Concurrent: {concurrent_time:.3f}s\")\n",
    "    print(f\"  Speedup: {sequential_time/concurrent_time:.1f}x\")\n",
    "    \n",
    "    # Show thread distribution\n",
    "    thread_ids = [result['thread_id'] for result in concurrent_results]\n",
    "    unique_threads = len(set(thread_ids))\n",
    "    print(f\"  Used {unique_threads} different threads\")\n",
    "    \n",
    "    # Show first few results\n",
    "    print(\"\\nFirst 5 concurrent results:\")\n",
    "    for result in concurrent_results[:5]:\n",
    "        print(f\"  {result['request_id']}: Class {result['predicted_class']} \"\n",
    "              f\"({result['inference_time_ms']:.1f}ms, thread {result['thread_id']})\")\n",
    "\n",
    "# TensorFlow concurrent inference\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nðŸŸ  TensorFlow Concurrent Inference:\")\n",
    "    \n",
    "    class TensorFlowConcurrentInference:\n",
    "        def __init__(self, model, max_workers=4):\n",
    "            self.model = model\n",
    "            self.max_workers = max_workers\n",
    "            \n",
    "            # TensorFlow models are generally thread-safe for inference\n",
    "            # Create optimized inference function\n",
    "            self.inference_func = tf.function(self.model.call)\n",
    "            \n",
    "            # Thread-local storage for any thread-specific data\n",
    "            self.thread_local = threading.local()\n",
    "        \n",
    "        def predict_single_threadsafe(self, sample: np.ndarray, request_id: str = None) -> Dict[str, Any]:\n",
    "            \"\"\"Thread-safe single prediction\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Preprocessing\n",
    "            input_tensor = tf.constant(sample.reshape(1, -1), dtype=tf.float32)\n",
    "            \n",
    "            # Inference (TensorFlow handles thread safety)\n",
    "            probabilities = self.inference_func(input_tensor, training=False)\n",
    "            predicted_class = tf.argmax(probabilities, axis=1)\n",
    "            \n",
    "            result = {\n",
    "                'request_id': request_id,\n",
    "                'predicted_class': int(predicted_class.numpy()[0]),\n",
    "                'probabilities': probabilities.numpy()[0],\n",
    "                'confidence': float(tf.reduce_max(probabilities).numpy()),\n",
    "                'inference_time_ms': (time.time() - start_time) * 1000,\n",
    "                'thread_id': threading.current_thread().ident\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def process_concurrent_requests(self, samples: List[np.ndarray]) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Process multiple requests concurrently\"\"\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                # Submit all requests\n",
    "                futures = []\n",
    "                for i, sample in enumerate(samples):\n",
    "                    future = executor.submit(self.predict_single_threadsafe, sample, f\"req_{i}\")\n",
    "                    futures.append(future)\n",
    "                \n",
    "                # Collect results\n",
    "                results = []\n",
    "                for future in futures:\n",
    "                    results.append(future.result())\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Add summary statistics\n",
    "            for result in results:\n",
    "                result['total_processing_time'] = total_time\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        def process_batch_concurrent(self, samples: List[np.ndarray], batch_size: int = 8) -> List[Dict[str, Any]]:\n",
    "            \"\"\"Process samples in concurrent batches\"\"\"\n",
    "            def process_batch(batch_samples, batch_id):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Stack samples into batch\n",
    "                batch_array = np.array(batch_samples)\n",
    "                batch_tensor = tf.constant(batch_array, dtype=tf.float32)\n",
    "                \n",
    "                # Batch inference\n",
    "                probabilities = self.inference_func(batch_tensor, training=False)\n",
    "                predictions = tf.argmax(probabilities, axis=1)\n",
    "                \n",
    "                batch_time = time.time() - start_time\n",
    "                \n",
    "                # Convert to individual results\n",
    "                results = []\n",
    "                for i in range(len(batch_samples)):\n",
    "                    results.append({\n",
    "                        'request_id': f\"batch_{batch_id}_sample_{i}\",\n",
    "                        'predicted_class': int(predictions[i].numpy()),\n",
    "                        'probabilities': probabilities[i].numpy(),\n",
    "                        'confidence': float(probabilities[i].numpy().max()),\n",
    "                        'batch_time_ms': batch_time * 1000,\n",
    "                        'thread_id': threading.current_thread().ident\n",
    "                    })\n",
    "                \n",
    "                return results\n",
    "            \n",
    "            # Split into batches\n",
    "            batches = [samples[i:i + batch_size] for i in range(0, len(samples), batch_size)]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                # Submit batch processing tasks\n",
    "                futures = []\n",
    "                for i, batch in enumerate(batches):\n",
    "                    future = executor.submit(process_batch, batch, i)\n",
    "                    futures.append(future)\n",
    "                \n",
    "                # Collect results\n",
    "                all_results = []\n",
    "                for future in futures:\n",
    "                    batch_results = future.result()\n",
    "                    all_results.extend(batch_results)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Add total processing time\n",
    "            for result in all_results:\n",
    "                result['total_processing_time'] = total_time\n",
    "            \n",
    "            return all_results\n",
    "    \n",
    "    # Create concurrent inference engine\n",
    "    tf_concurrent = TensorFlowConcurrentInference(tf_engine.model, max_workers=4)\n",
    "    \n",
    "    # Test concurrent processing\n",
    "    test_samples = [X_test[i] for i in range(20)]\n",
    "    \n",
    "    # Sequential processing (baseline)\n",
    "    start_time = time.time()\n",
    "    sequential_results = [tf_engine.predict_single(sample) for sample in test_samples]\n",
    "    sequential_time = time.time() - start_time\n",
    "    \n",
    "    # Concurrent processing\n",
    "    concurrent_results = tf_concurrent.process_concurrent_requests(test_samples)\n",
    "    concurrent_time = concurrent_results[0]['total_processing_time']\n",
    "    \n",
    "    # Batch concurrent processing\n",
    "    batch_concurrent_results = tf_concurrent.process_batch_concurrent(test_samples, batch_size=8)\n",
    "    batch_concurrent_time = batch_concurrent_results[0]['total_processing_time']\n",
    "    \n",
    "    print(f\"ðŸ“Š Concurrent Processing Comparison ({len(test_samples)} samples):\")\n",
    "    print(f\"  Sequential: {sequential_time:.3f}s\")\n",
    "    print(f\"  Concurrent (single): {concurrent_time:.3f}s (speedup: {sequential_time/concurrent_time:.1f}x)\")\n",
    "    print(f\"  Concurrent (batch): {batch_concurrent_time:.3f}s (speedup: {sequential_time/batch_concurrent_time:.1f}x)\")\n",
    "    \n",
    "    # Show thread distribution\n",
    "    thread_ids = [result['thread_id'] for result in concurrent_results]\n",
    "    unique_threads = len(set(thread_ids))\n",
    "    print(f\"  Used {unique_threads} different threads\")\n",
    "    \n",
    "    # Show first few results\n",
    "    print(\"\\nFirst 5 batch concurrent results:\")\n",
    "    for result in batch_concurrent_results[:5]:\n",
    "        print(f\"  {result['request_id']}: Class {result['predicted_class']} \"\n",
    "              f\"({result['batch_time_ms']:.1f}ms batch time, thread {result['thread_id']})\")\n",
    "\n",
    "print(\"\\nðŸš€ Concurrent Inference Best Practices:\")\n",
    "print(\"â€¢ Use thread pools to manage concurrent requests\")\n",
    "print(\"â€¢ Consider model thread safety (PyTorch needs multiple instances)\")\n",
    "print(\"â€¢ Batch concurrent requests when possible for better throughput\")\n",
    "print(\"â€¢ Monitor resource usage (CPU, memory, GPU)\")\n",
    "print(\"â€¢ Implement proper error handling and timeouts\")\n",
    "print(\"â€¢ Use async patterns for I/O-bound operations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
