{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Basics: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Learn containerization with Docker for ML models\n",
    "- Understand environment management and dependencies\n",
    "- Explore deployment strategies and MLOps concepts\n",
    "- Get introduced to next steps in production ML\n",
    "\n",
    "**Prerequisites:** API endpoints, model serialization\n",
    "\n",
    "**Estimated Time:** 45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Check system information\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "system_info = {\n",
    "    'Platform': platform.platform(),\n",
    "    'Python Version': platform.python_version(),\n",
    "    'Architecture': platform.architecture()[0],\n",
    "    'Processor': platform.processor() or 'Unknown',\n",
    "    'Current Directory': os.getcwd()\n",
    "}\n",
    "\n",
    "for key, value in system_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Check if Docker is available\n",
    "def check_docker():\n",
    "    try:\n",
    "        result = subprocess.run(['docker', '--version'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            return True, result.stdout.strip()\n",
    "        else:\n",
    "            return False, \"Docker command failed\"\n",
    "    except (subprocess.TimeoutExpired, FileNotFoundError):\n",
    "        return False, \"Docker not found\"\n",
    "\n",
    "docker_available, docker_info = check_docker()\n",
    "print(f\"\\nDocker Available: {'‚úÖ' if docker_available else '‚ùå'} {docker_info}\")\n",
    "\n",
    "# Try to import ML frameworks\n",
    "try:\n",
    "    import torch\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Management\n",
    "\n",
    "Managing dependencies and environments for reproducible deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENVIRONMENT MANAGEMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create deployment directory structure\n",
    "deployment_dir = Path('../../deployment')\n",
    "deployment_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# PyTorch deployment setup\n",
    "pytorch_dir = deployment_dir / 'pytorch-api'\n",
    "pytorch_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# TensorFlow deployment setup\n",
    "tensorflow_dir = deployment_dir / 'tensorflow-api'\n",
    "tensorflow_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created deployment directories:\")\n",
    "print(f\"  PyTorch: {pytorch_dir}\")\n",
    "print(f\"  TensorFlow: {tensorflow_dir}\")\n",
    "\n",
    "# Generate requirements files\n",
    "print(\"\\nüì¶ Generating Requirements Files:\")\n",
    "\n",
    "pytorch_requirements = \"\"\"\n",
    "# PyTorch API Requirements\n",
    "torch>=1.9.0\n",
    "torchvision>=0.10.0\n",
    "flask>=2.0.0\n",
    "gunicorn>=20.1.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.0.0\n",
    "pandas>=1.3.0\n",
    "requests>=2.25.0\n",
    "python-dotenv>=0.19.0\n",
    "prometheus-client>=0.11.0\n",
    "\"\"\".strip()\n",
    "\n",
    "tensorflow_requirements = \"\"\"\n",
    "# TensorFlow API Requirements\n",
    "tensorflow>=2.6.0\n",
    "fastapi>=0.70.0\n",
    "uvicorn[standard]>=0.15.0\n",
    "pydantic>=1.8.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.0.0\n",
    "pandas>=1.3.0\n",
    "requests>=2.25.0\n",
    "python-dotenv>=0.19.0\n",
    "prometheus-client>=0.11.0\n",
    "\"\"\".strip()\n",
    "\n",
    "# Write requirements files\n",
    "with open(pytorch_dir / 'requirements.txt', 'w') as f:\n",
    "    f.write(pytorch_requirements)\n",
    "\n",
    "with open(tensorflow_dir / 'requirements.txt', 'w') as f:\n",
    "    f.write(tensorflow_requirements)\n",
    "\n",
    "print(\"‚úÖ Created requirements.txt files\")\n",
    "\n",
    "# Generate environment files\n",
    "print(\"\\nüîß Environment Configuration:\")\n",
    "\n",
    "env_template = \"\"\"\n",
    "# Environment Configuration\n",
    "MODEL_PATH=/app/models/model\n",
    "LOG_LEVEL=INFO\n",
    "MAX_WORKERS=4\n",
    "BATCH_SIZE=32\n",
    "DEVICE=cpu\n",
    "API_KEY=your-api-key-here\n",
    "CORS_ORIGINS=*\n",
    "METRICS_ENABLED=true\n",
    "HEALTH_CHECK_INTERVAL=30\n",
    "\"\"\".strip()\n",
    "\n",
    "# Write environment files\n",
    "with open(pytorch_dir / '.env.example', 'w') as f:\n",
    "    f.write(env_template)\n",
    "\n",
    "with open(tensorflow_dir / '.env.example', 'w') as f:\n",
    "    f.write(env_template)\n",
    "\n",
    "print(\"‚úÖ Created .env.example files\")\n",
    "\n",
    "# Generate conda environment files\n",
    "print(\"\\nüêç Conda Environment Files:\")\n",
    "\n",
    "pytorch_conda_env = {\n",
    "    'name': 'pytorch-api',\n",
    "    'channels': ['pytorch', 'conda-forge', 'defaults'],\n",
    "    'dependencies': [\n",
    "        'python=3.9',\n",
    "        'pytorch',\n",
    "        'torchvision',\n",
    "        'numpy',\n",
    "        'pandas',\n",
    "        'scikit-learn',\n",
    "        'pip',\n",
    "        {\n",
    "            'pip': [\n",
    "                'flask>=2.0.0',\n",
    "                'gunicorn>=20.1.0',\n",
    "                'python-dotenv>=0.19.0',\n",
    "                'prometheus-client>=0.11.0'\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "tensorflow_conda_env = {\n",
    "    'name': 'tensorflow-api',\n",
    "    'channels': ['conda-forge', 'defaults'],\n",
    "    'dependencies': [\n",
    "        'python=3.9',\n",
    "        'tensorflow',\n",
    "        'numpy',\n",
    "        'pandas',\n",
    "        'scikit-learn',\n",
    "        'pip',\n",
    "        {\n",
    "            'pip': [\n",
    "                'fastapi>=0.70.0',\n",
    "                'uvicorn[standard]>=0.15.0',\n",
    "                'pydantic>=1.8.0',\n",
    "                'python-dotenv>=0.19.0',\n",
    "                'prometheus-client>=0.11.0'\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Write conda environment files\n",
    "import yaml\n",
    "\n",
    "try:\n",
    "    with open(pytorch_dir / 'environment.yml', 'w') as f:\n",
    "        yaml.dump(pytorch_conda_env, f, default_flow_style=False)\n",
    "    \n",
    "    with open(tensorflow_dir / 'environment.yml', 'w') as f:\n",
    "        yaml.dump(tensorflow_conda_env, f, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Created environment.yml files\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyYAML not available - creating JSON environment files instead\")\n",
    "    \n",
    "    with open(pytorch_dir / 'environment.json', 'w') as f:\n",
    "        json.dump(pytorch_conda_env, f, indent=2)\n",
    "    \n",
    "    with open(tensorflow_dir / 'environment.json', 'w') as f:\n",
    "        json.dump(tensorflow_conda_env, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Created environment.json files\")\n",
    "\n",
    "# Environment management commands\n",
    "print(\"\\nüí° Environment Management Commands:\")\n",
    "\n",
    "env_commands = {\n",
    "    'Virtual Environment': [\n",
    "        'python -m venv venv',\n",
    "        'source venv/bin/activate  # Linux/Mac',\n",
    "        'venv\\\\Scripts\\\\activate     # Windows',\n",
    "        'pip install -r requirements.txt'\n",
    "    ],\n",
    "    'Conda Environment': [\n",
    "        'conda env create -f environment.yml',\n",
    "        'conda activate pytorch-api',\n",
    "        'conda env update -f environment.yml  # Update existing'\n",
    "    ],\n",
    "    'Docker Environment': [\n",
    "        'docker build -t my-ml-api .',\n",
    "        'docker run -p 8000:8000 --env-file .env my-ml-api',\n",
    "        'docker-compose up -d  # With docker-compose'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for env_type, commands in env_commands.items():\n",
    "    print(f\"\\n{env_type}:\")\n",
    "    for cmd in commands:\n",
    "        print(f\"  {cmd}\")\n",
    "\n",
    "print(\"\\nüîç Environment Best Practices:\")\n",
    "best_practices = [\n",
    "    \"Pin exact versions for production deployments\",\n",
    "    \"Use separate environments for development and production\",\n",
    "    \"Include all system dependencies in documentation\",\n",
    "    \"Test deployments in staging environments first\",\n",
    "    \"Use multi-stage Docker builds to reduce image size\",\n",
    "    \"Regularly update dependencies for security patches\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"  ‚Ä¢ {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Docker Containerization\n",
    "\n",
    "Creating Docker containers for consistent deployment across environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DOCKER CONTAINERIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch Dockerfile\n",
    "print(\"\\nüî• PyTorch Dockerfile:\")\n",
    "\n",
    "pytorch_dockerfile = \"\"\"\n",
    "# Multi-stage build for PyTorch API\n",
    "FROM python:3.9-slim as base\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1 \\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PIP_NO_CACHE_DIR=1 \\\n",
    "    PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd --create-home --shell /bin/bash app\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY --chown=app:app . .\n",
    "\n",
    "# Create directories for models and logs\n",
    "RUN mkdir -p /app/models /app/logs && \\\n",
    "    chown -R app:app /app\n",
    "\n",
    "# Switch to non-root user\n",
    "USER app\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 5000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:5000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"--workers\", \"4\", \\\n",
    "     \"--worker-class\", \"sync\", \"--timeout\", \"120\", \\\n",
    "     \"--access-logfile\", \"-\", \"--error-logfile\", \"-\", \\\n",
    "     \"app:app\"]\n",
    "\"\"\".strip()\n",
    "\n",
    "# TensorFlow Dockerfile\n",
    "print(\"\\nüü† TensorFlow Dockerfile:\")\n",
    "\n",
    "tensorflow_dockerfile = \"\"\"\n",
    "# Multi-stage build for TensorFlow API\n",
    "FROM python:3.9-slim as base\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1 \\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PIP_NO_CACHE_DIR=1 \\\n",
    "    PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd --create-home --shell /bin/bash app\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY --chown=app:app . .\n",
    "\n",
    "# Create directories for models and logs\n",
    "RUN mkdir -p /app/models /app/logs && \\\n",
    "    chown -R app:app /app\n",
    "\n",
    "# Switch to non-root user\n",
    "USER app\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \\\n",
    "     \"--workers\", \"4\", \"--access-log\", \"--log-level\", \"info\"]\n",
    "\"\"\".strip()\n",
    "\n",
    "# Write Dockerfiles\n",
    "with open(pytorch_dir / 'Dockerfile', 'w') as f:\n",
    "    f.write(pytorch_dockerfile)\n",
    "\n",
    "with open(tensorflow_dir / 'Dockerfile', 'w') as f:\n",
    "    f.write(tensorflow_dockerfile)\n",
    "\n",
    "print(\"‚úÖ Created Dockerfile for PyTorch API\")\n",
    "print(\"‚úÖ Created Dockerfile for TensorFlow API\")\n",
    "\n",
    "# Docker Compose files\n",
    "print(\"\\nüê≥ Docker Compose Configuration:\")\n",
    "\n",
    "pytorch_compose = {\n",
    "    'version': '3.8',\n",
    "    'services': {\n",
    "        'pytorch-api': {\n",
    "            'build': '.',\n",
    "            'ports': ['5000:5000'],\n",
    "            'environment': [\n",
    "                'MODEL_PATH=/app/models/model.pth',\n",
    "                'LOG_LEVEL=INFO',\n",
    "                'DEVICE=cpu'\n",
    "            ],\n",
    "            'volumes': [\n",
    "                './models:/app/models:ro',\n",
    "                './logs:/app/logs'\n",
    "            ],\n",
    "            'restart': 'unless-stopped',\n",
    "            'healthcheck': {\n",
    "                'test': ['CMD', 'curl', '-f', 'http://localhost:5000/health'],\n",
    "                'interval': '30s',\n",
    "                'timeout': '10s',\n",
    "                'retries': 3,\n",
    "                'start_period': '40s'\n",
    "            }\n",
    "        },\n",
    "        'nginx': {\n",
    "            'image': 'nginx:alpine',\n",
    "            'ports': ['80:80'],\n",
    "            'volumes': ['./nginx.conf:/etc/nginx/nginx.conf:ro'],\n",
    "            'depends_on': ['pytorch-api'],\n",
    "            'restart': 'unless-stopped'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tensorflow_compose = {\n",
    "    'version': '3.8',\n",
    "    'services': {\n",
    "        'tensorflow-api': {\n",
    "            'build': '.',\n",
    "            'ports': ['8000:8000'],\n",
    "            'environment': [\n",
    "                'MODEL_PATH=/app/models/model',\n",
    "                'LOG_LEVEL=INFO'\n",
    "            ],\n",
    "            'volumes': [\n",
    "                './models:/app/models:ro',\n",
    "                './logs:/app/logs'\n",
    "            ],\n",
    "            'restart': 'unless-stopped',\n",
    "            'healthcheck': {\n",
    "                'test': ['CMD', 'curl', '-f', 'http://localhost:8000/health'],\n",
    "                'interval': '30s',\n",
    "                'timeout': '10s',\n",
    "                'retries': 3,\n",
    "                'start_period': '40s'\n",
    "            }\n",
    "        },\n",
    "        'nginx': {\n",
    "            'image': 'nginx:alpine',\n",
    "            'ports': ['80:80'],\n",
    "            'volumes': ['./nginx.conf:/etc/nginx/nginx.conf:ro'],\n",
    "            'depends_on': ['tensorflow-api'],\n",
    "            'restart': 'unless-stopped'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write docker-compose files\n",
    "try:\n",
    "    with open(pytorch_dir / 'docker-compose.yml', 'w') as f:\n",
    "        yaml.dump(pytorch_compose, f, default_flow_style=False)\n",
    "    \n",
    "    with open(tensorflow_dir / 'docker-compose.yml', 'w') as f:\n",
    "        yaml.dump(tensorflow_compose, f, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Created docker-compose.yml files\")\n",
    "except NameError:\n",
    "    # YAML not available, create JSON versions\n",
    "    with open(pytorch_dir / 'docker-compose.json', 'w') as f:\n",
    "        json.dump(pytorch_compose, f, indent=2)\n",
    "    \n",
    "    with open(tensorflow_dir / 'docker-compose.json', 'w') as f:\n",
    "        json.dump(tensorflow_compose, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Created docker-compose.json files\")\n",
    "\n",
    "# Nginx configuration\n",
    "print(\"\\nüåê Nginx Load Balancer Configuration:\")\n",
    "\n",
    "nginx_config = \"\"\"\n",
    "events {\n",
    "    worker_connections 1024;\n",
    "}\n",
    "\n",
    "http {\n",
    "    upstream api_backend {\n",
    "        server pytorch-api:5000;  # or tensorflow-api:8000\n",
    "        # Add more servers for load balancing\n",
    "        # server pytorch-api-2:5000;\n",
    "    }\n",
    "\n",
    "    server {\n",
    "        listen 80;\n",
    "        \n",
    "        # Health check endpoint\n",
    "        location /health {\n",
    "            proxy_pass http://api_backend/health;\n",
    "            proxy_set_header Host $host;\n",
    "            proxy_set_header X-Real-IP $remote_addr;\n",
    "        }\n",
    "        \n",
    "        # API endpoints\n",
    "        location /api/ {\n",
    "            proxy_pass http://api_backend/;\n",
    "            proxy_set_header Host $host;\n",
    "            proxy_set_header X-Real-IP $remote_addr;\n",
    "            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "            proxy_set_header X-Forwarded-Proto $scheme;\n",
    "            \n",
    "            # Timeout settings\n",
    "            proxy_connect_timeout 60s;\n",
    "            proxy_send_timeout 60s;\n",
    "            proxy_read_timeout 60s;\n",
    "            \n",
    "            # Buffer settings\n",
    "            proxy_buffering on;\n",
    "            proxy_buffer_size 4k;\n",
    "            proxy_buffers 8 4k;\n",
    "        }\n",
    "        \n",
    "        # Rate limiting\n",
    "        location /predict {\n",
    "            limit_req zone=api burst=10 nodelay;\n",
    "            proxy_pass http://api_backend/predict;\n",
    "            proxy_set_header Host $host;\n",
    "            proxy_set_header X-Real-IP $remote_addr;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Rate limiting zone\n",
    "    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;\n",
    "    \n",
    "    # Logging\n",
    "    access_log /var/log/nginx/access.log;\n",
    "    error_log /var/log/nginx/error.log;\n",
    "}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Write nginx config\n",
    "with open(pytorch_dir / 'nginx.conf', 'w') as f:\n",
    "    f.write(nginx_config)\n",
    "\n",
    "with open(tensorflow_dir / 'nginx.conf', 'w') as f:\n",
    "    f.write(nginx_config.replace('pytorch-api:5000', 'tensorflow-api:8000'))\n",
    "\n",
    "print(\"‚úÖ Created nginx.conf files\")\n",
    "\n",
    "# Docker commands\n",
    "print(\"\\nüîß Docker Commands:\")\n",
    "\n",
    "docker_commands = {\n",
    "    'Build and Run': [\n",
    "        'docker build -t my-ml-api .',\n",
    "        'docker run -d -p 8000:8000 --name ml-api my-ml-api',\n",
    "        'docker logs ml-api',\n",
    "        'docker exec -it ml-api /bin/bash'\n",
    "    ],\n",
    "    'Docker Compose': [\n",
    "        'docker-compose up -d',\n",
    "        'docker-compose logs -f',\n",
    "        'docker-compose scale tensorflow-api=3',\n",
    "        'docker-compose down'\n",
    "    ],\n",
    "    'Maintenance': [\n",
    "        'docker system prune -f',\n",
    "        'docker image prune -f',\n",
    "        'docker volume prune -f',\n",
    "        'docker stats'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, commands in docker_commands.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for cmd in commands:\n",
    "        print(f\"  {cmd}\")\n",
    "\n",
    "print(\"\\nüõ°Ô∏è Docker Security Best Practices:\")\n",
    "security_practices = [\n",
    "    \"Use non-root users in containers\",\n",
    "    \"Scan images for vulnerabilities regularly\",\n",
    "    \"Use multi-stage builds to reduce attack surface\",\n",
    "    \"Keep base images updated\",\n",
    "    \"Use secrets management for sensitive data\",\n",
    "    \"Implement proper network segmentation\",\n",
    "    \"Set resource limits (CPU, memory)\",\n",
    "    \"Use read-only filesystems where possible\"\n",
    "]\n",
    "\n",
    "for practice in security_practices:\n",
    "    print(f\"  ‚Ä¢ {practice}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  },
  {

   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLOps and Next Steps\n",
    "\n",
    "Introduction to MLOps concepts and advanced deployment topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MLOPS AND NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# MLOps pipeline overview\n",
    "print(\"\\nüîÑ MLOps Pipeline Overview:\")\n",
    "\n",
    "mlops_stages = {\n",
    "    \"1. Data Management\": [\n",
    "        \"Data versioning (DVC, Git LFS)\",\n",
    "        \"Data validation and monitoring\",\n",
    "        \"Feature stores for reusable features\",\n",
    "        \"Data lineage tracking\"\n",
    "    ],\n",
    "    \"2. Model Development\": [\n",
    "        \"Experiment tracking (MLflow, Weights & Biases)\",\n",
    "        \"Model versioning and registry\",\n",
    "        \"Automated hyperparameter tuning\",\n",
    "        \"Reproducible training pipelines\"\n",
    "    ],\n",
    "    \"3. Model Deployment\": [\n",
    "        \"Containerization (Docker, Kubernetes)\",\n",
    "        \"CI/CD pipelines for ML\",\n",
    "        \"A/B testing for model versions\",\n",
    "        \"Blue-green deployments\"\n",
    "    ],\n",
    "    \"4. Monitoring & Maintenance\": [\n",
    "        \"Model performance monitoring\",\n",
    "        \"Data drift detection\",\n",
    "        \"Model retraining triggers\",\n",
    "        \"Alerting and incident response\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for stage, components in mlops_stages.items():\n",
    "    print(f\"\\n{stage}:\")\n",
    "    for component in components:\n",
    "        print(f\"  ‚Ä¢ {component}\")\n",
    "\n",
    "# Framework-specific deployment paths\n",
    "print(\"\\nüõ§Ô∏è Framework-Specific Deployment Paths:\")\n",
    "\n",
    "deployment_paths = {\n",
    "    \"PyTorch\": {\n",
    "        \"Development\": \"Python scripts with torch.save/load\",\n",
    "        \"Production\": \"TorchScript (torch.jit) or TorchServe\",\n",
    "        \"Mobile\": \"PyTorch Mobile\",\n",
    "        \"Edge\": \"ONNX export ‚Üí various runtimes\",\n",
    "        \"Cloud\": \"Docker + Kubernetes or cloud ML services\"\n",
    "    },\n",
    "    \"TensorFlow\": {\n",
    "        \"Development\": \"SavedModel format\",\n",
    "        \"Production\": \"TensorFlow Serving or TF Extended (TFX)\",\n",
    "        \"Mobile\": \"TensorFlow Lite\",\n",
    "        \"Edge\": \"TensorFlow Lite or TensorFlow.js\",\n",
    "        \"Cloud\": \"Docker + Kubernetes or Google AI Platform\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for framework, paths in deployment_paths.items():\n",
    "    print(f\"\\n{framework} Deployment Options:\")\n",
    "    for target, solution in paths.items():\n",
    "        print(f\"  {target:<12}: {solution}\")\n",
    "\n",
    "# Sample MLOps configuration\n",
    "print(\"\\n‚öôÔ∏è Sample MLOps Configuration Files:\")\n",
    "\n",
    "# GitHub Actions workflow\n",
    "github_workflow = \"\"\"\n",
    "# .github/workflows/ml-pipeline.yml\n",
    "name: ML Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: 3.9\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "        pip install pytest\n",
    "    \n",
    "    - name: Run tests\n",
    "      run: pytest tests/\n",
    "    \n",
    "    - name: Validate model\n",
    "      run: python scripts/validate_model.py\n",
    "  \n",
    "  deploy:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    \n",
    "    - name: Build Docker image\n",
    "      run: docker build -t ml-model:${{ github.sha }} .\n",
    "    \n",
    "    - name: Deploy to staging\n",
    "      run: |\n",
    "        # Deploy to staging environment\n",
    "        echo \"Deploying to staging...\"\n",
    "\"\"\".strip()\n",
    "\n",
    "# Kubernetes deployment\n",
    "k8s_deployment = \"\"\"\n",
    "# k8s-deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-model-deployment\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-model\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ml-model\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: ml-model\n",
    "        image: ml-model:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: MODEL_PATH\n",
    "          value: \"/app/models/model\"\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: ml-model-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: ml-model\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "\"\"\".strip()\n",
    "\n",
    "# Write configuration files\n",
    "with open(pytorch_dir / 'github-workflow.yml', 'w') as f:\n",
    "    f.write(github_workflow)\n",
    "\n",
    "with open(tensorflow_dir / 'k8s-deployment.yaml', 'w') as f:\n",
    "    f.write(k8s_deployment)\n",
    "\n",
    "print(\"‚úÖ Created sample MLOps configuration files\")\n",
    "\n",
    "# Monitoring and observability\n",
    "print(\"\\nüìä Monitoring and Observability:\")\n",
    "\n",
    "monitoring_aspects = {\n",
    "    \"Infrastructure Metrics\": [\n",
    "        \"CPU and memory usage\",\n",
    "        \"Request latency and throughput\",\n",
    "        \"Error rates and status codes\",\n",
    "        \"Container health and restarts\"\n",
    "    ],\n",
    "    \"Model Performance\": [\n",
    "        \"Prediction accuracy over time\",\n",
    "        \"Model confidence distributions\",\n",
    "        \"Feature importance changes\",\n",
    "        \"Prediction drift detection\"\n",
    "    ],\n",
    "    \"Data Quality\": [\n",
    "        \"Input data distribution shifts\",\n",
    "        \"Missing or invalid features\",\n",
    "        \"Data freshness and completeness\",\n",
    "        \"Schema validation failures\"\n",
    "    ],\n",
    "    \"Business Metrics\": [\n",
    "        \"Model impact on business KPIs\",\n",
    "        \"A/B test results\",\n",
    "        \"User engagement with predictions\",\n",
    "        \"Revenue attribution\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, metrics in monitoring_aspects.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"  ‚Ä¢ {metric}\")\n",
    "\n",
    "# Next steps and learning path\n",
    "print(\"\\nüéØ Next Steps in Your ML Journey:\")\n",
    "\n",
    "learning_path = {\n",
    "    \"Immediate (1-2 months)\": [\n",
    "        \"Practice with the tutorial examples\",\n",
    "        \"Build and deploy a simple model\",\n",
    "        \"Learn Docker basics for containerization\",\n",
    "        \"Set up basic monitoring and logging\"\n",
    "    ],\n",
    "    \"Short-term (3-6 months)\": [\n",
    "        \"Implement CI/CD for ML projects\",\n",
    "        \"Learn Kubernetes for orchestration\",\n",
    "        \"Explore MLOps tools (MLflow, Kubeflow)\",\n",
    "        \"Practice model versioning and A/B testing\"\n",
    "    ],\n",
    "    \"Medium-term (6-12 months)\": [\n",
    "        \"Build end-to-end ML pipelines\",\n",
    "        \"Implement advanced monitoring and alerting\",\n",
    "        \"Learn about model governance and compliance\",\n",
    "        \"Explore edge deployment and optimization\"\n",
    "    ],\n",
    "    \"Long-term (1+ years)\": [\n",
    "        \"Design ML platform architecture\",\n",
    "        \"Implement automated retraining systems\",\n",
    "        \"Build custom ML infrastructure tools\",\n",
    "        \"Contribute to open-source ML projects\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for timeframe, goals in learning_path.items():\n",
    "    print(f\"\\n{timeframe}:\")\n",
    "    for goal in goals:\n",
    "        print(f\"  ‚Ä¢ {goal}\")\n",
    "\n",
    "# Recommended resources\n",
    "print(\"\\nüìö Recommended Resources:\")\n",
    "\n",
    "resources = {\n",
    "    \"Books\": [\n",
    "        \"'Building Machine Learning Pipelines' by Hannes Hapke\",\n",
    "        \"'Designing Machine Learning Systems' by Chip Huyen\",\n",
    "        \"'Machine Learning Engineering' by Andriy Burkov\"\n",
    "    ],\n",
    "    \"Online Courses\": [\n",
    "        \"MLOps Specialization (Coursera)\",\n",
    "        \"Machine Learning Engineering for Production (Coursera)\",\n",
    "        \"Kubernetes for Machine Learning (various platforms)\"\n",
    "    ],\n",
    "    \"Tools to Explore\": [\n",
    "        \"MLflow for experiment tracking\",\n",
    "        \"Kubeflow for ML workflows on Kubernetes\",\n",
    "        \"Apache Airflow for pipeline orchestration\",\n",
    "        \"Prometheus + Grafana for monitoring\"\n",
    "    ],\n",
    "    \"Communities\": [\n",
    "        \"MLOps Community (mlops.community)\",\n",
    "        \"PyTorch and TensorFlow forums\",\n",
    "        \"Kubernetes SIG-ML working group\",\n",
    "        \"Local ML meetups and conferences\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in resources.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  ‚Ä¢ {item}\")\n",
    "\n",
    "print(\"\\nüéâ Congratulations!\")\n",
    "print(\"You've completed the ML Frameworks Tutorial series!\")\n",
    "print(\"\\n‚úÖ What you've learned:\")\n",
    "print(\"  ‚Ä¢ NumPy and Pandas foundations for ML\")\n",
    "print(\"  ‚Ä¢ PyTorch vs TensorFlow framework differences\")\n",
    "print(\"  ‚Ä¢ NLP, tabular data, and time series applications\")\n",
    "print(\"  ‚Ä¢ Production deployment patterns and best practices\")\n",
    "print(\"  ‚Ä¢ MLOps concepts and next steps\")\n",
    "print(\"\\nüöÄ You're now ready to build and deploy production ML systems!\")\n",
    "print(\"\\nHappy coding! ü§ñ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
