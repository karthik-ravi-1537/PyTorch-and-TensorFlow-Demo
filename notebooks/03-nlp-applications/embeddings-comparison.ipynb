{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master embedding layer creation in both frameworks\n",
    "- Compare embedding training and usage patterns\n",
    "- Learn pre-trained embedding integration\n",
    "- Understand embedding visualization and analysis\n",
    "\n",
    "**Prerequisites:** Text preprocessing, tensor operations\n",
    "\n",
    "**Estimated Time:** 35 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from foundations.data_utils import get_tutorial_text_data\n",
    "from foundations.preprocessing import TextPreprocessor\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Embedding Layers\n",
    "\n",
    "Comparing how to create and initialize embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREATING EMBEDDING LAYERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get sample data and build vocabulary\n",
    "text_data = get_tutorial_text_data(num_samples=200)\n",
    "texts = text_data['texts']\n",
    "labels = text_data['labels']\n",
    "\n",
    "# Build vocabulary\n",
    "preprocessor = TextPreprocessor()\n",
    "preprocessor.build_vocabulary(texts, min_freq=2, max_vocab_size=1000)\n",
    "\n",
    "vocab_size = preprocessor.vocab_size\n",
    "embedding_dim = 50\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# PyTorch embedding layer\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Embedding Layer:\")\n",
    "\n",
    "    # Create embedding layer\n",
    "    pt_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "    print(f\"Embedding layer: {pt_embedding}\")\n",
    "    print(f\"Weight shape: {pt_embedding.weight.shape}\")\n",
    "    print(f\"Padding index: {pt_embedding.padding_idx}\")\n",
    "\n",
    "    # Test with sample indices\n",
    "    sample_indices = torch.tensor([1, 5, 10, 0])  # 0 is padding\n",
    "    embedded = pt_embedding(sample_indices)\n",
    "\n",
    "    print(f\"\\nSample indices: {sample_indices}\")\n",
    "    print(f\"Embedded shape: {embedded.shape}\")\n",
    "    print(f\"Padding vector (should be zeros): {embedded[3][:5]}...\")  # First 5 dims\n",
    "\n",
    "# TensorFlow embedding layer\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Embedding Layer:\")\n",
    "\n",
    "    # Create embedding layer\n",
    "    tf_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=True  # Equivalent to padding_idx=0\n",
    "    )\n",
    "\n",
    "    # Build the layer\n",
    "    sample_indices = tf.constant([1, 5, 10, 0])\n",
    "    embedded = tf_embedding(sample_indices)\n",
    "\n",
    "    print(f\"Embedding layer: {tf_embedding}\")\n",
    "    print(f\"Weight shape: {tf_embedding.weights[0].shape}\")\n",
    "    print(f\"Mask zero: {tf_embedding.mask_zero}\")\n",
    "\n",
    "    print(f\"\\nSample indices: {sample_indices}\")\n",
    "    print(f\"Embedded shape: {embedded.shape}\")\n",
    "    print(f\"Padding vector (should be zeros): {embedded[3][:5]}...\")  # First 5 dims\n",
    "\n",
    "# Side-by-side comparison\n",
    "pytorch_embedding_code = \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create embedding layer\n",
    "embedding = nn.Embedding(\n",
    "    num_embeddings=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    padding_idx=0  # Index 0 will be zero vector\n",
    ")\n",
    "\n",
    "# Initialize with custom weights (optional)\n",
    "nn.init.normal_(embedding.weight, mean=0, std=0.1)\n",
    "\n",
    "# Use embedding\n",
    "indices = torch.tensor([1, 5, 10, 0])\n",
    "embedded_vectors = embedding(indices)\n",
    "print(f\"Shape: {embedded_vectors.shape}\")\n",
    "\n",
    "# Freeze embeddings (optional)\n",
    "embedding.weight.requires_grad = False\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_embedding_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create embedding layer\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    mask_zero=True,  # Index 0 will be masked\n",
    "    embeddings_initializer='normal'\n",
    ")\n",
    "\n",
    "# Use embedding\n",
    "indices = tf.constant([1, 5, 10, 0])\n",
    "embedded_vectors = embedding(indices)\n",
    "print(f\"Shape: {embedded_vectors.shape}\")\n",
    "\n",
    "# Freeze embeddings (optional)\n",
    "embedding.trainable = False\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_embedding_code, tensorflow_embedding_code, \"Embedding Layer Creation\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Embeddings\n",
    "\n",
    "Training embeddings as part of a simple classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING EMBEDDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data\n",
    "sequences = preprocessor.texts_to_sequences(texts, max_length=20)\n",
    "sequences = np.array(sequences)\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "print(f\"Data shape: {sequences.shape}\")\n",
    "print(f\"Labels shape: {labels_array.shape}\")\n",
    "print(f\"Number of classes: {len(set(labels))}\")\n",
    "\n",
    "# PyTorch training\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Embedding Training:\")\n",
    "\n",
    "    class SimpleClassifier(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "            super().__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "            self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # x shape: (batch_size, seq_len)\n",
    "            embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "            # Simple pooling: average over sequence length\n",
    "            pooled = embedded.mean(dim=1)  # (batch_size, embedding_dim)\n",
    "            pooled = self.dropout(pooled)\n",
    "\n",
    "            output = self.fc(pooled)  # (batch_size, num_classes)\n",
    "            return output\n",
    "\n",
    "    # Create model\n",
    "    pt_model = SimpleClassifier(vocab_size, embedding_dim, len(set(labels)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(pt_model.parameters(), lr=0.001)\n",
    "\n",
    "    print(f\"Model: {pt_model}\")\n",
    "\n",
    "    # Convert data to tensors\n",
    "    X_train = torch.tensor(sequences, dtype=torch.long)\n",
    "    y_train = torch.tensor(labels_array, dtype=torch.long)\n",
    "\n",
    "    # Training loop (simplified)\n",
    "    pt_model.train()\n",
    "    for epoch in range(5):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = pt_model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Get trained embeddings\n",
    "    pt_trained_embeddings = pt_model.embedding.weight.detach().numpy()\n",
    "    print(f\"\\nTrained embeddings shape: {pt_trained_embeddings.shape}\")\n",
    "\n",
    "# TensorFlow training\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Embedding Training:\")\n",
    "\n",
    "    # Create model\n",
    "    tf_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),  # Average pooling\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(len(set(labels)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    tf_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"Model summary:\")\n",
    "    tf_model.build((None, 20))  # Build with input shape\n",
    "    tf_model.summary()\n",
    "\n",
    "    # Training\n",
    "    history = tf_model.fit(\n",
    "        sequences, labels_array,\n",
    "        epochs=5,\n",
    "        verbose=1,\n",
    "        batch_size=32\n",
    "    )\n",
    "\n",
    "    # Get trained embeddings\n",
    "    tf_trained_embeddings = tf_model.layers[0].get_weights()[0]\n",
    "    print(f\"\\nTrained embeddings shape: {tf_trained_embeddings.shape}\")\n",
    "\n",
    "# Compare embedding similarities\n",
    "if PYTORCH_AVAILABLE and TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüìä Embedding Analysis:\")\n",
    "\n",
    "    # Find most similar words to a target word\n",
    "    def find_similar_words(embeddings, word_to_idx, idx_to_word, target_word, top_k=5):\n",
    "        if target_word not in word_to_idx:\n",
    "            return []\n",
    "\n",
    "        target_idx = word_to_idx[target_word]\n",
    "        target_embedding = embeddings[target_idx]\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        similarities = np.dot(embeddings, target_embedding) / (\n",
    "            np.linalg.norm(embeddings, axis=1) * np.linalg.norm(target_embedding)\n",
    "        )\n",
    "\n",
    "        # Get top similar words (excluding the target word itself)\n",
    "        similar_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "\n",
    "        similar_words = []\n",
    "        for idx in similar_indices:\n",
    "            if idx in idx_to_word:\n",
    "                similar_words.append((idx_to_word[idx], similarities[idx]))\n",
    "\n",
    "        return similar_words\n",
    "\n",
    "    # Test with a common word\n",
    "    test_word = \"good\" if \"good\" in preprocessor.word_to_idx else list(preprocessor.word_to_idx.keys())[10]\n",
    "\n",
    "    print(f\"\\nWords similar to '{test_word}':\")\n",
    "\n",
    "    pt_similar = find_similar_words(\n",
    "        pt_trained_embeddings, preprocessor.word_to_idx, preprocessor.idx_to_word, test_word\n",
    "    )\n",
    "    print(f\"PyTorch: {pt_similar}\")\n",
    "\n",
    "    tf_similar = find_similar_words(\n",
    "        tf_trained_embeddings, preprocessor.word_to_idx, preprocessor.idx_to_word, test_word\n",
    "    )\n",
    "    print(f\"TensorFlow: {tf_similar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Visualization\n",
    "\n",
    "Visualizing learned embeddings using dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EMBEDDING VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    # Use PyTorch embeddings for visualization\n",
    "    embeddings_to_plot = pt_trained_embeddings\n",
    "    framework_name = \"PyTorch\"\n",
    "elif TENSORFLOW_AVAILABLE:\n",
    "    # Use TensorFlow embeddings for visualization\n",
    "    embeddings_to_plot = tf_trained_embeddings\n",
    "    framework_name = \"TensorFlow\"\n",
    "else:\n",
    "    print(\"No frameworks available for visualization\")\n",
    "    embeddings_to_plot = None\n",
    "\n",
    "if embeddings_to_plot is not None:\n",
    "    # Select top words for visualization (exclude padding)\n",
    "    top_words = 50\n",
    "    word_indices = list(range(1, min(top_words + 1, len(preprocessor.idx_to_word))))\n",
    "    selected_embeddings = embeddings_to_plot[word_indices]\n",
    "    selected_words = [preprocessor.idx_to_word[i] for i in word_indices]\n",
    "\n",
    "    print(f\"Visualizing {len(selected_words)} words using {framework_name} embeddings\")\n",
    "\n",
    "    # PCA reduction\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d_pca = pca.fit_transform(selected_embeddings)\n",
    "\n",
    "    # t-SNE reduction (for smaller subset)\n",
    "    if len(selected_words) <= 30:  # t-SNE is computationally expensive\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(selected_words)-1))\n",
    "        embeddings_2d_tsne = tsne.fit_transform(selected_embeddings)\n",
    "    else:\n",
    "        embeddings_2d_tsne = None\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2 if embeddings_2d_tsne is not None else 1, figsize=(15, 6))\n",
    "    if embeddings_2d_tsne is None:\n",
    "        axes = [axes]\n",
    "\n",
    "    # PCA plot\n",
    "    axes[0].scatter(embeddings_2d_pca[:, 0], embeddings_2d_pca[:, 1], alpha=0.7)\n",
    "\n",
    "    # Add word labels for a subset\n",
    "    for i, word in enumerate(selected_words[:20]):  # Show first 20 labels\n",
    "        axes[0].annotate(word, (embeddings_2d_pca[i, 0], embeddings_2d_pca[i, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "    axes[0].set_title(f'{framework_name} Embeddings - PCA')\n",
    "    axes[0].set_xlabel(f'PC1 (explained variance: {pca.explained_variance_ratio_[0]:.2%})')\n",
    "    axes[0].set_ylabel(f'PC2 (explained variance: {pca.explained_variance_ratio_[1]:.2%})')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # t-SNE plot (if computed)\n",
    "    if embeddings_2d_tsne is not None:\n",
    "        axes[1].scatter(embeddings_2d_tsne[:, 0], embeddings_2d_tsne[:, 1], alpha=0.7)\n",
    "\n",
    "        # Add word labels\n",
    "        for i, word in enumerate(selected_words):\n",
    "            axes[1].annotate(word, (embeddings_2d_tsne[i, 0], embeddings_2d_tsne[i, 1]),\n",
    "                            xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "        axes[1].set_title(f'{framework_name} Embeddings - t-SNE')\n",
    "        axes[1].set_xlabel('t-SNE 1')\n",
    "        axes[1].set_ylabel('t-SNE 2')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Embedding statistics\n",
    "    print(\"\\nEmbedding Statistics:\")\n",
    "    print(f\"Mean embedding norm: {np.mean(np.linalg.norm(selected_embeddings, axis=1)):.4f}\")\n",
    "    print(f\"Std embedding norm: {np.std(np.linalg.norm(selected_embeddings, axis=1)):.4f}\")\n",
    "    print(f\"PCA explained variance (first 2 components): {pca.explained_variance_ratio_[:2].sum():.2%}\")\n",
    "\n",
    "# Embedding analysis\n",
    "print(\"\\nüìà Embedding Quality Analysis:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE and TENSORFLOW_AVAILABLE:\n",
    "    # Compare embedding spaces\n",
    "    def embedding_similarity_analysis(emb1, emb2, name1, name2):\n",
    "        # Exclude padding embedding (index 0)\n",
    "        emb1_clean = emb1[1:]\n",
    "        emb2_clean = emb2[1:]\n",
    "\n",
    "        # Compute average cosine similarity between corresponding embeddings\n",
    "        similarities = []\n",
    "        for i in range(min(len(emb1_clean), len(emb2_clean))):\n",
    "            sim = np.dot(emb1_clean[i], emb2_clean[i]) / (\n",
    "                np.linalg.norm(emb1_clean[i]) * np.linalg.norm(emb2_clean[i])\n",
    "            )\n",
    "            similarities.append(sim)\n",
    "\n",
    "        avg_similarity = np.mean(similarities)\n",
    "        print(f\"Average cosine similarity between {name1} and {name2} embeddings: {avg_similarity:.4f}\")\n",
    "\n",
    "        return avg_similarity\n",
    "\n",
    "    similarity = embedding_similarity_analysis(\n",
    "        pt_trained_embeddings, tf_trained_embeddings, \"PyTorch\", \"TensorFlow\"\n",
    "    )\n",
    "\n",
    "    if similarity > 0.5:\n",
    "        print(\"‚úÖ Embeddings show good agreement between frameworks\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Embeddings show significant differences - this is normal for different initializations\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"‚Ä¢ Embeddings learn semantic relationships during training\")\n",
    "print(\"‚Ä¢ Similar words cluster together in the embedding space\")\n",
    "print(\"‚Ä¢ Both frameworks can learn meaningful representations\")\n",
    "print(\"‚Ä¢ Visualization helps understand what the model has learned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}