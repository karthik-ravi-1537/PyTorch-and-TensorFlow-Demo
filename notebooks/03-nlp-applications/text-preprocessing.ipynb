{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master text preprocessing techniques in both frameworks\n",
    "- Compare tokenization approaches and libraries\n",
    "- Learn text cleaning and normalization strategies\n",
    "- Understand framework-specific text handling patterns\n",
    "\n",
    "**Prerequisites:** Framework fundamentals, tensor operations\n",
    "\n",
    "**Estimated Time:** 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Add src to path for our utilities\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from foundations.data_utils import get_tutorial_text_data\n",
    "from foundations.preprocessing import TextPreprocessor\n",
    "from utils.comparison_tools import FrameworkComparison, create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  {

   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Text Cleaning\n",
    "\n",
    "Essential text preprocessing steps before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASIC TEXT CLEANING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get sample text data\n",
    "text_data = get_tutorial_text_data(num_samples=100)\n",
    "sample_texts = text_data['texts'][:10]\n",
    "labels = text_data['labels'][:10]\n",
    "label_names = text_data['label_names']\n",
    "\n",
    "print(\"Sample texts:\")\n",
    "for i, (text, label) in enumerate(zip(sample_texts[:3], labels[:3])):\n",
    "    print(f\"{i+1}. [{label_names[label]}] {text}\")\n",
    "\n",
    "# Basic text cleaning functions\n",
    "def clean_text_basic(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text_advanced(text):\n",
    "    \"\"\"Advanced text cleaning\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove numbers (optional)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Demonstrate cleaning\n",
    "print(\"\\nText Cleaning Examples:\")\n",
    "sample_text = \"This is an EXCELLENT product! I highly recommend it. üëç #awesome @company\"\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Basic cleaning: {clean_text_basic(sample_text)}\")\n",
    "print(f\"Advanced cleaning: {clean_text_advanced(sample_text)}\")\n",
    "\n",
    "# Clean all sample texts\n",
    "cleaned_texts = [clean_text_basic(text) for text in sample_texts]\n",
    "print(f\"\\nCleaned {len(cleaned_texts)} texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization Approaches\n",
    "\n",
    "Comparing different tokenization methods and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOKENIZATION APPROACHES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple word tokenization\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple whitespace tokenization\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "# Using our TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "preprocessor.build_vocabulary(cleaned_texts, min_freq=1, max_vocab_size=1000)\n",
    "\n",
    "print(f\"Built vocabulary with {preprocessor.vocab_size} words\")\n",
    "print(f\"Most common words: {list(preprocessor.vocab.keys())[:10]}\")\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = preprocessor.texts_to_sequences(cleaned_texts, max_length=20)\n",
    "print(f\"\\nConverted to sequences of max length 20\")\n",
    "print(f\"First sequence: {sequences[0]}\")\n",
    "print(f\"Corresponding text: {cleaned_texts[0]}\")\n",
    "\n",
    "# Framework-specific tokenization\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Tokenization:\")\n",
    "    \n",
    "    # Using Keras Tokenizer\n",
    "    tf_tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')\n",
    "    tf_tokenizer.fit_on_texts(cleaned_texts)\n",
    "    \n",
    "    print(f\"TensorFlow tokenizer vocabulary size: {len(tf_tokenizer.word_index)}\")\n",
    "    print(f\"Most common words: {list(tf_tokenizer.word_index.keys())[:10]}\")\n",
    "    \n",
    "    # Convert to sequences\n",
    "    tf_sequences = tf_tokenizer.texts_to_sequences(cleaned_texts)\n",
    "    tf_padded = pad_sequences(tf_sequences, maxlen=20, padding='post')\n",
    "    \n",
    "    print(f\"TensorFlow sequences shape: {tf_padded.shape}\")\n",
    "    print(f\"First sequence: {tf_padded[0]}\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Tokenization:\")\n",
    "    \n",
    "    # Convert sequences to PyTorch tensors\n",
    "    pt_sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "    \n",
    "    # Pad sequences\n",
    "    pt_padded = pad_sequence(pt_sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    print(f\"PyTorch sequences shape: {pt_padded.shape}\")\n",
    "    print(f\"First sequence: {pt_padded[0]}\")\n",
    "\n",
    "# Compare tokenization approaches\n",
    "pytorch_tokenization = \"\"\"\n",
    "# PyTorch tokenization workflow\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "# Build vocabulary manually\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        word_counts.update(text.split())\n",
    "    \n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Tokenize and convert to indices\n",
    "vocab = build_vocab(texts)\n",
    "sequences = []\n",
    "for text in texts:\n",
    "    seq = [vocab.get(word, vocab['<UNK>']) \n",
    "           for word in text.split()]\n",
    "    sequences.append(torch.tensor(seq))\n",
    "\n",
    "# Pad sequences\n",
    "padded = pad_sequence(sequences, batch_first=True)\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_tokenization = \"\"\"\n",
    "# TensorFlow tokenization workflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Use built-in Tokenizer\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=10000,\n",
    "    oov_token='<OOV>',\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    ")\n",
    "\n",
    "# Fit on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "padded = pad_sequences(\n",
    "    sequences, \n",
    "    maxlen=100, \n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_tokenization, tensorflow_tokenization, \"Tokenization Workflows\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Text Processing\n",
    "\n",
    "N-grams, TF-IDF, and other advanced techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ADVANCED TEXT PROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# N-gram extraction\n",
    "def extract_ngrams(text, n=2):\n",
    "    \"\"\"Extract n-grams from text\"\"\"\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = ' '.join(words[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Demonstrate n-grams\n",
    "sample_text = \"this is an excellent product that i highly recommend\"\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Unigrams: {extract_ngrams(sample_text, 1)}\")\n",
    "print(f\"Bigrams: {extract_ngrams(sample_text, 2)}\")\n",
    "print(f\"Trigrams: {extract_ngrams(sample_text, 3)}\")\n",
    "\n",
    "# TF-IDF features\n",
    "print(\"\\nTF-IDF Features:\")\n",
    "tfidf_features, tfidf_vectorizer = preprocessor.create_tfidf_features(cleaned_texts, max_features=100)\n",
    "print(f\"TF-IDF features shape: {tfidf_features.shape}\")\n",
    "print(f\"Feature names (first 10): {tfidf_vectorizer.get_feature_names_out()[:10]}\")\n",
    "\n",
    "# Bag of Words features\n",
    "print(\"\\nBag of Words Features:\")\n",
    "bow_features, bow_vectorizer = preprocessor.create_bow_features(cleaned_texts, max_features=100)\n",
    "print(f\"BoW features shape: {bow_features.shape}\")\n",
    "print(f\"Feature names (first 10): {bow_vectorizer.get_feature_names_out()[:10]}\")\n",
    "\n",
    "# Compare feature extraction methods\n",
    "print(\"\\nFeature Comparison for first text:\")\n",
    "print(f\"Original text: {cleaned_texts[0]}\")\n",
    "print(f\"Sequence representation: {sequences[0]}\")\n",
    "print(f\"TF-IDF (non-zero): {np.nonzero(tfidf_features[0])[0][:5]}\")\n",
    "print(f\"BoW (non-zero): {np.nonzero(bow_features[0])[0][:5]}\")\n",
    "\n",
    "# Text statistics\n",
    "print(\"\\nText Statistics:\")\n",
    "text_lengths = [len(text.split()) for text in cleaned_texts]\n",
    "print(f\"Average text length: {np.mean(text_lengths):.1f} words\")\n",
    "print(f\"Min length: {np.min(text_lengths)} words\")\n",
    "print(f\"Max length: {np.max(text_lengths)} words\")\n",
    "print(f\"Std deviation: {np.std(text_lengths):.1f} words\")\n",
    "\n",
    "# Vocabulary analysis\n",
    "all_words = ' '.join(cleaned_texts).split()\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"\\nVocabulary Statistics:\")\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "print(f\"Unique words: {len(word_freq)}\")\n",
    "print(f\"Most common words: {word_freq.most_common(10)}\")\n",
    "\n",
    "# Visualize word frequency distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(text_lengths, bins=10, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Text Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "top_words = [word for word, count in word_freq.most_common(10)]\n",
    "top_counts = [count for word, count in word_freq.most_common(10)]\n",
    "plt.bar(range(len(top_words)), top_counts, alpha=0.7)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Most Common Words')\n",
    "plt.xticks(range(len(top_words)), top_words, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Framework-Specific Text Handling\n",
    "\n",
    "How each framework handles text data in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FRAMEWORK-SPECIFIC TEXT HANDLING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create datasets for both frameworks\n",
    "if PYTORCH_AVAILABLE and TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüìä Framework Comparison:\")\n",
    "    \n",
    "    # PyTorch approach\n",
    "    def pytorch_text_pipeline():\n",
    "        # Custom dataset class\n",
    "        class TextDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, texts, labels, vocab, max_length=20):\n",
    "                self.texts = texts\n",
    "                self.labels = labels\n",
    "                self.vocab = vocab\n",
    "                self.max_length = max_length\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.texts)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                text = self.texts[idx]\n",
    "                label = self.labels[idx]\n",
    "                \n",
    "                # Tokenize and convert to indices\n",
    "                tokens = text.split()\n",
    "                indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "                \n",
    "                # Pad or truncate\n",
    "                if len(indices) > self.max_length:\n",
    "                    indices = indices[:self.max_length]\n",
    "                else:\n",
    "                    indices.extend([self.vocab['<PAD>']] * (self.max_length - len(indices)))\n",
    "                \n",
    "                return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        vocab = preprocessor.word_to_idx\n",
    "        dataset = TextDataset(cleaned_texts, labels, vocab)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "        \n",
    "        return dataset, dataloader\n",
    "    \n",
    "    pt_dataset, pt_dataloader = pytorch_text_pipeline()\n",
    "    print(f\"PyTorch dataset size: {len(pt_dataset)}\")\n",
    "    \n",
    "    # Show a batch\n",
    "    for batch_texts, batch_labels in pt_dataloader:\n",
    "        print(f\"PyTorch batch - texts: {batch_texts.shape}, labels: {batch_labels.shape}\")\n",
    "        print(f\"First text in batch: {batch_texts[0]}\")\n",
    "        break\n",
    "    \n",
    "    # TensorFlow approach\n",
    "    def tensorflow_text_pipeline():\n",
    "        # Use Keras preprocessing\n",
    "        tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts(cleaned_texts)\n",
    "        \n",
    "        sequences = tokenizer.texts_to_sequences(cleaned_texts)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=20, padding='post')\n",
    "        \n",
    "        # Create tf.data.Dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "            'text': padded_sequences,\n",
    "            'label': labels\n",
    "        })\n",
    "        \n",
    "        dataset = dataset.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return tokenizer, dataset\n",
    "    \n",
    "    tf_tokenizer, tf_dataset = tensorflow_text_pipeline()\n",
    "    print(f\"TensorFlow tokenizer vocab size: {len(tf_tokenizer.word_index)}\")\n",
    "    \n",
    "    # Show a batch\n",
    "    for batch in tf_dataset.take(1):\n",
    "        print(f\"TensorFlow batch - texts: {batch['text'].shape}, labels: {batch['label'].shape}\")\n",
    "        print(f\"First text in batch: {batch['text'][0]}\")\n",
    "\n",
    "# Text preprocessing comparison\n",
    "pytorch_text_code = \"\"\"\n",
    "# PyTorch text preprocessing\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Convert to indices\n",
    "        indices = [self.vocab.get(token, self.vocab['<UNK>']) \n",
    "                  for token in tokens]\n",
    "        \n",
    "        # Pad/truncate\n",
    "        if len(indices) > self.max_len:\n",
    "            indices = indices[:self.max_len]\n",
    "        else:\n",
    "            indices += [0] * (self.max_len - len(indices))\n",
    "        \n",
    "        return torch.tensor(indices), torch.tensor(label)\n",
    "\n",
    "# Usage\n",
    "dataset = TextDataset(texts, labels, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_text_code = \"\"\"\n",
    "# TensorFlow text preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=10000,\n",
    "    oov_token='<OOV>'\n",
    ")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "padded = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=128,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'text': padded,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_text_code, tensorflow_text_code, \"Text Preprocessing Pipelines\"\n",
    "))\n",
    "\n",
    "# Performance comparison\n",
    "if PYTORCH_AVAILABLE and TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n‚ö° Performance Comparison:\")\n",
    "    \n",
    "    # Time PyTorch preprocessing\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(10):\n",
    "        for batch in pt_dataloader:\n",
    "            pass\n",
    "    pt_time = (time.time() - start_time) / 10\n",
    "    \n",
    "    # Time TensorFlow preprocessing\n",
    "    start_time = time.time()\n",
    "    for _ in range(10):\n",
    "        for batch in tf_dataset:\n",
    "            pass\n",
    "    tf_time = (time.time() - start_time) / 10\n",
    "    \n",
    "    print(f\"PyTorch data loading: {pt_time:.4f}s per epoch\")\n",
    "    print(f\"TensorFlow data loading: {tf_time:.4f}s per epoch\")\n",
    "    \n",
