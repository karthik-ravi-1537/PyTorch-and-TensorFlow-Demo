{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Modeling: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Build RNN/LSTM models for sequence tasks\n",
    "- Compare sequence modeling approaches\n",
    "- Handle variable-length sequences\n",
    "- Understand encoder-decoder patterns\n",
    "\n",
    "**Prerequisites:** Text preprocessing, embeddings\n",
    "\n",
    "**Estimated Time:** 45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence Data Preparation\n",
    "\n",
    "Creating sequence data for modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SEQUENCE DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create simple sequence prediction task (next number prediction)\n",
    "def create_sequence_data(num_samples=1000, seq_length=10):\n",
    "    \"\"\"Create sequences for next-number prediction\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Create arithmetic sequence\n",
    "        start = np.random.randint(1, 10)\n",
    "        step = np.random.randint(1, 5)\n",
    "        \n",
    "        seq = [start + i * step for i in range(seq_length)]\n",
    "        target = start + seq_length * step  # Next number in sequence\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Generate data\n",
    "X, y = create_sequence_data(num_samples=2000, seq_length=8)\n",
    "\n",
    "print(f\"Data shapes:\")\n",
    "print(f\"  Sequences: {X.shape}\")\n",
    "print(f\"  Targets: {y.shape}\")\n",
    "\n",
    "print(f\"\\nSample sequences:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Sequence {i+1}: {X[i]} ‚Üí {y[i]}\")\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Training: {X_train.shape[0]} samples\")\n",
    "print(f\"  Testing: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Normalize data\n",
    "X_mean, X_std = X_train.mean(), X_train.std()\n",
    "y_mean, y_std = y_train.mean(), y_train.std()\n",
    "\n",
    "X_train_norm = (X_train - X_mean) / X_std\n",
    "X_test_norm = (X_test - X_mean) / X_std\n",
    "y_train_norm = (y_train - y_mean) / y_std\n",
    "y_test_norm = (y_test - y_mean) / y_std\n",
    "\n",
    "print(f\"\\nNormalization stats:\")\n",
    "print(f\"  X: mean={X_mean:.2f}, std={X_std:.2f}\")\n",
    "print(f\"  y: mean={y_mean:.2f}, std={y_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Sequence Models\n",
    "\n",
    "Building sequence models with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PYTORCH SEQUENCE MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dataset class\n",
    "    class SequenceDataset(Dataset):\n",
    "        def __init__(self, sequences, targets):\n",
    "            self.sequences = torch.FloatTensor(sequences)\n",
    "            self.targets = torch.FloatTensor(targets)\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.sequences)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.sequences[idx], self.targets[idx]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SequenceDataset(X_train_norm, y_train_norm)\n",
    "    test_dataset = SequenceDataset(X_test_norm, y_test_norm)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # LSTM Model\n",
    "    class LSTMSequenceModel(nn.Module):\n",
    "        def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n",
    "            super().__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            \n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                              batch_first=True, dropout=0.2)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # x shape: (batch_size, seq_len)\n",
    "            x = x.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "            \n",
    "            # LSTM forward pass\n",
    "            lstm_out, (hidden, _) = self.lstm(x)\n",
    "            \n",
    "            # Use the last output\n",
    "            last_output = lstm_out[:, -1, :]  # (batch_size, hidden_size)\n",
    "            \n",
    "            # Final prediction\n",
    "            output = self.fc(self.dropout(last_output))\n",
    "            return output.squeeze(-1)  # (batch_size,)\n",
    "    \n",
    "    # GRU Model for comparison\n",
    "    class GRUSequenceModel(nn.Module):\n",
    "        def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n",
    "            super().__init__()\n",
    "            self.gru = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                             batch_first=True, dropout=0.2)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(-1)\n",
    "            gru_out, _ = self.gru(x)\n",
    "            last_output = gru_out[:, -1, :]\n",
    "            output = self.fc(self.dropout(last_output))\n",
    "            return output.squeeze(-1)\n",
    "    \n",
    "    # Training function\n",
    "    def train_pytorch_model(model, train_loader, test_loader, epochs=20):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Testing\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in test_loader:\n",
    "                    outputs = model(batch_x)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    test_loss += loss.item()\n",
    "            \n",
    "            test_loss = test_loss / len(test_loader)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "        \n",
    "        return train_losses, test_losses\n",
    "    \n",
    "    # Train LSTM model\n",
    "    print(\"\\nüî• Training LSTM Model:\")\n",
    "    lstm_model = LSTMSequenceModel()\n",
    "    lstm_train_losses, lstm_test_losses = train_pytorch_model(lstm_model, train_loader, test_loader)\n",
    "    \n",
    "    # Train GRU model\n",
    "    print(\"\\nüî• Training GRU Model:\")\n",
    "    gru_model = GRUSequenceModel()\n",
    "    gru_train_losses, gru_test_losses = train_pytorch_model(gru_model, train_loader, test_loader)\n",
    "    \n",
    "    # Store results\n",
    "    pytorch_results = {\n",
    "        'lstm_test_loss': lstm_test_losses[-1],\n",
    "        'gru_test_loss': gru_test_losses[-1],\n",
    "        'lstm_model': lstm_model,\n",
    "        'gru_model': gru_model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä PyTorch Results:\")\n",
    "    print(f\"  LSTM Test Loss: {lstm_test_losses[-1]:.4f}\")\n",
    "    print(f\"  GRU Test Loss: {gru_test_losses[-1]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"PyTorch not available - skipping PyTorch models\")\n",
    "    pytorch_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow Sequence Models\n",
    "\n",
    "Building equivalent models with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TENSORFLOW SEQUENCE MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Reshape data for TensorFlow (add feature dimension)\n",
    "    X_train_tf = X_train_norm.reshape(X_train_norm.shape[0], X_train_norm.shape[1], 1)\n",
    "    X_test_tf = X_test_norm.reshape(X_test_norm.shape[0], X_test_norm.shape[1], 1)\n",
    "    \n",
    "    print(f\"TensorFlow data shapes:\")\n",
    "    print(f\"  X_train: {X_train_tf.shape}\")\n",
    "    print(f\"  y_train: {y_train_norm.shape}\")\n",
    "    \n",
    "    # LSTM Model\n",
    "    def create_lstm_model(input_shape):\n",
    "        model = models.Sequential([\n",
    "            layers.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2,\n",
    "                       input_shape=input_shape),\n",
    "            layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    # GRU Model\n",
    "    def create_gru_model(input_shape):\n",
    "        model = models.Sequential([\n",
    "            layers.GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2,\n",
    "                      input_shape=input_shape),\n",
    "            layers.GRU(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    # Train LSTM model\n",
    "    print(\"\\nüü† Training LSTM Model:\")\n",
    "    tf_lstm_model = create_lstm_model((X_train_tf.shape[1], 1))\n",
    "    \n",
    "    lstm_history = tf_lstm_model.fit(\n",
    "        X_train_tf, y_train_norm,\n",
    "        batch_size=32,\n",
    "        epochs=20,\n",
    "        validation_data=(X_test_tf, y_test_norm),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Train GRU model\n",
    "    print(\"\\nüü† Training GRU Model:\")\n",
    "    tf_gru_model = create_gru_model((X_train_tf.shape[1], 1))\n",
    "    \n",
    "    gru_history = tf_gru_model.fit(\n",
    "        X_train_tf, y_train_norm,\n",
    "        batch_size=32,\n",
    "        epochs=20,\n",
    "        validation_data=(X_test_tf, y_test_norm),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    tensorflow_results = {\n",
    "        'lstm_test_loss': lstm_history.history['val_loss'][-1],\n",
    "        'gru_test_loss': gru_history.history['val_loss'][-1],\n",
    "        'lstm_model': tf_lstm_model,\n",
    "        'gru_model': tf_gru_model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä TensorFlow Results:\")\n",
    "    print(f\"  LSTM Test Loss: {lstm_history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"  GRU Test Loss: {gru_history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping TensorFlow models\")\n",
    "    tensorflow_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison and Evaluation\n",
    "\n",
    "Comparing sequence models across frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON AND EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Framework comparison\n",
    "if pytorch_results and tensorflow_results:\n",
    "    print(\"\\nüìä Framework Comparison:\")\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Model': ['LSTM', 'GRU'],\n",
    "        'PyTorch': [pytorch_results['lstm_test_loss'], pytorch_results['gru_test_loss']],\n",
    "        'TensorFlow': [tensorflow_results['lstm_test_loss'], tensorflow_results['gru_test_loss']]\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Model':<10} | {'PyTorch':<12} | {'TensorFlow':<12}\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, model in enumerate(comparison_data['Model']):\n",
    "        pt_loss = comparison_data['PyTorch'][i]\n",
    "        tf_loss = comparison_data['TensorFlow'][i]\n",
    "        print(f\"{model:<10} | {pt_loss:<12.4f} | {tf_loss:<12.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(comparison_data['Model']))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, comparison_data['PyTorch'], width, label='PyTorch', color='#EE4C2C', alpha=0.7)\n",
    "    plt.bar(x + width/2, comparison_data['TensorFlow'], width, label='TensorFlow', color='#FF6F00', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Model Architecture')\n",
    "    plt.ylabel('Test Loss (MSE)')\n",
    "    plt.title('Sequence Modeling: PyTorch vs TensorFlow')\n",
    "    plt.xticks(x, comparison_data['Model'])\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (pt_loss, tf_loss) in enumerate(zip(comparison_data['PyTorch'], comparison_data['TensorFlow'])):\n",
    "        plt.text(i - width/2, pt_loss + 0.001, f'{pt_loss:.3f}', ha='center', va='bottom')\n",
    "        plt.text(i + width/2, tf_loss + 0.001, f'{tf_loss:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Prediction examples\n",
    "def make_predictions(model, X_test, framework='pytorch'):\n",
    "    \"\"\"Make predictions and denormalize\"\"\"\n",
    "    if framework == 'pytorch':\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_test)\n",
    "            predictions = model(X_tensor).numpy()\n",
    "    else:  # tensorflow\n",
    "        X_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "        predictions = model.predict(X_reshaped, verbose=0).flatten()\n",
    "    \n",
    "    # Denormalize predictions\n",
    "    predictions_denorm = predictions * y_std + y_mean\n",
    "    return predictions_denorm\n",
    "\n",
    "# Show prediction examples\n",
    "print(\"\\nüîç Prediction Examples:\")\n",
    "\n",
    "# Get a few test examples\n",
    "test_indices = [0, 1, 2, 3, 4]\n",
    "test_sequences = X_test[test_indices]\n",
    "test_targets = y_test[test_indices]\n",
    "test_sequences_norm = X_test_norm[test_indices]\n",
    "\n",
    "if pytorch_results:\n",
    "    # Use best PyTorch model\n",
    "    best_pt_model = pytorch_results['lstm_model'] if pytorch_results['lstm_test_loss'] <= pytorch_results['gru_test_loss'] else pytorch_results['gru_model']\n",
    "    pt_predictions = make_predictions(best_pt_model, test_sequences_norm, 'pytorch')\n",
    "\n",
    "if tensorflow_results:\n",
    "    # Use best TensorFlow model\n",
    "    best_tf_model = tensorflow_results['lstm_model'] if tensorflow_results['lstm_test_loss'] <= tensorflow_results['gru_test_loss'] else tensorflow_results['gru_model']\n",
    "    tf_predictions = make_predictions(best_tf_model, test_sequences_norm, 'tensorflow')\n",
    "\n",
    "print(f\"{'Sequence':<30} | {'True':<8} | {'PyTorch':<8} | {'TensorFlow':<8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, (seq, target) in enumerate(zip(test_sequences, test_targets)):\n",
    "    seq_str = str(list(seq))\n",
    "    if len(seq_str) > 28:\n",
    "        seq_str = seq_str[:25] + '...'\n",
    "    \n",
    "    pt_pred = pt_predictions[i] if pytorch_results else 0\n",
    "    tf_pred = tf_predictions[i] if tensorflow_results else 0\n",
    "    \n",
    "    print(f\"{seq_str:<30} | {target:<8.1f} | {pt_pred:<8.1f} | {tf_pred:<8.1f}\")\n",
    "\n",
    "# Side-by-side code comparison\n",
    "pytorch_sequence_code = \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMSequenceModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)  # Add feature dimension\n",
    "        lstm_out, (hidden, _) = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]  # Last timestep\n",
    "        output = self.fc(self.dropout(last_output))\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "# Training\n",
    "model = LSTMSequenceModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_sequence_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.LSTM(64, return_sequences=True, dropout=0.2, \n",
    "                   recurrent_dropout=0.2, input_shape=input_shape),\n",
    "        layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Training\n",
    "model = create_lstm_model((sequence_length, 1))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + create_side_by_side_comparison(\n",
    "    pytorch_sequence_code, tensorflow_sequence_code, \"Sequence Modeling Implementation\"\n",
    "))\n",
    "\n",
    "print(\"\\n‚úÖ Key Takeaways:\")\n",
    "print(\"  ‚Ä¢ Both LSTM and GRU are effective for sequence modeling\")\n",
    "print(\"  ‚Ä¢ GRU is often faster to train with similar performance\")\n",
    "print(\"  ‚Ä¢ Proper data normalization is crucial for sequence tasks\")\n",
    "print(\"  ‚Ä¢ Both frameworks provide excellent sequence modeling capabilities\")\n",
    "print(\"  ‚Ä¢ TensorFlow's Keras API makes model building more concise\")\n",
    "print(\"  ‚Ä¢ PyTorch offers more flexibility for custom architectures\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}