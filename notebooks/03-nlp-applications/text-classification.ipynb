{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Build text classifiers using both frameworks\n",
    "- Compare neural network architectures for text classification\n",
    "- Learn evaluation metrics and model comparison techniques\n",
    "- Master end-to-end text classification pipelines\n",
    "\n",
    "**Prerequisites:** Text preprocessing, embeddings comparison\n",
    "\n",
    "**Estimated Time:** 45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from foundations.data_utils import get_tutorial_text_data\n",
    "from foundations.preprocessing import TextPreprocessor\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Preparing text data for classification in both frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get text classification data\n",
    "data = get_tutorial_text_data(num_samples=2000, task_type='classification')\n",
    "texts = data['texts']\n",
    "labels = data['labels']\n",
    "label_names = data['label_names']\n",
    "\n",
    "print(f\"Dataset info:\")\n",
    "print(f\"  Total samples: {len(texts)}\")\n",
    "print(f\"  Number of classes: {len(set(labels))}\")\n",
    "print(f\"  Class names: {label_names}\")\n",
    "print(f\"  Class distribution: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n",
    "\n",
    "# Show sample texts\n",
    "print(f\"\\nSample texts:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {i+1}. [{label_names[labels[i]]}] {texts[i][:100]}...\")\n",
    "\n",
    "# Prepare train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "\n",
    "# Build vocabulary\n",
    "preprocessor = TextPreprocessor()\n",
    "preprocessor.build_vocabulary(X_train, min_freq=2, max_vocab_size=5000)\n",
    "\n",
    "vocab_size = preprocessor.vocab_size\n",
    "max_length = 100\n",
    "\n",
    "print(f\"\\nVocabulary info:\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Max sequence length: {max_length}\")\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_seq = preprocessor.texts_to_sequences(X_train, max_length=max_length)\n",
    "X_test_seq = preprocessor.texts_to_sequences(X_test, max_length=max_length)\n",
    "\n",
    "print(f\"\\nSequence conversion:\")\n",
    "print(f\"  Training sequences shape: {np.array(X_train_seq).shape}\")\n",
    "print(f\"  Test sequences shape: {np.array(X_test_seq).shape}\")\n",
    "print(f\"  Sample sequence: {X_train_seq[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Text Classification\n",
    "\n",
    "Building and training text classifiers with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PYTORCH TEXT CLASSIFICATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Custom Dataset class\n",
    "    class TextDataset(Dataset):\n",
    "        def __init__(self, sequences, labels):\n",
    "            self.sequences = torch.LongTensor(sequences)\n",
    "            self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.sequences)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.sequences[idx], self.labels[idx]\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TextDataset(X_train_seq, y_train)\n",
    "    test_dataset = TextDataset(X_test_seq, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(f\"DataLoaders created:\")\n",
    "    print(f\"  Training batches: {len(train_loader)}\")\n",
    "    print(f\"  Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Model 1: Simple CNN classifier\n",
    "    class CNNTextClassifier(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_dim, num_classes, num_filters=100):\n",
    "            super().__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "            \n",
    "            # Multiple filter sizes for different n-grams\n",
    "            self.conv1 = nn.Conv1d(embed_dim, num_filters, kernel_size=3, padding=1)\n",
    "            self.conv2 = nn.Conv1d(embed_dim, num_filters, kernel_size=4, padding=2)\n",
    "            self.conv3 = nn.Conv1d(embed_dim, num_filters, kernel_size=5, padding=2)\n",
    "            \n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.fc = nn.Linear(num_filters * 3, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # x shape: (batch_size, seq_len)\n",
    "            embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "            embedded = embedded.transpose(1, 2)  # (batch_size, embed_dim, seq_len)\n",
    "            \n",
    "            # Apply convolutions\n",
    "            conv1_out = torch.relu(self.conv1(embedded))\n",
    "            conv2_out = torch.relu(self.conv2(embedded))\n",
    "            conv3_out = torch.relu(self.conv3(embedded))\n",
    "            \n",
    "            # Global max pooling\n",
    "            pool1 = torch.max(conv1_out, dim=2)[0]\n",
    "            pool2 = torch.max(conv2_out, dim=2)[0]\n",
    "            pool3 = torch.max(conv3_out, dim=2)[0]\n",
    "            \n",
    "            # Concatenate and classify\n",
    "            pooled = torch.cat([pool1, pool2, pool3], dim=1)\n",
    "            pooled = self.dropout(pooled)\n",
    "            output = self.fc(pooled)\n",
    "            \n",
    "            return output\n",
    "    \n",
    "    # Model 2: LSTM classifier\n",
    "    class LSTMTextClassifier(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2):\n",
    "            super().__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "            self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                              batch_first=True, dropout=0.3, bidirectional=True)\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # x shape: (batch_size, seq_len)\n",
    "            embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "            \n",
    "            # LSTM\n",
    "            lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "            \n",
    "            # Use the last hidden state\n",
    "            # hidden shape: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "            # Take the last layer's hidden state from both directions\n",
    "            forward_hidden = hidden[-2]  # Last layer, forward direction\n",
    "            backward_hidden = hidden[-1]  # Last layer, backward direction\n",
    "            \n",
    "            # Concatenate forward and backward hidden states\n",
    "            final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "            \n",
    "            # Classify\n",
    "            final_hidden = self.dropout(final_hidden)\n",
    "            output = self.fc(final_hidden)\n",
    "            \n",
    "            return output\n",
    "    \n",
    "    # Training function\n",
    "    def train_pytorch_model(model, train_loader, test_loader, epochs=5):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "                total += target.size(0)\n",
    "            \n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = correct / total\n",
    "            \n",
    "            # Testing\n",
    "            model.eval()\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    output = model(data)\n",
    "                    pred = output.argmax(dim=1)\n",
    "                    test_correct += pred.eq(target).sum().item()\n",
    "                    test_total += target.size(0)\n",
    "            \n",
    "            test_acc = test_correct / test_total\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}: '\n",
    "                  f'Loss: {train_loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.4f}, '\n",
    "                  f'Test Acc: {test_acc:.4f}')\n",
    "        \n",
    "        return train_losses, train_accuracies, test_accuracies\n",
    "    \n",
    "    # Train CNN model\n",
    "    print(\"\\nüî• Training CNN Text Classifier:\")\n",
    "    cnn_model = CNNTextClassifier(vocab_size, embed_dim=128, num_classes=len(label_names))\n",
    "    cnn_losses, cnn_train_acc, cnn_test_acc = train_pytorch_model(cnn_model, train_loader, test_loader)\n",
    "    \n",
    "    # Train LSTM model\n",
    "    print(\"\\nüî• Training LSTM Text Classifier:\")\n",
    "    lstm_model = LSTMTextClassifier(vocab_size, embed_dim=128, hidden_dim=64, num_classes=len(label_names))\n",
    "    lstm_losses, lstm_train_acc, lstm_test_acc = train_pytorch_model(lstm_model, train_loader, test_loader)\n",
    "    \n",
    "    # Store PyTorch results\n",
    "    pytorch_results = {\n",
    "        'cnn_test_acc': cnn_test_acc[-1],\n",
    "        'lstm_test_acc': lstm_test_acc[-1],\n",
    "        'cnn_model': cnn_model,\n",
    "        'lstm_model': lstm_model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä PyTorch Results:\")\n",
    "    print(f\"  CNN Test Accuracy: {cnn_test_acc[-1]:.4f}\")\n",
    "    print(f\"  LSTM Test Accuracy: {lstm_test_acc[-1]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"PyTorch not available - skipping PyTorch models\")\n",
    "    pytorch_results = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow Text Classification\n",
    "\n",
    "Building equivalent models with TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TENSORFLOW TEXT CLASSIFICATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Convert data to TensorFlow format\n",
    "    X_train_tf = np.array(X_train_seq)\n",
    "    X_test_tf = np.array(X_test_seq)\n",
    "    y_train_tf = np.array(y_train)\n",
    "    y_test_tf = np.array(y_test)\n",
    "    \n",
    "    print(f\"TensorFlow data shapes:\")\n",
    "    print(f\"  X_train: {X_train_tf.shape}\")\n",
    "    print(f\"  y_train: {y_train_tf.shape}\")\n",
    "    \n",
    "    # Model 1: CNN Text Classifier\n",
    "    def create_cnn_model(vocab_size, embed_dim, num_classes, max_length):\n",
    "        model = models.Sequential([\n",
    "            layers.Embedding(vocab_size, embed_dim, input_length=max_length, mask_zero=True),\n",
    "            \n",
    "            # Multiple conv layers with different kernel sizes\n",
    "            layers.Conv1D(100, 3, activation='relu', padding='same'),\n",
    "            layers.GlobalMaxPooling1D(),\n",
    "            \n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Model 2: LSTM Text Classifier\n",
    "    def create_lstm_model(vocab_size, embed_dim, num_classes, max_length):\n",
    "        model = models.Sequential([\n",
    "            layers.Embedding(vocab_size, embed_dim, input_length=max_length, mask_zero=True),\n",
    "            layers.Bidirectional(layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3)),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Train CNN model\n",
    "    print(\"\\nüü† Training CNN Text Classifier:\")\n",
    "    tf_cnn_model = create_cnn_model(vocab_size, 128, len(label_names), max_length)\n",
    "    \n",
    "    cnn_history = tf_cnn_model.fit(\n",
    "        X_train_tf, y_train_tf,\n",
    "        batch_size=32,\n",
    "        epochs=5,\n",
    "        validation_data=(X_test_tf, y_test_tf),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train LSTM model\n",
    "    print(\"\\nüü† Training LSTM Text Classifier:\")\n",
    "    tf_lstm_model = create_lstm_model(vocab_size, 128, len(label_names), max_length)\n",
    "    \n",
    "    lstm_history = tf_lstm_model.fit(\n",
    "        X_train_tf, y_train_tf,\n",
    "        batch_size=32,\n",
    "        epochs=5,\n",
    "        validation_data=(X_test_tf, y_test_tf),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Store TensorFlow results\n",
    "    tensorflow_results = {\n",
    "        'cnn_test_acc': cnn_history.history['val_accuracy'][-1],\n",
    "        'lstm_test_acc': lstm_history.history['val_accuracy'][-1],\n",
    "        'cnn_model': tf_cnn_model,\n",
    "        'lstm_model': tf_lstm_model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä TensorFlow Results:\")\n",
    "    print(f\"  CNN Test Accuracy: {cnn_history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"  LSTM Test Accuracy: {lstm_history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping TensorFlow models\")\n",
    "    tensorflow_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison and Evaluation\n",
    "\n",
    "Comparing the performance of different models and frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON AND EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Framework comparison\n",
    "if pytorch_results and tensorflow_results:\n",
    "    print(\"\\nüìä Framework Comparison:\")\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Model': ['CNN', 'LSTM'],\n",
    "        'PyTorch': [pytorch_results['cnn_test_acc'], pytorch_results['lstm_test_acc']],\n",
    "        'TensorFlow': [tensorflow_results['cnn_test_acc'], tensorflow_results['lstm_test_acc']]\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Model':<10} | {'PyTorch':<10} | {'TensorFlow':<10}\")\n",
    "    print(\"-\" * 35)\n",
    "    for i, model in enumerate(comparison_data['Model']):\n",
    "        pt_acc = comparison_data['PyTorch'][i]\n",
    "        tf_acc = comparison_data['TensorFlow'][i]\n",
    "        print(f\"{model:<10} | {pt_acc:<10.4f} | {tf_acc:<10.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(comparison_data['Model']))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, comparison_data['PyTorch'], width, label='PyTorch', color='#EE4C2C', alpha=0.7)\n",
    "    plt.bar(x + width/2, comparison_data['TensorFlow'], width, label='TensorFlow', color='#FF6F00', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Model Architecture')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title('Text Classification: PyTorch vs TensorFlow')\n",
    "    plt.xticks(x, comparison_data['Model'])\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (pt_acc, tf_acc) in enumerate(zip(comparison_data['PyTorch'], comparison_data['TensorFlow'])):\n",
    "        plt.text(i - width/2, pt_acc + 0.01, f'{pt_acc:.3f}', ha='center', va='bottom')\n",
    "        plt.text(i + width/2, tf_acc + 0.01, f'{tf_acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Detailed evaluation with best model\n",
    "def evaluate_model_detailed(model, test_loader, framework='pytorch'):\n",
    "    \"\"\"Detailed evaluation with classification report\"\"\"\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    if framework == 'pytorch':\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                pred = output.argmax(dim=1)\n",
    "                all_preds.extend(pred.cpu().numpy())\n",
    "                all_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    elif framework == 'tensorflow':\n",
    "        predictions = model.predict(X_test_tf, verbose=0)\n",
    "        all_preds = np.argmax(predictions, axis=1)\n",
    "        all_labels = y_test_tf\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=label_names)\n",
    "    \n",
    "    return accuracy, report, all_preds, all_labels\n",
    "\n",
    "# Evaluate best models\n",
    "if pytorch_results:\n",
    "    print(\"\\nüî• Best PyTorch Model Evaluation:\")\n",
    "    best_pt_model = pytorch_results['lstm_model'] if pytorch_results['lstm_test_acc'] > pytorch_results['cnn_test_acc'] else pytorch_results['cnn_model']\n",
    "    best_pt_name = 'LSTM' if pytorch_results['lstm_test_acc'] > pytorch_results['cnn_test_acc'] else 'CNN'\n",
    "    \n",
    "    pt_acc, pt_report, pt_preds, pt_labels = evaluate_model_detailed(best_pt_model, test_loader, 'pytorch')\n",
    "    print(f\"Best PyTorch model: {best_pt_name}\")\n",
    "    print(f\"Accuracy: {pt_acc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(pt_report)\n",
    "\n",
    "if tensorflow_results:\n",
    "    print(\"\\nüü† Best TensorFlow Model Evaluation:\")\n",
    "    best_tf_model = tensorflow_results['lstm_model'] if tensorflow_results['lstm_test_acc'] > tensorflow_results['cnn_test_acc'] else tensorflow_results['cnn_model']\n",
    "    best_tf_name = 'LSTM' if tensorflow_results['lstm_test_acc'] > tensorflow_results['cnn_test_acc'] else 'CNN'\n",
    "    \n",
    "    tf_acc, tf_report, tf_preds, tf_labels = evaluate_model_detailed(best_tf_model, None, 'tensorflow')\n",
    "    print(f\"Best TensorFlow model: {best_tf_name}\")\n",
    "    print(f\"Accuracy: {tf_acc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(tf_report)\n",
    "\n",
    "# Confusion matrix visualization\n",
    "if pytorch_results or tensorflow_results:\n",
    "    print(\"\\nüìà Confusion Matrix:\")\n",
    "    \n",
    "    # Use the best available model\n",
    "    if pytorch_results and tensorflow_results:\n",
    "        if pt_acc >= tf_acc:\n",
    "            preds, labels, title = pt_preds, pt_labels, f\"PyTorch {best_pt_name}\"\n",
    "        else:\n",
    "            preds, labels, title = tf_preds, tf_labels, f\"TensorFlow {best_tf_name}\"\n",
    "    elif pytorch_results:\n",
    "        preds, labels, title = pt_preds, pt_labels, f\"PyTorch {best_pt_name}\"\n",
    "    else:\n",
    "        preds, labels, title = tf_preds, tf_labels, f\"TensorFlow {best_tf_name}\"\n",
    "    \n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.title(f'Confusion Matrix - {title}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Side-by-side code comparison\n",
    "pytorch_classifier_code = \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, \n",
    "                           bidirectional=True, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use last hidden state\n",
    "        forward_hidden = hidden[-2]\n",
    "        backward_hidden = hidden[-1]\n",
    "        final_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        \n",
    "        output = self.fc(self.dropout(final_hidden))\n",
    "        return output\n",
    "\n",
    "# Training\n",
    "model = LSTMTextClassifier(vocab_size, 128, 64, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_classifier_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_lstm_model(vocab_size, embed_dim, num_classes, max_length):\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, embed_dim, \n",
    "                        input_length=max_length, mask_zero=True),\n",
    "        layers.Bidirectional(\n",
    "            layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3)\n",
    "        ),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training\n",
    "model = create_lstm_model(vocab_size, 128, num_classes, max_length)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + create_side_by_side_comparison(\n",
    "    pytorch_classifier_code, tensorflow_classifier_code, \"Text Classification Implementation\"\n",
    "))\n",
    "\n",
    "print(\"\\n‚úÖ Key Takeaways:\")\n",
    "print(\"  ‚Ä¢ Both frameworks can build effective text classifiers\")\n",
    "print(\"  ‚Ä¢ LSTM models often outperform CNN for text classification\")\n",
    "print(\"  ‚Ä¢ Bidirectional LSTMs capture context from both directions\")\n",
    "print(\"  ‚Ä¢ Proper evaluation requires multiple metrics beyond accuracy\")\n",
    "print(\"  ‚Ä¢ Framework choice depends on deployment and team preferences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
