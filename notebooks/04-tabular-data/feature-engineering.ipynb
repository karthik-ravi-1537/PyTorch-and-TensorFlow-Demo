{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master preprocessing techniques for neural networks\n",
    "- Compare categorical encoding approaches\n",
    "- Learn normalization and feature selection methods\n",
    "- Build framework-specific data pipelines\n",
    "\n",
    "**Prerequisites:** NumPy/Pandas foundations, data preparation\n",
    "\n",
    "**Estimated Time:** 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from foundations.data_utils import get_tutorial_tabular_data\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"✅ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"❌ PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"✅ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"❌ TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "Loading tabular data and understanding its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADING AND EXPLORATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load tabular data with mixed types\n",
    "data = get_tutorial_tabular_data(num_samples=2000, return_as_dataframe=True, include_categorical=True)\n",
    "df = data['dataframe']\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['target'].value_counts().sort_index())\n",
    "\n",
    "# Identify column types\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remove target from features\n",
    "if 'target' in numeric_columns:\n",
    "    numeric_columns.remove('target')\n",
    "if 'target' in categorical_columns:\n",
    "    categorical_columns.remove('target')\n",
    "\n",
    "print(f\"\\nColumn types:\")\n",
    "print(f\"  Numeric columns ({len(numeric_columns)}): {numeric_columns}\")\n",
    "print(f\"  Categorical columns ({len(categorical_columns)}): {categorical_columns}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nNumeric features statistics:\")\n",
    "print(df[numeric_columns].describe())\n",
    "\n",
    "if categorical_columns:\n",
    "    print(f\"\\nCategorical features:\")\n",
    "    for col in categorical_columns:\n",
    "        print(f\"  {col}: {df[col].nunique()} unique values - {list(df[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Techniques\n",
    "\n",
    "Applying various preprocessing techniques to prepare data for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING TECHNIQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# 1. Handle missing values (if any)\n",
    "print(\"\\n1. Missing Value Analysis:\")\n",
    "missing_counts = df_processed.isnull().sum()\n",
    "print(f\"Missing values per column:\")\n",
    "for col, count in missing_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count} ({count/len(df_processed)*100:.1f}%)\")\n",
    "\n",
    "if missing_counts.sum() == 0:\n",
    "    print(\"  No missing values found ✅\")\n",
    "else:\n",
    "    # Fill missing values\n",
    "    for col in numeric_columns:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "\n",
    "# 2. Categorical Encoding\n",
    "print(\"\\n2. Categorical Encoding:\")\n",
    "\n",
    "encoded_dfs = {}\n",
    "\n",
    "if categorical_columns:\n",
    "    # Method 1: Label Encoding\n",
    "    df_label_encoded = df_processed.copy()\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_label_encoded[col] = le.fit_transform(df_label_encoded[col])\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  Label encoded {col}: {df_processed[col].nunique()} categories → {df_label_encoded[col].nunique()} integers\")\n",
    "    \n",
    "    encoded_dfs['label_encoded'] = df_label_encoded\n",
    "    \n",
    "    # Method 2: One-Hot Encoding\n",
    "    df_onehot = df_processed.copy()\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        # Create dummy variables\n",
    "        dummies = pd.get_dummies(df_onehot[col], prefix=col, drop_first=True)\n",
    "        df_onehot = pd.concat([df_onehot, dummies], axis=1)\n",
    "        df_onehot.drop(col, axis=1, inplace=True)\n",
    "        print(f\"  One-hot encoded {col}: {df_processed[col].nunique()} categories → {len(dummies.columns)} binary features\")\n",
    "    \n",
    "    encoded_dfs['onehot_encoded'] = df_onehot\n",
    "    \n",
    "    print(f\"\\n  Original shape: {df_processed.shape}\")\n",
    "    print(f\"  Label encoded shape: {df_label_encoded.shape}\")\n",
    "    print(f\"  One-hot encoded shape: {df_onehot.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"  No categorical columns to encode\")\n",
    "    encoded_dfs['original'] = df_processed\n",
    "\n",
    "# 3. Feature Scaling\n",
    "print(\"\\n3. Feature Scaling:\")\n",
    "\n",
    "# Use label encoded version for scaling demonstration\n",
    "df_for_scaling = encoded_dfs.get('label_encoded', df_processed).copy()\n",
    "\n",
    "# Separate features and target\n",
    "X = df_for_scaling.drop('target', axis=1)\n",
    "y = df_for_scaling['target']\n",
    "\n",
    "print(f\"  Features shape: {X.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")\n",
    "\n",
    "# Split data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\n  Train set: {X_train.shape}\")\n",
    "print(f\"  Test set: {X_test.shape}\")\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n  Scaling statistics (training set):\")\n",
    "print(f\"    Original - Mean: {X_train.mean().mean():.3f}, Std: {X_train.std().mean():.3f}\")\n",
    "print(f\"    Scaled - Mean: {X_train_scaled.mean():.3f}, Std: {X_train_scaled.std():.3f}\")\n",
    "\n",
    "# 4. Feature Selection\n",
    "print(\"\\n4. Feature Selection:\")\n",
    "\n",
    "# Select top k features using ANOVA F-test\n",
    "k_best = min(10, X_train_scaled.shape[1])  # Select top 10 or all features if less\n",
    "selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "\n",
    "print(f\"  Selected {k_best} best features from {X_train_scaled.shape[1]} total features\")\n",
    "print(f\"  Selected features and their scores:\")\n",
    "for feature, score in zip(selected_features, feature_scores):\n",
    "    print(f\"    {feature}: {score:.2f}\")\n",
    "\n",
    "print(f\"\\n  Final feature matrix shape: {X_train_selected.shape}\")\n",
    "\n",
    "# Store processed data\n",
    "processed_data = {\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'X_train_selected': X_train_selected,\n",
    "    'X_test_selected': X_test_selected,\n",
    "    'y_train': y_train.values,\n",
    "    'y_test': y_test.values,\n",
    "    'feature_names': X.columns.tolist(),\n",
    "    'selected_features': selected_features,\n",
    "    'scaler': scaler,\n",
    "    'selector': selector\n",
    "}\n",
    "\n",
    "print(f\"\\n✅ Feature engineering completed!\")\n",
    "print(f\"  Original features: {len(X.columns)}\")\n",
    "print(f\"  Selected features: {len(selected_features)}\")\n",
    "print(f\"  Training samples: {X_train_selected.shape[0]}\")\n",
    "print(f\"  Test samples: {X_test_selected.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Data Pipeline\n",
    "\n",
    "Creating PyTorch-specific data pipelines and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PYTORCH DATA PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Custom Dataset class with preprocessing\n",
    "    class TabularDataset(Dataset):\n",
    "        def __init__(self, features, targets, transform=None):\n",
    "            self.features = torch.FloatTensor(features)\n",
    "            self.targets = torch.LongTensor(targets)\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.features)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            feature = self.features[idx]\n",
    "            target = self.targets[idx]\n",
    "            \n",
    "            if self.transform:\n",
    "                feature = self.transform(feature)\n",
    "            \n",
    "            return feature, target\n",
    "    \n",
    "    # Custom transforms\n",
    "    class AddNoise:\n",
    "        \"\"\"Add Gaussian noise for data augmentation\"\"\"\n",
    "        def __init__(self, noise_factor=0.1):\n",
    "            self.noise_factor = noise_factor\n",
    "        \n",
    "        def __call__(self, tensor):\n",
    "            noise = torch.randn_like(tensor) * self.noise_factor\n",
    "            return tensor + noise\n",
    "    \n",
    "    class FeatureDropout:\n",
    "        \"\"\"Randomly set some features to zero\"\"\"\n",
    "        def __init__(self, dropout_prob=0.1):\n",
    "            self.dropout_prob = dropout_prob\n",
    "        \n",
    "        def __call__(self, tensor):\n",
    "            mask = torch.rand_like(tensor) > self.dropout_prob\n",
    "            return tensor * mask.float()\n",
    "    \n",
    "    # Create datasets with different preprocessing\n",
    "    print(\"\\n🔥 Creating PyTorch Datasets:\")\n",
    "    \n",
    "    # Basic dataset\n",
    "    train_dataset_basic = TabularDataset(\n",
    "        processed_data['X_train_selected'], \n",
    "        processed_data['y_train']\n",
    "    )\n",
    "    \n",
    "    test_dataset_basic = TabularDataset(\n",
    "        processed_data['X_test_selected'], \n",
    "        processed_data['y_test']\n",
    "    )\n",
    "    \n",
    "    # Dataset with augmentation\n",
    "    train_dataset_augmented = TabularDataset(\n",
    "        processed_data['X_train_selected'], \n",
    "        processed_data['y_train'],\n",
    "        transform=AddNoise(noise_factor=0.05)\n",
    "    )\n",
    "    \n",
    "    print(f\"  Basic training dataset: {len(train_dataset_basic)} samples\")\n",
    "    print(f\"  Augmented training dataset: {len(train_dataset_augmented)} samples\")\n",
    "    print(f\"  Test dataset: {len(test_dataset_basic)} samples\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader_basic = DataLoader(train_dataset_basic, batch_size=64, shuffle=True)\n",
    "    train_loader_augmented = DataLoader(train_dataset_augmented, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset_basic, batch_size=64, shuffle=False)\n",
    "    \n",
    "    print(f\"\\n  DataLoaders created:\")\n",
    "    print(f\"    Basic training batches: {len(train_loader_basic)}\")\n",
    "    print(f\"    Augmented training batches: {len(train_loader_augmented)}\")\n",
    "    print(f\"    Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Demonstrate batch processing\n",
    "    print(f\"\\n  Sample batch:\")\n",
    "    for batch_features, batch_targets in train_loader_basic:\n",
    "        print(f\"    Features shape: {batch_features.shape}\")\n",
    "        print(f\"    Targets shape: {batch_targets.shape}\")\n",
    "        print(f\"    Feature range: [{batch_features.min():.3f}, {batch_features.max():.3f}]\")\n",
    "        print(f\"    Target classes: {torch.unique(batch_targets).tolist()}\")\n",
    "        break\n",
    "    \n",
    "    # Compare augmented vs non-augmented\n",
    "    print(f\"\\n  Augmentation comparison:\")\n",
    "    \n",
    "    # Get same sample from both datasets\n",
    "    basic_sample, _ = train_dataset_basic[0]\n",
    "    augmented_sample, _ = train_dataset_augmented[0]\n",
    "    \n",
    "    print(f\"    Original sample (first 5 features): {basic_sample[:5]}\")\n",
    "    print(f\"    Augmented sample (first 5 features): {augmented_sample[:5]}\")\n",
    "    print(f\"    Difference: {(augmented_sample[:5] - basic_sample[:5]).abs()}\")\n",
    "    \n",
    "    # Store PyTorch data\n",
    "    pytorch_data = {\n",
    "        'train_loader_basic': train_loader_basic,\n",
    "        'train_loader_augmented': train_loader_augmented,\n",
    "        'test_loader': test_loader,\n",
    "        'num_features': processed_data['X_train_selected'].shape[1],\n",
    "        'num_classes': len(np.unique(processed_data['y_train']))\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✅ PyTorch pipeline ready!\")\n",
    "    print(f\"  Input features: {pytorch_data['num_features']}\")\n",
    "    print(f\"  Output classes: {pytorch_data['num_classes']}\")\n",
    "\n",
    "else:\n",
    "    print(\"PyTorch not available - skipping PyTorch pipeline\")\n",
    "    pytorch_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorFlow Data Pipeline\n",
    "\n",
    "Creating TensorFlow-specific data pipelines and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TENSORFLOW DATA PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create tf.data.Dataset\n",
    "    print(\"\\n🟠 Creating TensorFlow Datasets:\")\n",
    "    \n",
    "    # Basic dataset\n",
    "    train_dataset_tf = tf.data.Dataset.from_tensor_slices((\n",
    "        processed_data['X_train_selected'].astype(np.float32),\n",
    "        processed_data['y_train'].astype(np.int32)\n",
    "    ))\n",
    "    \n",
    "    test_dataset_tf = tf.data.Dataset.from_tensor_slices((\n",
    "        processed_data['X_test_selected'].astype(np.float32),\n",
    "        processed_data['y_test'].astype(np.int32)\n",
    "    ))\n",
    "    \n",
    "    print(f\"  Training dataset created: {len(processed_data['y_train'])} samples\")\n",
    "    print(f\"  Test dataset created: {len(processed_data['y_test'])} samples\")\n",
    "    \n",
    "    # Add preprocessing functions\n",
    "    def add_noise(features, labels, noise_factor=0.05):\n",
    "        \"\"\"Add Gaussian noise to features\"\"\"\n",
    "        noise = tf.random.normal(tf.shape(features), stddev=noise_factor)\n",
    "        return features + noise, labels\n",
    "    \n",
    "    def feature_dropout(features, labels, dropout_rate=0.1):\n",
    "        \"\"\"Randomly set some features to zero\"\"\"\n",
    "        mask = tf.random.uniform(tf.shape(features)) > dropout_rate\n",
    "        return features * tf.cast(mask, tf.float32), labels\n",
    "    \n",
    "    # Create different pipeline configurations\n",
    "    batch_size = 64\n",
    "    \n",
    "    # Basic pipeline\n",
    "    train_pipeline_basic = (\n",
    "        train_dataset_tf\n",
    "        .shuffle(buffer_size=1000)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    # Augmented pipeline\n",
    "    train_pipeline_augmented = (\n",
    "        train_dataset_tf\n",
    "        .map(lambda x, y: add_noise(x, y, 0.05), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .shuffle(buffer_size=1000)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    # Test pipeline\n",
    "    test_pipeline = (\n",
    "        test_dataset_tf\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  Pipelines created:\")\n",
    "    print(f\"    Basic training pipeline: {len(list(train_pipeline_basic))} batches\")\n",
    "    print(f\"    Augmented training pipeline: {len(list(train_pipeline_augmented))} batches\")\n",
    "    print(f\"    Test pipeline: {len(list(test_pipeline))} batches\")\n",
    "    \n",
    "    # Demonstrate batch processing\n",
    "    print(f\"\\n  Sample batch:\")\n",
    "    for batch_features, batch_targets in train_pipeline_basic.take(1):\n",
    "        print(f\"    Features shape: {batch_features.shape}\")\n",
    "        print(f\"    Targets shape: {batch_targets.shape}\")\n",
    "        print(f\"    Feature range: [{tf.reduce_min(batch_features):.3f}, {tf.reduce_max(batch_features):.3f}]\")\n",
    "        print(f\"    Target classes: {tf.unique(batch_targets)[0].numpy().tolist()}\")\n",
    "    \n",
    "    # Compare augmented vs non-augmented\n",
    "    print(f\"\\n  Augmentation comparison:\")\n",
    "    \n",
    "    # Get samples from both pipelines\n",
    "    basic_batch = next(iter(train_pipeline_basic))\n",
    "    augmented_batch = next(iter(train_pipeline_augmented))\n",
    "    \n",
    "    basic_features = basic_batch[0][0]  # First sample, features\n",
    "    augmented_features = augmented_batch[0][0]  # First sample, features\n",
    "    \n",
    "    print(f\"    Original sample (first 5 features): {basic_features[:5].numpy()}\")\n",
    "    print(f\"    Augmented sample (first 5 features): {augmented_features[:5].numpy()}\")\n",
    "    \n",
    "    # Performance optimization example\n",
    "    print(f\"\\n  Performance optimization:\")\n",
    "    \n",
    "    # Optimized pipeline with caching\n",
    "    train_pipeline_optimized = (\n",
    "        train_dataset_tf\n",
    "        .cache()  # Cache dataset in memory\n",
    "        .map(lambda x, y: add_noise(x, y, 0.05), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .shuffle(buffer_size=1000)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    print(f\"    Optimized pipeline with caching created\")\n",
    "    print(f\"    Features: prefetching, parallel mapping, caching\")\n",
    "    \n",
    "    # Store TensorFlow data\n",
    "    tensorflow_data = {\n",
    "        'train_pipeline_basic': train_pipeline_basic,\n",
    "        'train_pipeline_augmented': train_pipeline_augmented,\n",
    "        'train_pipeline_optimized': train_pipeline_optimized,\n",
    "        'test_pipeline': test_pipeline,\n",
    "        'num_features': processed_data['X_train_selected'].shape[1],\n",
    "        'num_classes': len(np.unique(processed_data['y_train']))\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✅ TensorFlow pipeline ready!\")\n",
    "    print(f\"  Input features: {tensorflow_data['num_features']}\")\n",
    "    print(f\"  Output classes: {tensorflow_data['num_classes']}\")\n",
    "\n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping TensorFlow pipeline\")\n",
    "    tensorflow_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Best Practices\n",
    "\n",
    "Comparing approaches and summarizing best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualization of feature importance\n",
    "if 'selected_features' in processed_data:\n",
    "    print(\"\\n📊 Feature Importance Visualization:\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot feature scores\n",
    "    feature_scores = processed_data['selector'].scores_[processed_data['selector'].get_support()]\n",
    "    selected_features = processed_data['selected_features']\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(range(len(selected_features)), feature_scores, color='skyblue', alpha=0.7)\n",
    "    plt.yticks(range(len(selected_features)), selected_features)\n",
    "    plt.xlabel('ANOVA F-Score')\n",
    "    plt.title('Selected Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Plot correlation matrix of selected features\n",
    "    plt.subplot(1, 2, 2)\n",
    "    selected_data = df_for_scaling[selected_features]\n",
    "    correlation_matrix = selected_data.corr()\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Side-by-side framework comparison\n",
    "pytorch_pipeline_code = \"\"\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, targets, transform=None):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "        \n",
    "        return feature, target\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TabularDataset(X_scaled, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Data augmentation transform\n",
    "class AddNoise:\n",
    "    def __init__(self, noise_factor=0.1):\n",
    "        self.noise_factor = noise_factor\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn_like(tensor) * self.noise_factor\n",
    "        return tensor + noise\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_pipeline_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    X_scaled.astype(np.float32),\n",
    "    y_train.astype(np.int32)\n",
    "))\n",
    "\n",
    "# Data augmentation function\n",
    "def add_noise(features, labels, noise_factor=0.1):\n",
    "    noise = tf.random.normal(tf.shape(features), stddev=noise_factor)\n",
    "    return features + noise, labels\n",
    "\n",
    "# Create optimized pipeline\n",
    "pipeline = (\n",
    "    dataset\n",
    "    .cache()  # Cache in memory\n",
    "    .map(lambda x, y: add_noise(x, y), \n",
    "         num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .shuffle(buffer_size=1000)\n",
    "    .batch(64)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + create_side_by_side_comparison(\n",
    "    pytorch_pipeline_code, tensorflow_pipeline_code, \"Data Pipeline Implementation\"\n",
    "))\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\\n💡 Feature Engineering Best Practices:\")\n",
    "\n",
    "best_practices = {\n",
    "    \"Data Preprocessing\": [\n",
    "        \"Handle missing values before any other processing\",\n",
    "        \"Apply feature scaling (StandardScaler, MinMaxScaler)\",\n",
    "        \"Encode categorical variables appropriately\",\n",
    "        \"Split data before preprocessing to avoid data leakage\"\n",
    "    ],\n",
    "    \"Feature Selection\": [\n",
    "        \"Use statistical tests (ANOVA, chi-square) for initial selection\",\n",
    "        \"Consider correlation between features\",\n",
    "        \"Apply domain knowledge for feature engineering\",\n",
    "        \"Use cross-validation for robust feature selection\"\n",
    "    ],\n",
    "    \"Framework-Specific\": [\n",
    "        \"PyTorch: Use custom Dataset classes for complex preprocessing\",\n",
    "        \"TensorFlow: Leverage tf.data for optimized pipelines\",\n",
    "        \"Both: Implement data augmentation for better generalization\",\n",
    "        \"Both: Use appropriate batch sizes for your hardware\"\n",
    "    ],\n",
    "    \"Performance\": [\n",
    "        \"Cache preprocessed data when possible\",\n",
    "        \"Use parallel processing for data loading\",\n",
    "        \"Prefetch batches to overlap computation and I/O\",\n",
    "        \"Monitor memory usage with large datasets\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  • {practice}\")\n",
    "\n",
    "print(\"\\n🎯 Key Takeaways:\")\n",
    "print(\"  • Proper feature engineering is crucial for neural network performance\")\n",
    "print(\"  • Both frameworks offer powerful data pipeline capabilities\")\n",
    "print(\"  • PyTorch provides more flexibility with custom Dataset classes\")\n",
    "print(\"  • TensorFlow's tf.data API offers excellent optimization features\")\n",
    "print(\"  • Data augmentation can improve model generalization\")\n",
    "print(\"  • Always validate preprocessing steps with domain experts\")\n",
    "\n",
    "print(f\"\\n✅ Feature engineering tutorial completed!\")\n",
    "if pytorch_data and tensorflow_data:\n",
    "    print(f\"  Both PyTorch and TensorFlow pipelines are ready for model training\")\n",
    "elif pytorch_data:\n",
    "    print(f\"  PyTorch pipeline is ready for model training\")\n",
    "elif tensorflow_data:\n",
    "    print(f\"  TensorFlow pipeline is ready for model training\")\n",
    "else:\n",
    "    print(f\"  Preprocessing completed - frameworks not available for pipeline creation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}