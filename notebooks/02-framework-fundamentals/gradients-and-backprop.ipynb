{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and Backpropagation: Autograd vs GradientTape\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master automatic differentiation in both PyTorch and TensorFlow\n",
    "- Understand the differences between autograd and GradientTape\n",
    "- Learn gradient computation patterns and best practices\n",
    "- Explore advanced gradient techniques and debugging\n",
    "\n",
    "**Prerequisites:** Computational graphs, tensor operations\n",
    "\n",
    "**Estimated Time:** 45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "Automatic differentiation is the backbone of modern deep learning. Understanding how PyTorch's autograd and TensorFlow's GradientTape work is essential for:\n",
    "- **Training neural networks**: Computing gradients for optimization\n",
    "- **Custom operations**: Implementing new layers and functions\n",
    "- **Debugging**: Understanding gradient flow and vanishing/exploding gradients\n",
    "- **Advanced techniques**: Gradient clipping, accumulation, and higher-order derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Add src to path for our utilities\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Gradient Computation\n",
    "\n",
    "Let's start with simple examples to understand how each framework computes gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASIC GRADIENT COMPUTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Automatic Differentiation:\n",
    "‚Ä¢ Computes derivatives automatically using chain rule\n",
    "‚Ä¢ Essential for training neural networks\n",
    "‚Ä¢ Two main approaches: forward-mode and reverse-mode (backpropagation)\n",
    "‚Ä¢ Both frameworks use reverse-mode AD\n",
    "\"\"\")\n",
    "\n",
    "# Simple function: f(x) = x^2 + 2x + 1\n",
    "# Derivative: f'(x) = 2x + 2\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Autograd Example:\")\n",
    "\n",
    "    def pytorch_gradient_example():\n",
    "        # Create tensor with gradient tracking\n",
    "        x = torch.tensor(3.0, requires_grad=True)\n",
    "        print(f\"Input x: {x.item()}\")\n",
    "        print(f\"Requires grad: {x.requires_grad}\")\n",
    "\n",
    "        # Forward pass\n",
    "        y = x**2 + 2*x + 1\n",
    "        print(f\"y = x¬≤ + 2x + 1 = {y.item()}\")\n",
    "        print(f\"Gradient function: {y.grad_fn}\")\n",
    "\n",
    "        # Backward pass\n",
    "        y.backward()\n",
    "        print(f\"Computed gradient dy/dx: {x.grad.item()}\")\n",
    "        print(f\"Expected gradient (2x + 2): {2*x.item() + 2}\")\n",
    "        print(f\"Gradients match: {abs(x.grad.item() - (2*x.item() + 2)) < 1e-6}\")\n",
    "\n",
    "        return x.grad.item()\n",
    "\n",
    "    pytorch_grad = pytorch_gradient_example()\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow GradientTape Example:\")\n",
    "\n",
    "    def tensorflow_gradient_example():\n",
    "        # Create variable\n",
    "        x = tf.Variable(3.0)\n",
    "        print(f\"Input x: {x.numpy()}\")\n",
    "        print(f\"Trainable: {x.trainable}\")\n",
    "\n",
    "        # Forward pass with gradient tape\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = x**2 + 2*x + 1\n",
    "            print(f\"y = x¬≤ + 2x + 1 = {y.numpy()}\")\n",
    "\n",
    "        # Compute gradient\n",
    "        dy_dx = tape.gradient(y, x)\n",
    "        print(f\"Computed gradient dy/dx: {dy_dx.numpy()}\")\n",
    "        print(f\"Expected gradient (2x + 2): {2*x.numpy() + 2}\")\n",
    "        print(f\"Gradients match: {abs(dy_dx.numpy() - (2*x.numpy() + 2)) < 1e-6}\")\n",
    "\n",
    "        return dy_dx.numpy()\n",
    "\n",
    "    tensorflow_grad = tensorflow_gradient_example()\n",
    "\n",
    "# Compare approaches\n",
    "pytorch_code = \"\"\"\n",
    "import torch\n",
    "\n",
    "# PyTorch: requires_grad=True\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "# Automatic gradient computation\n",
    "y.backward()\n",
    "print(f\"Gradient: {x.grad}\")\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow: GradientTape context\n",
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2 + 2*x + 1\n",
    "\n",
    "# Explicit gradient computation\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(f\"Gradient: {dy_dx}\")\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_code, tensorflow_code, \"Basic Gradient Computation\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Variables and Partial Derivatives\n",
    "\n",
    "Computing gradients with respect to multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MULTIPLE VARIABLES AND PARTIAL DERIVATIVES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Function: f(x, y) = x¬≤y + xy¬≤ + x + y\n",
    "# ‚àÇf/‚àÇx = 2xy + y¬≤ + 1\n",
    "# ‚àÇf/‚àÇy = x¬≤ + 2xy + 1\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Multiple Variables:\")\n",
    "\n",
    "    def pytorch_multivariable_example():\n",
    "        x = torch.tensor(2.0, requires_grad=True)\n",
    "        y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "        print(f\"x = {x.item()}, y = {y.item()}\")\n",
    "\n",
    "        # Forward pass\n",
    "        z = x**2 * y + x * y**2 + x + y\n",
    "        print(f\"z = x¬≤y + xy¬≤ + x + y = {z.item()}\")\n",
    "\n",
    "        # Backward pass\n",
    "        z.backward()\n",
    "\n",
    "        print(f\"‚àÇz/‚àÇx = {x.grad.item()}\")\n",
    "        print(f\"‚àÇz/‚àÇy = {y.grad.item()}\")\n",
    "\n",
    "        # Verify gradients\n",
    "        expected_dx = 2*x.item()*y.item() + y.item()**2 + 1\n",
    "        expected_dy = x.item()**2 + 2*x.item()*y.item() + 1\n",
    "\n",
    "        print(f\"Expected ‚àÇz/‚àÇx: {expected_dx}\")\n",
    "        print(f\"Expected ‚àÇz/‚àÇy: {expected_dy}\")\n",
    "\n",
    "        print(f\"‚àÇz/‚àÇx correct: {abs(x.grad.item() - expected_dx) < 1e-6}\")\n",
    "        print(f\"‚àÇz/‚àÇy correct: {abs(y.grad.item() - expected_dy) < 1e-6}\")\n",
    "\n",
    "        return x.grad.item(), y.grad.item()\n",
    "\n",
    "    pt_grad_x, pt_grad_y = pytorch_multivariable_example()\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Multiple Variables:\")\n",
    "\n",
    "    def tensorflow_multivariable_example():\n",
    "        x = tf.Variable(2.0)\n",
    "        y = tf.Variable(3.0)\n",
    "\n",
    "        print(f\"x = {x.numpy()}, y = {y.numpy()}\")\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = x**2 * y + x * y**2 + x + y\n",
    "            print(f\"z = x¬≤y + xy¬≤ + x + y = {z.numpy()}\")\n",
    "\n",
    "        # Compute gradients for both variables\n",
    "        gradients = tape.gradient(z, [x, y])\n",
    "        dz_dx, dz_dy = gradients\n",
    "\n",
    "        print(f\"‚àÇz/‚àÇx = {dz_dx.numpy()}\")\n",
    "        print(f\"‚àÇz/‚àÇy = {dz_dy.numpy()}\")\n",
    "\n",
    "        # Verify gradients\n",
    "        expected_dx = 2*x.numpy()*y.numpy() + y.numpy()**2 + 1\n",
    "        expected_dy = x.numpy()**2 + 2*x.numpy()*y.numpy() + 1\n",
    "\n",
    "        print(f\"Expected ‚àÇz/‚àÇx: {expected_dx}\")\n",
    "        print(f\"Expected ‚àÇz/‚àÇy: {expected_dy}\")\n",
    "\n",
    "        print(f\"‚àÇz/‚àÇx correct: {abs(dz_dx.numpy() - expected_dx) < 1e-6}\")\n",
    "        print(f\"‚àÇz/‚àÇy correct: {abs(dz_dy.numpy() - expected_dy) < 1e-6}\")\n",
    "\n",
    "        return dz_dx.numpy(), dz_dy.numpy()\n",
    "\n",
    "    tf_grad_x, tf_grad_y = tensorflow_multivariable_example()\n",
    "\n",
    "# Vector and matrix gradients\n",
    "print(\"\\nüìä Vector and Matrix Gradients:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nPyTorch Vector Gradients:\")\n",
    "\n",
    "    # Vector input\n",
    "    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "    # Scalar output (sum of squares)\n",
    "    y = torch.sum(x**2)\n",
    "    y.backward()\n",
    "\n",
    "    print(f\"Input vector: {x.data}\")\n",
    "    print(f\"Output scalar: {y.item()}\")\n",
    "    print(f\"Gradient vector: {x.grad}\")\n",
    "    print(f\"Expected (2x): {2 * x.data}\")\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nTensorFlow Vector Gradients:\")\n",
    "\n",
    "    x = tf.Variable([1.0, 2.0, 3.0])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = tf.reduce_sum(x**2)\n",
    "\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "\n",
    "    print(f\"Input vector: {x.numpy()}\")\n",
    "    print(f\"Output scalar: {y.numpy()}\")\n",
    "    print(f\"Gradient vector: {dy_dx.numpy()}\")\n",
    "    print(f\"Expected (2x): {2 * x.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network Gradients\n",
    "\n",
    "Computing gradients for neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEURAL NETWORK GRADIENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple neural network: y = ReLU(Wx + b)\n",
    "# Loss: MSE between prediction and target\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Neural Network Gradients:\")\n",
    "\n",
    "    def pytorch_nn_gradients():\n",
    "        # Create simple network\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Input and target\n",
    "        x = torch.randn(5, 3)  # 5 samples, 3 features\n",
    "        target = torch.randn(5, 2)  # 5 samples, 2 outputs\n",
    "\n",
    "        # Network parameters\n",
    "        W = torch.randn(3, 2, requires_grad=True)\n",
    "        b = torch.randn(2, requires_grad=True)\n",
    "\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        print(f\"Weight shape: {W.shape}\")\n",
    "        print(f\"Bias shape: {b.shape}\")\n",
    "        print(f\"Target shape: {target.shape}\")\n",
    "\n",
    "        # Forward pass\n",
    "        linear_output = x @ W + b\n",
    "        prediction = torch.relu(linear_output)\n",
    "        loss = torch.mean((prediction - target)**2)\n",
    "\n",
    "        print(f\"\\nLoss: {loss.item():.4f}\")\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        print(\"\\nGradient shapes:\")\n",
    "        print(f\"dL/dW shape: {W.grad.shape}\")\n",
    "        print(f\"dL/db shape: {b.grad.shape}\")\n",
    "\n",
    "        print(\"\\nGradient magnitudes:\")\n",
    "        print(f\"||dL/dW||: {torch.norm(W.grad).item():.4f}\")\n",
    "        print(f\"||dL/db||: {torch.norm(b.grad).item():.4f}\")\n",
    "\n",
    "        return loss.item(), W.grad.clone(), b.grad.clone()\n",
    "\n",
    "    pt_loss, pt_W_grad, pt_b_grad = pytorch_nn_gradients()\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Neural Network Gradients:\")\n",
    "\n",
    "    def tensorflow_nn_gradients():\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        # Input and target\n",
    "        x = tf.random.normal((5, 3))  # 5 samples, 3 features\n",
    "        target = tf.random.normal((5, 2))  # 5 samples, 2 outputs\n",
    "\n",
    "        # Network parameters\n",
    "        W = tf.Variable(tf.random.normal((3, 2)))\n",
    "        b = tf.Variable(tf.random.normal((2,)))\n",
    "\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        print(f\"Weight shape: {W.shape}\")\n",
    "        print(f\"Bias shape: {b.shape}\")\n",
    "        print(f\"Target shape: {target.shape}\")\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            linear_output = x @ W + b\n",
    "            prediction = tf.nn.relu(linear_output)\n",
    "            loss = tf.reduce_mean((prediction - target)**2)\n",
    "\n",
    "        print(f\"\\nLoss: {loss.numpy():.4f}\")\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, [W, b])\n",
    "        dL_dW, dL_db = gradients\n",
    "\n",
    "        print(\"\\nGradient shapes:\")\n",
    "        print(f\"dL/dW shape: {dL_dW.shape}\")\n",
    "        print(f\"dL/db shape: {dL_db.shape}\")\n",
    "\n",
    "        print(\"\\nGradient magnitudes:\")\n",
    "        print(f\"||dL/dW||: {tf.norm(dL_dW).numpy():.4f}\")\n",
    "        print(f\"||dL/db||: {tf.norm(dL_db).numpy():.4f}\")\n",
    "\n",
    "        return loss.numpy(), dL_dW, dL_db\n",
    "\n",
    "    tf_loss, tf_W_grad, tf_b_grad = tensorflow_nn_gradients()\n",
    "\n",
    "# Using built-in layers\n",
    "print(\"\\nüèóÔ∏è Using Built-in Layers:\")\n",
    "\n",
    "pytorch_layer_code = \"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "# PyTorch built-in layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(5, 3)\n",
    "target = torch.randn(5, 2)\n",
    "prediction = model(x)\n",
    "loss = nn.MSELoss()(prediction, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.grad.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_layer_code = \"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow built-in layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "# Forward pass with GradientTape\n",
    "x = tf.random.normal((5, 3))\n",
    "target = tf.random.normal((5, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = model(x)\n",
    "    loss = tf.keras.losses.MSE(target, prediction)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Compute gradients\n",
    "gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "# Access gradients\n",
    "for i, grad in enumerate(gradients):\n",
    "    print(f\"Layer {i}: {grad.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_layer_code, tensorflow_layer_code, \"Built-in Layers Gradients\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Gradient Techniques\n",
    "\n",
    "Gradient clipping, accumulation, and higher-order derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ADVANCED GRADIENT TECHNIQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Gradient Clipping\n",
    "print(\"\\n1. Gradient Clipping:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Gradient Clipping:\")\n",
    "\n",
    "    # Create model with large gradients\n",
    "    model = nn.Linear(5, 1)\n",
    "    x = torch.randn(10, 5)\n",
    "    target = torch.randn(10, 1) * 100  # Large target values\n",
    "\n",
    "    # Forward pass\n",
    "    prediction = model(x)\n",
    "    loss = nn.MSELoss()(prediction, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Check gradient norms before clipping\n",
    "    total_norm_before = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            total_norm_before += param.grad.data.norm(2).item() ** 2\n",
    "    total_norm_before = total_norm_before ** 0.5\n",
    "\n",
    "    print(f\"Gradient norm before clipping: {total_norm_before:.4f}\")\n",
    "\n",
    "    # Clip gradients\n",
    "    max_norm = 1.0\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "    # Check gradient norms after clipping\n",
    "    total_norm_after = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            total_norm_after += param.grad.data.norm(2).item() ** 2\n",
    "    total_norm_after = total_norm_after ** 0.5\n",
    "\n",
    "    print(f\"Gradient norm after clipping: {total_norm_after:.4f}\")\n",
    "    print(f\"Clipping applied: {total_norm_before > max_norm}\")\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Gradient Clipping:\")\n",
    "\n",
    "    # Create model\n",
    "    model = tf.keras.layers.Dense(1)\n",
    "    x = tf.random.normal((10, 5))\n",
    "    target = tf.random.normal((10, 1)) * 100\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(prediction - target))\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Check gradient norms before clipping\n",
    "    total_norm_before = tf.sqrt(sum([tf.reduce_sum(tf.square(g)) for g in gradients]))\n",
    "    print(f\"Gradient norm before clipping: {total_norm_before.numpy():.4f}\")\n",
    "\n",
    "    # Clip gradients\n",
    "    max_norm = 1.0\n",
    "    clipped_gradients, global_norm = tf.clip_by_global_norm(gradients, max_norm)\n",
    "\n",
    "    print(f\"Gradient norm after clipping: {global_norm.numpy():.4f}\")\n",
    "    print(f\"Clipping applied: {global_norm.numpy() > max_norm}\")\n",
    "\n",
    "# 2. Gradient Accumulation\n",
    "print(\"\\n2. Gradient Accumulation:\")\n",
    "\n",
    "pytorch_accumulation_code = \"\"\"\n",
    "# PyTorch gradient accumulation\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "accumulation_steps = 4\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for i in range(accumulation_steps):\n",
    "    # Mini-batch\n",
    "    x = torch.randn(8, 10)\n",
    "    target = torch.randn(8, 1)\n",
    "\n",
    "    prediction = model(x)\n",
    "    loss = nn.MSELoss()(prediction, target)\n",
    "\n",
    "    # Scale loss by accumulation steps\n",
    "    loss = loss / accumulation_steps\n",
    "    loss.backward()  # Accumulate gradients\n",
    "\n",
    "# Update parameters with accumulated gradients\n",
    "optimizer.step()\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_accumulation_code = \"\"\"\n",
    "# TensorFlow gradient accumulation\n",
    "model = tf.keras.layers.Dense(1)\n",
    "optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "\n",
    "accumulation_steps = 4\n",
    "accumulated_gradients = []\n",
    "\n",
    "for i in range(accumulation_steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x = tf.random.normal((8, 10))\n",
    "        target = tf.random.normal((8, 1))\n",
    "\n",
    "        prediction = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(prediction - target))\n",
    "        loss = loss / accumulation_steps\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Accumulate gradients\n",
    "    if i == 0:\n",
    "        accumulated_gradients = gradients\n",
    "    else:\n",
    "        accumulated_gradients = [\n",
    "            acc_grad + grad for acc_grad, grad\n",
    "            in zip(accumulated_gradients, gradients)\n",
    "        ]\n",
    "\n",
    "# Apply accumulated gradients\n",
    "optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_accumulation_code, tensorflow_accumulation_code, \"Gradient Accumulation\"\n",
    "))\n",
    "\n",
    "# 3. Higher-order derivatives\n",
    "print(\"\\n3. Higher-order Derivatives:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Second Derivatives:\")\n",
    "\n",
    "    # Function: f(x) = x^3\n",
    "    # First derivative: f'(x) = 3x^2\n",
    "    # Second derivative: f''(x) = 6x\n",
    "\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = x**3\n",
    "\n",
    "    # First derivative\n",
    "    dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "    print(f\"First derivative dy/dx: {dy_dx.item()}\")\n",
    "    print(f\"Expected (3x¬≤): {3 * x.item()**2}\")\n",
    "\n",
    "    # Second derivative\n",
    "    d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
    "    print(f\"Second derivative d¬≤y/dx¬≤: {d2y_dx2.item()}\")\n",
    "    print(f\"Expected (6x): {6 * x.item()}\")\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Second Derivatives:\")\n",
    "\n",
    "    x = tf.Variable(2.0)\n",
    "\n",
    "    with tf.GradientTape() as tape2:\n",
    "        with tf.GradientTape() as tape1:\n",
    "            y = x**3\n",
    "\n",
    "        # First derivative\n",
    "        dy_dx = tape1.gradient(y, x)\n",
    "\n",
    "    # Second derivative\n",
    "    d2y_dx2 = tape2.gradient(dy_dx, x)\n",
    "\n",
    "    print(f\"First derivative dy/dx: {dy_dx.numpy()}\")\n",
    "    print(f\"Expected (3x¬≤): {3 * x.numpy()**2}\")\n",
    "    print(f\"Second derivative d¬≤y/dx¬≤: {d2y_dx2.numpy()}\")\n",
    "    print(f\"Expected (6x): {6 * x.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Debugging and Validation\n",
    "\n",
    "Techniques for debugging gradient computation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GRADIENT DEBUGGING AND VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Common Gradient Issues:\n",
    "‚Ä¢ Vanishing gradients: Gradients become very small\n",
    "‚Ä¢ Exploding gradients: Gradients become very large\n",
    "‚Ä¢ Dead neurons: Gradients are zero (e.g., ReLU saturation)\n",
    "‚Ä¢ Incorrect gradient computation: Implementation bugs\n",
    "\"\"\")\n",
    "\n",
    "# Gradient checking (numerical vs analytical)\n",
    "print(\"\\n1. Gradient Checking:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Gradient Checking:\")\n",
    "\n",
    "    def pytorch_gradient_check():\n",
    "        # Simple function for testing\n",
    "        def test_function(x):\n",
    "            return torch.sum(x**2 + 2*x)\n",
    "\n",
    "        x = torch.randn(5, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "        # Use PyTorch's gradient checker\n",
    "        from torch.autograd import gradcheck\n",
    "\n",
    "        # Check if gradients are correct\n",
    "        test_passed = gradcheck(test_function, x, eps=1e-6, atol=1e-4)\n",
    "\n",
    "        print(f\"Gradient check passed: {test_passed}\")\n",
    "\n",
    "        # Manual numerical gradient check\n",
    "        eps = 1e-5\n",
    "        x_np = x.detach().numpy()\n",
    "\n",
    "        # Analytical gradient\n",
    "        y = test_function(x)\n",
    "        y.backward()\n",
    "        analytical_grad = x.grad.numpy()\n",
    "\n",
    "        # Numerical gradient\n",
    "        numerical_grad = np.zeros_like(x_np)\n",
    "        for i in range(len(x_np)):\n",
    "            x_plus = x_np.copy()\n",
    "            x_minus = x_np.copy()\n",
    "            x_plus[i] += eps\n",
    "            x_minus[i] -= eps\n",
    "\n",
    "            y_plus = test_function(torch.tensor(x_plus)).item()\n",
    "            y_minus = test_function(torch.tensor(x_minus)).item()\n",
    "\n",
    "            numerical_grad[i] = (y_plus - y_minus) / (2 * eps)\n",
    "\n",
    "        # Compare gradients\n",
    "        diff = np.abs(analytical_grad - numerical_grad)\n",
    "        max_diff = np.max(diff)\n",
    "\n",
    "        print(f\"Max difference: {max_diff:.2e}\")\n",
    "        print(f\"Gradients match: {max_diff < 1e-4}\")\n",
    "\n",
    "        return max_diff\n",
    "\n",
    "    pytorch_gradient_check()\n",
    "\n",
    "# Gradient flow visualization\n",
    "print(\"\\n2. Gradient Flow Analysis:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Gradient Flow:\")\n",
    "\n",
    "    def analyze_gradient_flow():\n",
    "        # Create a deep network\n",
    "        layers = []\n",
    "        input_size = 10\n",
    "\n",
    "        for i in range(5):  # 5 layers\n",
    "            layers.append(nn.Linear(input_size, input_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        model = nn.Sequential(*layers)\n",
    "\n",
    "        # Forward pass\n",
    "        x = torch.randn(32, 10)\n",
    "        target = torch.randn(32, 10)\n",
    "\n",
    "        prediction = model(x)\n",
    "        loss = nn.MSELoss()(prediction, target)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Analyze gradient magnitudes\n",
    "        print(\"Gradient magnitudes by layer:\")\n",
    "        for i, (name, param) in enumerate(model.named_parameters()):\n",
    "            if param.grad is not None and 'weight' in name:\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                print(f\"Layer {i//2}: {grad_norm:.6f}\")\n",
    "\n",
    "                # Check for vanishing/exploding gradients\n",
    "                if grad_norm < 1e-6:\n",
    "                    print(\"  ‚ö†Ô∏è  Potential vanishing gradient\")\n",
    "                elif grad_norm > 10:\n",
    "                    print(\"  ‚ö†Ô∏è  Potential exploding gradient\")\n",
    "\n",
    "    analyze_gradient_flow()\n",
    "\n",
    "# Common debugging techniques\n",
    "print(\"\\nüìã Gradient Debugging Checklist:\")\n",
    "debugging_checklist = [\n",
    "    \"‚úì Check if requires_grad=True (PyTorch) or GradientTape is used (TensorFlow)\",\n",
    "    \"‚úì Verify gradient shapes match parameter shapes\",\n",
    "    \"‚úì Look for NaN or Inf values in gradients\",\n",
    "    \"‚úì Check gradient magnitudes (too small = vanishing, too large = exploding)\",\n",
    "    \"‚úì Use gradient clipping for exploding gradients\",\n",
    "    \"‚úì Use proper initialization for vanishing gradients\",\n",
    "    \"‚úì Verify custom operations have correct backward passes\",\n",
    "    \"‚úì Use gradient checking for custom functions\",\n",
    "    \"‚úì Monitor gradient flow through the network\",\n",
    "    \"‚úì Check for dead neurons (zero gradients)\"\n",
    "]\n",
    "\n",
    "for item in debugging_checklist:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Debugging Tools Comparison:\")\n",
    "debugging_tools = {\n",
    "    \"Gradient checking\": {\n",
    "        \"PyTorch\": \"torch.autograd.gradcheck()\",\n",
    "        \"TensorFlow\": \"tf.test.compute_gradient()\"\n",
    "    },\n",
    "    \"Gradient clipping\": {\n",
    "        \"PyTorch\": \"torch.nn.utils.clip_grad_norm_()\",\n",
    "        \"TensorFlow\": \"tf.clip_by_global_norm()\"\n",
    "    },\n",
    "    \"NaN detection\": {\n",
    "        \"PyTorch\": \"torch.isnan(), torch.isinf()\",\n",
    "        \"TensorFlow\": \"tf.debugging.check_numerics()\"\n",
    "    },\n",
    "    \"Gradient inspection\": {\n",
    "        \"PyTorch\": \"param.grad for param in model.parameters()\",\n",
    "        \"TensorFlow\": \"tape.gradient(loss, model.trainable_variables)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for tool, frameworks in debugging_tools.items():\n",
    "    print(f\"{tool:18} | PyTorch: {frameworks['PyTorch']:35} | TensorFlow: {frameworks['TensorFlow']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "**What we've learned:**\n",
    "\n",
    "1. **Automatic Differentiation**: Both frameworks use reverse-mode AD for efficient gradient computation\n",
    "2. **Framework Differences**: PyTorch's autograd vs TensorFlow's GradientTape\n",
    "3. **Neural Network Gradients**: Computing gradients for complex models\n",
    "4. **Advanced Techniques**: Gradient clipping, accumulation, and higher-order derivatives\n",
    "5. **Debugging**: Tools and techniques for gradient validation\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | PyTorch Autograd | TensorFlow GradientTape |\n",
    "|--------|------------------|-------------------------|\n",
    "| **Activation** | `requires_grad=True` | `with tf.GradientTape():` |\n",
    "| **Computation** | `loss.backward()` | `tape.gradient(loss, vars)` |\n",
    "| **Persistence** | Automatic | Must specify `persistent=True` |\n",
    "| **Multiple Outputs** | Separate backward calls | Single gradient call |\n",
    "| **Higher-order** | `create_graph=True` | Nested GradientTapes |\n",
    "| **Memory** | Automatic cleanup | Automatic cleanup |\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "**PyTorch:**\n",
    "- Use `requires_grad=True` only for parameters that need gradients\n",
    "- Call `optimizer.zero_grad()` before each backward pass\n",
    "- Use `torch.no_grad()` for inference to save memory\n",
    "- Leverage `torch.autograd.gradcheck()` for custom functions\n",
    "\n",
    "**TensorFlow:**\n",
    "- Use `tf.GradientTape()` context for gradient computation\n",
    "- Set `persistent=True` for multiple gradient computations\n",
    "- Use `tf.stop_gradient()` to prevent gradient flow\n",
    "- Leverage `tf.debugging.check_numerics()` for gradient validation\n",
    "\n",
    "**Common Patterns:**\n",
    "\n",
    "**Training Loop (PyTorch):**\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "loss = criterion(model(x), target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "**Training Loop (TensorFlow):**\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = criterion(model(x), target)\n",
    "gradients = tape.gradient(loss, model.trainable_variables)\n",
    "optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "```\n",
    "\n",
    "**Performance Considerations:**\n",
    "- Both frameworks are highly optimized for gradient computation\n",
    "- PyTorch's dynamic graphs have slight overhead but offer flexibility\n",
    "- TensorFlow's eager mode is similar to PyTorch, graph mode is faster\n",
    "- Gradient accumulation helps with memory-limited training\n",
    "\n",
    "**When to Use Advanced Techniques:**\n",
    "- **Gradient Clipping**: When training RNNs or very deep networks\n",
    "- **Gradient Accumulation**: When batch size is limited by memory\n",
    "- **Higher-order Derivatives**: For meta-learning or optimization research\n",
    "- **Gradient Checking**: When implementing custom operations\n",
    "\n",
    "**Next Steps:**\n",
    "- Learn about optimizers and how they use gradients\n",
    "- Explore advanced architectures and their gradient flow\n",
    "- Study regularization techniques that affect gradients\n",
    "- Practice debugging gradient issues in real models\n",
    "\n",
    "Understanding automatic differentiation is fundamental to deep learning success. Both PyTorch and TensorFlow provide powerful tools for gradient computation, each with their own strengths and use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
