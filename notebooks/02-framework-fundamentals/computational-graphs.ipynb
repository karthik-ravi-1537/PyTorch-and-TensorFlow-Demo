{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graphs: Dynamic vs Static Execution\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the fundamental difference between dynamic and static computational graphs\n",
    "- Learn how PyTorch's eager execution compares to TensorFlow's graph modes\n",
    "- Explore the trade-offs between flexibility and performance\n",
    "- Master debugging techniques for each approach\n",
    "\n",
    "**Prerequisites:** Tensor operations, basic neural network concepts\n",
    "\n",
    "**Estimated Time:** 50 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Add src to path for our utilities\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from utils.comparison_tools import create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "    print(f\"   Eager execution: {tf.executing_eagerly()}\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graph Fundamentals\n",
    "\n",
    "Understanding the core concepts of computational graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPUTATIONAL GRAPH FUNDAMENTALS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "A computational graph represents mathematical operations as a network:\n",
    "‚Ä¢ Nodes = Operations (add, multiply, relu, etc.)\n",
    "‚Ä¢ Edges = Data flow (tensors)\n",
    "‚Ä¢ Direction = Forward pass (input ‚Üí output)\n",
    "‚Ä¢ Reverse = Backward pass (gradients)\n",
    "\n",
    "Example: z = relu((x + y) * w)\n",
    "\n",
    "    x   y\n",
    "     \\\\ /\n",
    "      +  ‚Üê addition node\n",
    "      |\n",
    "      *  ‚Üê multiplication node\n",
    "     / \\\n",
    "    w   |\n",
    "        relu ‚Üê activation node\n",
    "        |\n",
    "        z\n",
    "\n",
    "Two fundamental approaches:\n",
    "1. DYNAMIC: Build graph during execution (PyTorch)\n",
    "2. STATIC: Build graph first, then execute (TensorFlow @tf.function)\n",
    "\"\"\")\n",
    "\n",
    "# Simple demonstration of graph concepts\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Dynamic Graph Example:\")\n",
    "\n",
    "    # Create tensors with gradient tracking\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = torch.tensor(3.0, requires_grad=True)\n",
    "    w = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "    print(f\"Inputs: x={x.item()}, y={y.item()}, w={w.item()}\")\n",
    "\n",
    "    # Build graph step by step\n",
    "    temp = x + y  # Addition node created\n",
    "    print(f\"After x + y: {temp.item()}, grad_fn: {temp.grad_fn}\")\n",
    "\n",
    "    product = temp * w  # Multiplication node created\n",
    "    print(f\"After * w: {product.item()}, grad_fn: {product.grad_fn}\")\n",
    "\n",
    "    z = torch.relu(product)  # ReLU node created\n",
    "    print(f\"After ReLU: {z.item()}, grad_fn: {z.grad_fn}\")\n",
    "\n",
    "    # The graph exists and can be traversed\n",
    "    print(\"\\nGraph structure:\")\n",
    "    print(f\"  z.grad_fn: {z.grad_fn}\")\n",
    "    print(f\"  z.grad_fn.next_functions: {z.grad_fn.next_functions}\")\n",
    "\n",
    "    # Compute gradients\n",
    "    z.backward()\n",
    "    print(\"\\nGradients:\")\n",
    "    print(f\"  dx/dz: {x.grad.item()}\")\n",
    "    print(f\"  dy/dz: {y.grad.item()}\")\n",
    "    print(f\"  dw/dz: {w.grad.item()}\")\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Eager Execution Example:\")\n",
    "\n",
    "    # Create variables\n",
    "    x = tf.Variable(2.0)\n",
    "    y = tf.Variable(3.0)\n",
    "    w = tf.Variable(4.0)\n",
    "\n",
    "    print(f\"Inputs: x={x.numpy()}, y={y.numpy()}, w={w.numpy()}\")\n",
    "    print(f\"Eager execution enabled: {tf.executing_eagerly()}\")\n",
    "\n",
    "    # Use GradientTape to record operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        temp = x + y  # Executed immediately\n",
    "        print(f\"After x + y: {temp.numpy()}\")\n",
    "\n",
    "        product = temp * w  # Executed immediately\n",
    "        print(f\"After * w: {product.numpy()}\")\n",
    "\n",
    "        z = tf.nn.relu(product)  # Executed immediately\n",
    "        print(f\"After ReLU: {z.numpy()}\")\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(z, [x, y, w])\n",
    "    print(\"\\nGradients:\")\n",
    "    print(f\"  dx/dz: {gradients[0].numpy()}\")\n",
    "    print(f\"  dy/dz: {gradients[1].numpy()}\")\n",
    "    print(f\"  dw/dz: {gradients[2].numpy()}\")\n",
    "\n",
    "print(\"\\nüí° Key Differences:\")\n",
    "print(\"  ‚Ä¢ PyTorch: Graph built during forward pass, stored for backward pass\")\n",
    "print(\"  ‚Ä¢ TensorFlow Eager: Operations executed immediately, tape records for gradients\")\n",
    "print(\"  ‚Ä¢ Both support automatic differentiation\")\n",
    "print(\"  ‚Ä¢ Graph structure affects memory usage and performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Dynamic Graphs\n",
    "\n",
    "Exploring PyTorch's define-by-run approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PYTORCH DYNAMIC GRAPHS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\"\"\n",
    "    PyTorch Dynamic Graph Features:\n",
    "    ‚Ä¢ Graph built during forward pass (define-by-run)\n",
    "    ‚Ä¢ Different graph structure each iteration\n",
    "    ‚Ä¢ Easy debugging with standard Python tools\n",
    "    ‚Ä¢ Flexible control flow (if/else, loops, recursion)\n",
    "    ‚Ä¢ Slight performance overhead due to graph construction\n",
    "    \"\"\")\n",
    "\n",
    "    # Example 1: Conditional computation\n",
    "    print(\"\\n1. Conditional Computation:\")\n",
    "\n",
    "    def conditional_model(x, use_nonlinearity=True):\n",
    "        \"\"\"Model with conditional computation\"\"\"\n",
    "        print(f\"  Input shape: {x.shape}, mean: {x.mean().item():.4f}\")\n",
    "\n",
    "        # Linear transformation\n",
    "        y = x * 2.0 + 1.0\n",
    "        print(f\"  After linear: mean = {y.mean().item():.4f}\")\n",
    "\n",
    "        # Conditional nonlinearity - graph structure changes!\n",
    "        if use_nonlinearity:\n",
    "            if y.mean() > 0:\n",
    "                z = torch.relu(y)\n",
    "                operation = \"ReLU applied\"\n",
    "            else:\n",
    "                z = torch.tanh(y)\n",
    "                operation = \"Tanh applied\"\n",
    "        else:\n",
    "            z = y\n",
    "            operation = \"No nonlinearity\"\n",
    "\n",
    "        result = z.sum()\n",
    "        print(f\"  Operation: {operation}\")\n",
    "        print(f\"  Final result: {result.item():.4f}\")\n",
    "        print(f\"  Grad function: {result.grad_fn}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Test with different conditions\n",
    "    x1 = torch.randn(5, requires_grad=True)\n",
    "    x2 = torch.randn(5, requires_grad=True) - 2.0  # Negative mean\n",
    "\n",
    "    print(\"\\n  Test 1 - Positive input with nonlinearity:\")\n",
    "    result1 = conditional_model(x1, use_nonlinearity=True)\n",
    "\n",
    "    print(\"\\n  Test 2 - Negative input with nonlinearity:\")\n",
    "    result2 = conditional_model(x2, use_nonlinearity=True)\n",
    "\n",
    "    print(\"\\n  Test 3 - No nonlinearity:\")\n",
    "    result3 = conditional_model(x1, use_nonlinearity=False)\n",
    "\n",
    "    # Example 2: Variable-length sequences\n",
    "    print(\"\\n\\n2. Variable-Length Sequence Processing:\")\n",
    "\n",
    "    def process_variable_sequences(sequences):\n",
    "        \"\"\"Process sequences of different lengths\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for i, seq_length in enumerate(sequences):\n",
    "            # Create sequence of variable length\n",
    "            x = torch.randn(seq_length, 3, requires_grad=True)\n",
    "\n",
    "            # Different processing based on length\n",
    "            if seq_length <= 3:\n",
    "                # Short sequences: simple mean\n",
    "                result = x.mean()\n",
    "                method = \"mean\"\n",
    "            else:\n",
    "                # Long sequences: weighted sum\n",
    "                weights = torch.softmax(torch.randn(seq_length), dim=0)\n",
    "                result = (x * weights.unsqueeze(1)).sum()\n",
    "                method = \"weighted sum\"\n",
    "\n",
    "            print(f\"  Sequence {i+1}: length={seq_length}, method={method}, result={result.item():.4f}\")\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Process sequences of different lengths\n",
    "    sequence_lengths = [2, 5, 3, 8, 1, 6]\n",
    "    results = process_variable_sequences(sequence_lengths)\n",
    "\n",
    "    # Example 3: Recursive computation\n",
    "    print(\"\\n\\n3. Recursive Computation:\")\n",
    "\n",
    "    def recursive_computation(x, depth=0, max_depth=3):\n",
    "        \"\"\"Recursive function that builds different graphs\"\"\"\n",
    "        print(f\"  {'  ' * depth}Depth {depth}: input mean = {x.mean().item():.4f}\")\n",
    "\n",
    "        # Base case\n",
    "        if depth >= max_depth or x.mean() < 0.1:\n",
    "            result = x.sum()\n",
    "            print(f\"  {'  ' * depth}Base case reached: {result.item():.4f}\")\n",
    "            return result\n",
    "\n",
    "        # Recursive case\n",
    "        y = torch.relu(x - 0.5)  # Reduce values\n",
    "        return recursive_computation(y, depth + 1, max_depth)\n",
    "\n",
    "    x_recursive = torch.randn(4, requires_grad=True) + 1.0  # Start with positive values\n",
    "    recursive_result = recursive_computation(x_recursive)\n",
    "    print(f\"  Final recursive result: {recursive_result.item():.4f}\")\n",
    "\n",
    "    # Compute gradients for recursive computation\n",
    "    recursive_result.backward()\n",
    "    print(f\"  Gradient computed: {x_recursive.grad is not None}\")\n",
    "    print(f\"  Gradient values: {x_recursive.grad}\")\n",
    "\n",
    "else:\n",
    "    print(\"PyTorch not available - skipping dynamic graph examples\")\n",
    "\n",
    "print(\"\\nüî• PyTorch Dynamic Graph Advantages:\")\n",
    "print(\"  ‚Ä¢ Natural Python control flow\")\n",
    "print(\"  ‚Ä¢ Easy debugging and introspection\")\n",
    "print(\"  ‚Ä¢ Flexible model architectures\")\n",
    "print(\"  ‚Ä¢ Great for research and prototyping\")\n",
    "print(\"  ‚Ä¢ Handles variable-length inputs naturally\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è PyTorch Dynamic Graph Considerations:\")\n",
    "print(\"  ‚Ä¢ Graph construction overhead each forward pass\")\n",
    "print(\"  ‚Ä¢ Memory usage for storing graph\")\n",
    "print(\"  ‚Ä¢ Harder to optimize for deployment\")\n",
    "print(\"  ‚Ä¢ Requires tracing for production optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow Graph Modes\n",
    "\n",
    "Understanding TensorFlow's eager execution and graph compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TENSORFLOW GRAPH MODES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\"\"\n",
    "    TensorFlow Execution Modes:\n",
    "    ‚Ä¢ EAGER (default): Operations execute immediately (like PyTorch)\n",
    "    ‚Ä¢ GRAPH (@tf.function): Compile to static graph for performance\n",
    "    ‚Ä¢ Can switch between modes as needed\n",
    "    ‚Ä¢ Graph mode enables optimizations and deployment\n",
    "    \"\"\")\n",
    "\n",
    "    # Example 1: Eager vs Graph execution\n",
    "    print(\"\\n1. Eager vs Graph Execution:\")\n",
    "\n",
    "    def eager_computation(x, y):\n",
    "        \"\"\"Computation in eager mode\"\"\"\n",
    "        print(\"  Eager mode - executing immediately\")\n",
    "        z1 = x + y\n",
    "        print(f\"  After addition: {z1.shape}\")\n",
    "\n",
    "        z2 = tf.nn.relu(z1)\n",
    "        print(f\"  After ReLU: {tf.reduce_mean(z2).numpy():.4f}\")\n",
    "\n",
    "        result = tf.reduce_sum(z2)\n",
    "        print(f\"  Final result: {result.numpy():.4f}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    @tf.function\n",
    "    def graph_computation(x, y):\n",
    "        \"\"\"Same computation compiled to graph\"\"\"\n",
    "        # Note: print statements won't work in graph mode\n",
    "        # Use tf.print for debugging in graph mode\n",
    "        tf.print(\"Graph mode - compiled execution\")\n",
    "\n",
    "        z1 = x + y\n",
    "        z2 = tf.nn.relu(z1)\n",
    "        result = tf.reduce_sum(z2)\n",
    "\n",
    "        tf.print(\"Final result:\", result)\n",
    "        return result\n",
    "\n",
    "    # Test both modes\n",
    "    x = tf.constant(tf.random.normal((3, 3)))\n",
    "    y = tf.constant(tf.random.normal((3, 3)))\n",
    "\n",
    "    print(\"\\n  Eager execution:\")\n",
    "    eager_result = eager_computation(x, y)\n",
    "\n",
    "    print(\"\\n  Graph execution:\")\n",
    "    graph_result = graph_computation(x, y)\n",
    "\n",
    "    print(f\"\\n  Results match: {tf.abs(eager_result - graph_result).numpy() < 1e-6}\")\n",
    "\n",
    "    # Example 2: Performance comparison\n",
    "    print(\"\\n\\n2. Performance Comparison:\")\n",
    "\n",
    "    def complex_computation_eager(x):\n",
    "        \"\"\"Complex computation in eager mode\"\"\"\n",
    "        for _i in range(10):\n",
    "            x = tf.nn.relu(x + 0.1)\n",
    "            x = tf.nn.dropout(x, rate=0.1)\n",
    "        return tf.reduce_sum(x)\n",
    "\n",
    "    @tf.function\n",
    "    def complex_computation_graph(x):\n",
    "        \"\"\"Same computation compiled to graph\"\"\"\n",
    "        for _i in range(10):\n",
    "            x = tf.nn.relu(x + 0.1)\n",
    "            x = tf.nn.dropout(x, rate=0.1)\n",
    "        return tf.reduce_sum(x)\n",
    "\n",
    "    # Benchmark performance\n",
    "    large_tensor = tf.random.normal((1000, 1000))\n",
    "\n",
    "    # Warm up graph compilation\n",
    "    _ = complex_computation_graph(large_tensor)\n",
    "\n",
    "    # Time eager execution\n",
    "    start_time = time.time()\n",
    "    for _ in range(5):\n",
    "        _ = complex_computation_eager(large_tensor)\n",
    "    eager_time = time.time() - start_time\n",
    "\n",
    "    # Time graph execution\n",
    "    start_time = time.time()\n",
    "    for _ in range(5):\n",
    "        _ = complex_computation_graph(large_tensor)\n",
    "    graph_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Eager execution (5 runs): {eager_time:.4f}s\")\n",
    "    print(f\"  Graph execution (5 runs): {graph_time:.4f}s\")\n",
    "    if graph_time > 0:\n",
    "        speedup = eager_time / graph_time\n",
    "        print(f\"  Graph speedup: {speedup:.2f}x\")\n",
    "\n",
    "    # Example 3: Graph tracing and retracing\n",
    "    print(\"\\n\\n3. Graph Tracing Behavior:\")\n",
    "\n",
    "    @tf.function\n",
    "    def traced_function(x):\n",
    "        print(f\"  Tracing with input shape: {x.shape}\")  # Only prints during tracing\n",
    "        return tf.reduce_sum(x * 2)\n",
    "\n",
    "    print(\"\\n  First call (triggers tracing):\")\n",
    "    result1 = traced_function(tf.constant([1.0, 2.0, 3.0]))\n",
    "    print(f\"  Result: {result1.numpy()}\")\n",
    "\n",
    "    print(\"\\n  Second call (uses cached graph):\")\n",
    "    result2 = traced_function(tf.constant([4.0, 5.0, 6.0]))\n",
    "    print(f\"  Result: {result2.numpy()}\")\n",
    "\n",
    "    print(\"\\n  Different shape (triggers retracing):\")\n",
    "    result3 = traced_function(tf.constant([1.0, 2.0]))  # Different shape!\n",
    "    print(f\"  Result: {result3.numpy()}\")\n",
    "\n",
    "    # Example 4: Conditional computation in graph mode\n",
    "    print(\"\\n\\n4. Conditional Computation in Graph Mode:\")\n",
    "\n",
    "    @tf.function\n",
    "    def conditional_graph_computation(x, training=True):\n",
    "        \"\"\"Conditional computation that works in graph mode\"\"\"\n",
    "        # Use tf.cond for conditional execution in graph mode\n",
    "        def apply_dropout():\n",
    "            return tf.nn.dropout(x, rate=0.5)\n",
    "\n",
    "        def no_dropout():\n",
    "            return x\n",
    "\n",
    "        # tf.cond is the graph-mode equivalent of if/else\n",
    "        x_processed = tf.cond(training, apply_dropout, no_dropout)\n",
    "\n",
    "        return tf.reduce_mean(x_processed)\n",
    "\n",
    "    test_tensor = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "    training_result = conditional_graph_computation(test_tensor, training=True)\n",
    "    inference_result = conditional_graph_computation(test_tensor, training=False)\n",
    "\n",
    "    print(f\"  Training mode result: {training_result.numpy():.4f}\")\n",
    "    print(f\"  Inference mode result: {inference_result.numpy():.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping graph mode examples\")\n",
    "\n",
    "print(\"\\nüü† TensorFlow Graph Mode Advantages:\")\n",
    "print(\"  ‚Ä¢ Excellent performance through optimization\")\n",
    "print(\"  ‚Ä¢ Ready for production deployment\")\n",
    "print(\"  ‚Ä¢ Memory efficient execution\")\n",
    "print(\"  ‚Ä¢ Cross-platform compatibility\")\n",
    "print(\"  ‚Ä¢ Can export to various formats (SavedModel, TFLite, etc.)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è TensorFlow Graph Mode Considerations:\")\n",
    "print(\"  ‚Ä¢ Limited Python control flow (use tf.cond, tf.while_loop)\")\n",
    "print(\"  ‚Ä¢ Debugging requires tf.print instead of print\")\n",
    "print(\"  ‚Ä¢ Graph retracing overhead with dynamic shapes\")\n",
    "print(\"  ‚Ä¢ Less flexible than eager execution\")\n",
    "print(\"  ‚Ä¢ Learning curve for graph-specific patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Debugging Strategies\n",
    "\n",
    "Different approaches to debugging in dynamic vs static execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEBUGGING STRATEGIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Debugging comparison\n",
    "pytorch_debug_code = \"\"\"\n",
    "# PyTorch - Easy debugging with standard Python\n",
    "import torch\n",
    "\n",
    "def debug_pytorch_model(x):\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Input mean: {x.mean().item():.4f}\")\n",
    "\n",
    "    # Can use regular Python debugging\n",
    "    y = torch.relu(x)\n",
    "    print(f\"After ReLU mean: {y.mean().item():.4f}\")\n",
    "\n",
    "    # Conditional debugging\n",
    "    if y.mean() > 0.5:\n",
    "        print(\"High activation detected!\")\n",
    "        # Can set breakpoints here\n",
    "        import pdb; pdb.set_trace()\n",
    "\n",
    "    z = y.sum()\n",
    "    print(f\"Final result: {z.item():.4f}\")\n",
    "\n",
    "    return z\n",
    "\n",
    "# Easy to debug\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "result = debug_pytorch_model(x)\n",
    "result.backward()\n",
    "\n",
    "# Can inspect gradients easily\n",
    "print(f\"Gradient: {x.grad}\")\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_debug_code = \"\"\"\n",
    "# TensorFlow - Different approaches for eager vs graph\n",
    "import tensorflow as tf\n",
    "\n",
    "# Eager mode debugging (similar to PyTorch)\n",
    "def debug_tf_eager(x):\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Input mean: {tf.reduce_mean(x).numpy():.4f}\")\n",
    "\n",
    "    y = tf.nn.relu(x)\n",
    "    print(f\"After ReLU mean: {tf.reduce_mean(y).numpy():.4f}\")\n",
    "\n",
    "    if tf.reduce_mean(y) > 0.5:\n",
    "        print(\"High activation detected!\")\n",
    "\n",
    "    return tf.reduce_sum(y)\n",
    "\n",
    "# Graph mode debugging (requires tf.print)\n",
    "@tf.function\n",
    "def debug_tf_graph(x):\n",
    "    tf.print(\"Input shape:\", tf.shape(x))\n",
    "    tf.print(\"Input mean:\", tf.reduce_mean(x))\n",
    "\n",
    "    y = tf.nn.relu(x)\n",
    "    tf.print(\"After ReLU mean:\", tf.reduce_mean(y))\n",
    "\n",
    "    # Conditional debugging in graph mode\n",
    "    tf.cond(tf.reduce_mean(y) > 0.5,\n",
    "            lambda: tf.print(\"High activation detected!\"),\n",
    "            lambda: tf.no_op())\n",
    "\n",
    "    return tf.reduce_sum(y)\n",
    "\n",
    "# Usage\n",
    "x = tf.Variable(tf.random.normal((10,)))\n",
    "with tf.GradientTape() as tape:\n",
    "    result = debug_tf_graph(x)\n",
    "gradients = tape.gradient(result, x)\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_debug_code,\n",
    "    tensorflow_debug_code,\n",
    "    \"Debugging Approaches\"\n",
    "))\n",
    "\n",
    "# Practical debugging examples\n",
    "if PYTORCH_AVAILABLE and TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüîç Practical Debugging Examples:\")\n",
    "\n",
    "    # PyTorch debugging\n",
    "    print(\"\\n1. PyTorch Debugging in Action:\")\n",
    "\n",
    "    def pytorch_debug_example():\n",
    "        x = torch.randn(5, 3, requires_grad=True)\n",
    "\n",
    "        # Easy inspection at any point\n",
    "        print(f\"  Input statistics: mean={x.mean().item():.4f}, std={x.std().item():.4f}\")\n",
    "        print(f\"  Input range: [{x.min().item():.4f}, {x.max().item():.4f}]\")\n",
    "\n",
    "        # Can check for common issues\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"  ‚ö†Ô∏è NaN detected in input!\")\n",
    "\n",
    "        if torch.isinf(x).any():\n",
    "            print(\"  ‚ö†Ô∏è Inf detected in input!\")\n",
    "\n",
    "        # Process data\n",
    "        y = torch.relu(x)\n",
    "        z = y.sum(dim=1)  # Sum over features\n",
    "\n",
    "        print(f\"  After processing: {z}\")\n",
    "        print(f\"  Gradient function: {z.grad_fn}\")\n",
    "\n",
    "        return z.sum()\n",
    "\n",
    "    result = pytorch_debug_example()\n",
    "\n",
    "    # TensorFlow debugging\n",
    "    print(\"\\n2. TensorFlow Debugging in Action:\")\n",
    "\n",
    "    def tensorflow_debug_example():\n",
    "        x = tf.Variable(tf.random.normal((5, 3)))\n",
    "\n",
    "        # Eager mode allows easy inspection\n",
    "        print(f\"  Input statistics: mean={tf.reduce_mean(x).numpy():.4f}, std={tf.math.reduce_std(x).numpy():.4f}\")\n",
    "        print(f\"  Input range: [{tf.reduce_min(x).numpy():.4f}, {tf.reduce_max(x).numpy():.4f}]\")\n",
    "\n",
    "        # Check for issues\n",
    "        if tf.reduce_any(tf.math.is_nan(x)):\n",
    "            print(\"  ‚ö†Ô∏è NaN detected in input!\")\n",
    "\n",
    "        if tf.reduce_any(tf.math.is_inf(x)):\n",
    "            print(\"  ‚ö†Ô∏è Inf detected in input!\")\n",
    "\n",
    "        # Process data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = tf.nn.relu(x)\n",
    "            z = tf.reduce_sum(y, axis=1)  # Sum over features\n",
    "            result = tf.reduce_sum(z)\n",
    "\n",
    "        print(f\"  After processing: {z.numpy()}\")\n",
    "\n",
    "        # Can compute gradients for debugging\n",
    "        gradients = tape.gradient(result, x)\n",
    "        print(f\"  Gradient statistics: mean={tf.reduce_mean(gradients).numpy():.4f}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    result = tensorflow_debug_example()\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Debugging Best Practices:\")\n",
    "\n",
    "debugging_tips = {\n",
    "    \"PyTorch\": [\n",
    "        \"Use standard Python debugging tools (pdb, print, etc.)\",\n",
    "        \"Check tensor shapes and values at each step\",\n",
    "        \"Use .item() to extract scalar values for printing\",\n",
    "        \"Inspect grad_fn to understand computation graph\",\n",
    "        \"Use torch.autograd.detect_anomaly() for gradient issues\"\n",
    "    ],\n",
    "    \"TensorFlow Eager\": [\n",
    "        \"Similar to PyTorch - use standard Python tools\",\n",
    "        \"Use .numpy() to extract values for inspection\",\n",
    "        \"Check tensor shapes and dtypes regularly\",\n",
    "        \"Use tf.debugging.assert_* functions for validation\",\n",
    "        \"Enable tf.debugging.enable_check_numerics() for NaN/Inf detection\"\n",
    "    ],\n",
    "    \"TensorFlow Graph\": [\n",
    "        \"Use tf.print() instead of print() for output\",\n",
    "        \"Use tf.debugging.assert_* for runtime checks\",\n",
    "        \"Add tf.summary for TensorBoard visualization\",\n",
    "        \"Use tf.py_function for complex debugging logic\",\n",
    "        \"Consider switching to eager mode for debugging\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for framework, tips in debugging_tips.items():\n",
    "    print(f\"\\n{framework}:\")\n",
    "    for tip in tips:\n",
    "        print(f\"  ‚Ä¢ {tip}\")\n",
    "\n",
    "print(\"\\nüìä Framework Comparison Summary:\")\n",
    "\n",
    "comparison_table = {\n",
    "    \"Aspect\": [\"Flexibility\", \"Performance\", \"Debugging\", \"Deployment\", \"Learning Curve\"],\n",
    "    \"PyTorch Dynamic\": [\"Very High\", \"Good\", \"Easy\", \"Requires Tracing\", \"Easy\"],\n",
    "    \"TensorFlow Eager\": [\"Very High\", \"Good\", \"Easy\", \"Requires Conversion\", \"Easy\"],\n",
    "    \"TensorFlow Graph\": [\"Limited\", \"Excellent\", \"Challenging\", \"Ready\", \"Moderate\"]\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Aspect':<15} | {'PyTorch':<15} | {'TF Eager':<15} | {'TF Graph':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, aspect in enumerate(comparison_table[\"Aspect\"]):\n",
    "    pytorch_val = comparison_table[\"PyTorch Dynamic\"][i]\n",
    "    tf_eager_val = comparison_table[\"TensorFlow Eager\"][i]\n",
    "    tf_graph_val = comparison_table[\"TensorFlow Graph\"][i]\n",
    "\n",
    "    print(f\"{aspect:<15} | {pytorch_val:<15} | {tf_eager_val:<15} | {tf_graph_val:<15}\")\n",
    "\n",
    "print(\"\\n‚úÖ Key Takeaways:\")\n",
    "print(\"  ‚Ä¢ Dynamic graphs offer maximum flexibility and easy debugging\")\n",
    "print(\"  ‚Ä¢ Static graphs provide optimal performance and deployment readiness\")\n",
    "print(\"  ‚Ä¢ TensorFlow offers both approaches - choose based on your needs\")\n",
    "print(\"  ‚Ä¢ Consider using eager mode for development, graph mode for production\")\n",
    "print(\"  ‚Ä¢ Both approaches support automatic differentiation effectively\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
