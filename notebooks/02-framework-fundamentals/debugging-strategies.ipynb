{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging Strategies: PyTorch vs TensorFlow\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master debugging techniques specific to each framework\n",
    "- Learn to identify and fix common deep learning issues\n",
    "- Understand profiling and optimization strategies\n",
    "- Develop systematic approaches to troubleshooting\n",
    "\n",
    "**Prerequisites:** Computational graphs, gradients, tensor operations\n",
    "\n",
    "**Estimated Time:** 40 minutes\n",
    "\n",
    "---\n",
    "\n",
    "Debugging deep learning models is often more challenging than traditional software debugging. This notebook covers:\n",
    "- **Framework-specific debugging tools**\n",
    "- **Common issues and their solutions**\n",
    "- **Performance profiling techniques**\n",
    "- **Best practices for systematic debugging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Add src to path for our utilities\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "from utils.comparison_tools import FrameworkComparison, create_side_by_side_comparison\n",
    "\n",
    "# Try to import frameworks\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"‚ùå TensorFlow not available\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "if PYTORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common Deep Learning Issues\n",
    "\n",
    "Let's start by identifying common problems and their symptoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMMON DEEP LEARNING ISSUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Common Issues and Symptoms:\n",
    "\n",
    "1. üî• EXPLODING GRADIENTS\n",
    "   ‚Ä¢ Loss becomes NaN or infinity\n",
    "   ‚Ä¢ Gradients have very large magnitudes\n",
    "   ‚Ä¢ Training becomes unstable\n",
    "\n",
    "2. üåä VANISHING GRADIENTS\n",
    "   ‚Ä¢ Loss stops decreasing\n",
    "   ‚Ä¢ Gradients become very small\n",
    "   ‚Ä¢ Early layers don't learn\n",
    "\n",
    "3. üíÄ DEAD NEURONS\n",
    "   ‚Ä¢ ReLU neurons output zero\n",
    "   ‚Ä¢ Gradients are zero\n",
    "   ‚Ä¢ Network capacity reduced\n",
    "\n",
    "4. üêå SLOW CONVERGENCE\n",
    "   ‚Ä¢ Loss decreases very slowly\n",
    "   ‚Ä¢ Poor learning rate choice\n",
    "   ‚Ä¢ Bad initialization\n",
    "\n",
    "5. üìà OVERFITTING\n",
    "   ‚Ä¢ Training loss << validation loss\n",
    "   ‚Ä¢ Model memorizes training data\n",
    "   ‚Ä¢ Poor generalization\n",
    "\"\"\")\n",
    "\n",
    "# Demonstrate exploding gradients\n",
    "print(\"\\n1. Exploding Gradients Example:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Exploding Gradients:\")\n",
    "    \n",
    "    def demonstrate_exploding_gradients():\n",
    "        # Create a deep network with poor initialization\n",
    "        layers = []\n",
    "        for i in range(10):  # Very deep network\n",
    "            layer = nn.Linear(50, 50)\n",
    "            # Bad initialization - too large weights\n",
    "            nn.init.normal_(layer.weight, mean=0, std=2.0)  # Large std\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = torch.randn(32, 50)\n",
    "        target = torch.randn(32, 50)\n",
    "        \n",
    "        prediction = model(x)\n",
    "        loss = nn.MSELoss()(prediction, target)\n",
    "        \n",
    "        print(f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(\"‚ö†Ô∏è  Loss is NaN or Inf - likely exploding gradients!\")\n",
    "            return\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check gradient magnitudes\n",
    "        total_norm = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "        print(f\"Total gradient norm: {total_norm:.4f}\")\n",
    "        \n",
    "        if total_norm > 100:\n",
    "            print(\"‚ö†Ô∏è  Large gradient norm - exploding gradients detected!\")\n",
    "            print(\"üí° Solutions: Gradient clipping, better initialization, skip connections\")\n",
    "        \n",
    "        return total_norm\n",
    "    \n",
    "    try:\n",
    "        grad_norm = demonstrate_exploding_gradients()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        print(\"This might be due to exploding gradients!\")\n",
    "\n",
    "# Demonstrate vanishing gradients\n",
    "print(\"\\n2. Vanishing Gradients Example:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Vanishing Gradients:\")\n",
    "    \n",
    "    def demonstrate_vanishing_gradients():\n",
    "        # Deep network with sigmoid activations (prone to vanishing gradients)\n",
    "        layers = []\n",
    "        for i in range(8):  # Deep network\n",
    "            layers.append(nn.Linear(20, 20))\n",
    "            layers.append(nn.Sigmoid())  # Sigmoid causes vanishing gradients\n",
    "        \n",
    "        model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = torch.randn(16, 20)\n",
    "        target = torch.randn(16, 20)\n",
    "        \n",
    "        prediction = model(x)\n",
    "        loss = nn.MSELoss()(prediction, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Analyze gradient magnitudes by layer\n",
    "        print(\"Gradient magnitudes by layer:\")\n",
    "        layer_idx = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name and param.grad is not None:\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                print(f\"Layer {layer_idx}: {grad_norm:.8f}\")\n",
    "                \n",
    "                if grad_norm < 1e-6:\n",
    "                    print(f\"  ‚ö†Ô∏è  Very small gradients - vanishing gradient problem!\")\n",
    "                \n",
    "                layer_idx += 1\n",
    "        \n",
    "        print(\"üí° Solutions: ReLU activations, residual connections, better initialization\")\n",
    "    \n",
    "    demonstrate_vanishing_gradients()\n",
    "\n",
    "# Dead ReLU neurons\n",
    "print(\"\\n3. Dead ReLU Neurons:\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Dead ReLU Detection:\")\n",
    "    \n",
    "    def detect_dead_neurons():\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(10, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize with large negative bias (causes dead ReLUs)\n",
    "        for layer in model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, -10)  # Large negative bias\n",
    "        \n",
    "        x = torch.randn(100, 10)\n",
    "        target = torch.randn(100, 1)\n",
    "        \n",
    "        # Forward pass with hooks to capture activations\n",
    "        activations = {}\n",
    "        \n",
    "        def hook_fn(name):\n",
    "            def hook(module, input, output):\n",
    "                activations[name] = output.detach()\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks\n",
    "        model[1].register_forward_hook(hook_fn('relu1'))\n",
    "        model[3].register_forward_hook(hook_fn('relu2'))\n",
    "        \n",
    "        prediction = model(x)\n",
    "        loss = nn.MSELoss()(prediction, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check for dead neurons\n",
    "        for name, activation in activations.items():\n",
    "            # Count neurons that are always zero\n",
    "            always_zero = (activation == 0).all(dim=0)\n",
    "            dead_count = always_zero.sum().item()\n",
    "            total_neurons = activation.shape[1]\n",
    "            \n",
    "            print(f\"{name}: {dead_count}/{total_neurons} dead neurons ({dead_count/total_neurons:.1%})\")\n",
    "            \n",
    "            if dead_count > total_neurons * 0.1:  # More than 10% dead\n",
    "                print(f\"  ‚ö†Ô∏è  High percentage of dead neurons!\")\n",
    "                print(f\"  üí° Solutions: Lower learning rate, better initialization, Leaky ReLU\")\n",
    "    \n",
    "    detect_dead_neurons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Framework-Specific Debugging Tools\n",
    "\n",
    "Each framework provides specific tools for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FRAMEWORK-SPECIFIC DEBUGGING TOOLS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch debugging tools\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Debugging Tools:\")\n",
    "    \n",
    "    # 1. Tensor inspection\n",
    "    print(\"\\n1. Tensor Inspection:\")\n",
    "    \n",
    "    def pytorch_tensor_inspection():\n",
    "        x = torch.randn(5, 3)\n",
    "        \n",
    "        print(f\"Tensor shape: {x.shape}\")\n",
    "        print(f\"Tensor dtype: {x.dtype}\")\n",
    "        print(f\"Tensor device: {x.device}\")\n",
    "        print(f\"Requires grad: {x.requires_grad}\")\n",
    "        print(f\"Is leaf: {x.is_leaf}\")\n",
    "        print(f\"Memory format: {x.is_contiguous()}\")\n",
    "        \n",
    "        # Check for problematic values\n",
    "        print(f\"Contains NaN: {torch.isnan(x).any()}\")\n",
    "        print(f\"Contains Inf: {torch.isinf(x).any()}\")\n",
    "        print(f\"Min value: {x.min().item():.4f}\")\n",
    "        print(f\"Max value: {x.max().item():.4f}\")\n",
    "        print(f\"Mean: {x.mean().item():.4f}\")\n",
    "        print(f\"Std: {x.std().item():.4f}\")\n",
    "    \n",
    "    pytorch_tensor_inspection()\n",
    "    \n",
    "    # 2. Model inspection\n",
    "    print(\"\\n2. Model Inspection:\")\n",
    "    \n",
    "    def pytorch_model_inspection():\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        )\n",
    "        \n",
    "        print(\"Model architecture:\")\n",
    "        print(model)\n",
    "        \n",
    "        print(\"\\nModel parameters:\")\n",
    "        total_params = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            print(f\"{name}: {param.shape} ({param.numel()} parameters)\")\n",
    "            total_params += param.numel()\n",
    "        \n",
    "        print(f\"Total parameters: {total_params}\")\n",
    "        \n",
    "        # Check parameter statistics\n",
    "        print(\"\\nParameter statistics:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            print(f\"{name}:\")\n",
    "            print(f\"  Mean: {param.data.mean().item():.6f}\")\n",
    "            print(f\"  Std: {param.data.std().item():.6f}\")\n",
    "            print(f\"  Min: {param.data.min().item():.6f}\")\n",
    "            print(f\"  Max: {param.data.max().item():.6f}\")\n",
    "    \n",
    "    pytorch_model_inspection()\n",
    "    \n",
    "    # 3. Gradient inspection\n",
    "    print(\"\\n3. Gradient Inspection:\")\n",
    "    \n",
    "    def pytorch_gradient_inspection():\n",
    "        model = nn.Linear(5, 1)\n",
    "        x = torch.randn(10, 5)\n",
    "        target = torch.randn(10, 1)\n",
    "        \n",
    "        prediction = model(x)\n",
    "        loss = nn.MSELoss()(prediction, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        print(\"Gradient inspection:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad = param.grad\n",
    "                print(f\"{name} gradient:\")\n",
    "                print(f\"  Shape: {grad.shape}\")\n",
    "                print(f\"  Norm: {grad.norm().item():.6f}\")\n",
    "                print(f\"  Mean: {grad.mean().item():.6f}\")\n",
    "                print(f\"  Contains NaN: {torch.isnan(grad).any()}\")\n",
    "                print(f\"  Contains Inf: {torch.isinf(grad).any()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient computed\")\n",
    "    \n",
    "    pytorch_gradient_inspection()\n",
    "\n",
    "# TensorFlow debugging tools\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Debugging Tools:\")\n",
    "    \n",
    "    # 1. Tensor inspection\n",
    "    print(\"\\n1. Tensor Inspection:\")\n",
    "    \n",
    "    def tensorflow_tensor_inspection():\n",
    "        x = tf.random.normal((5, 3))\n",
    "        \n",
    "        print(f\"Tensor shape: {x.shape}\")\n",
    "        print(f\"Tensor dtype: {x.dtype}\")\n",
    "        print(f\"Tensor device: {x.device}\")\n",
    "        \n",
    "        # Check for problematic values\n",
    "        print(f\"Contains NaN: {tf.reduce_any(tf.math.is_nan(x))}\")\n",
    "        print(f\"Contains Inf: {tf.reduce_any(tf.math.is_inf(x))}\")\n",
    "        print(f\"Min value: {tf.reduce_min(x).numpy():.4f}\")\n",
    "        print(f\"Max value: {tf.reduce_max(x).numpy():.4f}\")\n",
    "        print(f\"Mean: {tf.reduce_mean(x).numpy():.4f}\")\n",
    "        print(f\"Std: {tf.math.reduce_std(x).numpy():.4f}\")\n",
    "    \n",
    "    tensorflow_tensor_inspection()\n",
    "    \n",
    "    # 2. Model inspection\n",
    "    print(\"\\n2. Model Inspection:\")\n",
    "    \n",
    "    def tensorflow_model_inspection():\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(20, activation='relu', input_shape=(10,)),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # Build the model\n",
    "        model.build((None, 10))\n",
    "        \n",
    "        print(\"Model summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        print(\"\\nModel weights:\")\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if layer.weights:\n",
    "                print(f\"Layer {i} ({layer.name}):\")\n",
    "                for j, weight in enumerate(layer.weights):\n",
    "                    print(f\"  Weight {j}: {weight.shape}\")\n",
    "                    print(f\"    Mean: {tf.reduce_mean(weight).numpy():.6f}\")\n",
    "                    print(f\"    Std: {tf.math.reduce_std(weight).numpy():.6f}\")\n",
    "    \n",
    "    tensorflow_model_inspection()\n",
    "    \n",
    "    # 3. Debugging with tf.debugging\n",
    "    print(\"\\n3. TensorFlow Debugging Utilities:\")\n",
    "    \n",
    "    def tensorflow_debugging_utilities():\n",
    "        # Create some problematic data\n",
    "        x = tf.constant([1.0, 2.0, float('nan'), 4.0])\n",
    "        \n",
    "        print(\"Debugging utilities:\")\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        try:\n",
    "            tf.debugging.check_numerics(x, \"Input contains NaN or Inf\")\n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "            print(f\"Caught error: {e}\")\n",
    "        \n",
    "        # Assert operations\n",
    "        y = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "        \n",
    "        # This will pass\n",
    "        tf.debugging.assert_all_finite(y, \"y should be finite\")\n",
    "        print(\"All finite assertion passed\")\n",
    "        \n",
    "        # Assert shapes\n",
    "        tf.debugging.assert_equal(tf.shape(y), [4], \"Shape should be [4]\")\n",
    "        print(\"Shape assertion passed\")\n",
    "        \n",
    "        # Assert ranges\n",
    "        tf.debugging.assert_greater_equal(y, 0.0, \"All values should be >= 0\")\n",
    "        print(\"Range assertion passed\")\n",
    "    \n",
    "    tensorflow_debugging_utilities()\n",
    "\n",
    "# Side-by-side debugging comparison\n",
    "pytorch_debug_code = \"\"\"\n",
    "# PyTorch debugging workflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Check tensor properties\n",
    "x = torch.randn(5, 3)\n",
    "print(f\"Shape: {x.shape}, dtype: {x.dtype}\")\n",
    "print(f\"NaN: {torch.isnan(x).any()}\")\n",
    "print(f\"Inf: {torch.isinf(x).any()}\")\n",
    "\n",
    "# 2. Model inspection\n",
    "model = nn.Linear(3, 1)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# 3. Gradient checking\n",
    "loss = nn.MSELoss()(model(x), torch.randn(5, 1))\n",
    "loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad norm: {param.grad.norm()}\")\n",
    "\n",
    "# 4. Hook for intermediate values\n",
    "def hook_fn(module, input, output):\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "model.register_forward_hook(hook_fn)\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_debug_code = \"\"\"\n",
    "# TensorFlow debugging workflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. Check tensor properties\n",
    "x = tf.random.normal((5, 3))\n",
    "print(f\"Shape: {x.shape}, dtype: {x.dtype}\")\n",
    "print(f\"NaN: {tf.reduce_any(tf.math.is_nan(x))}\")\n",
    "print(f\"Inf: {tf.reduce_any(tf.math.is_inf(x))}\")\n",
    "\n",
    "# 2. Model inspection\n",
    "model = tf.keras.layers.Dense(1)\n",
    "model.build((None, 3))\n",
    "model.summary()\n",
    "\n",
    "# 3. Gradient checking\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = model(x)\n",
    "    loss = tf.reduce_mean(tf.square(prediction - tf.random.normal((5, 1))))\n",
    "\n",
    "gradients = tape.gradient(loss, model.trainable_variables)\n",
    "for i, grad in enumerate(gradients):\n",
    "    print(f\"Gradient {i} norm: {tf.norm(grad)}\")\n",
    "\n",
    "# 4. Debugging assertions\n",
    "tf.debugging.check_numerics(x, \"Input check\")\n",
    "tf.debugging.assert_all_finite(prediction, \"Prediction check\")\n",
    "\n",
    "# 5. Print debugging in graph mode\n",
    "@tf.function\n",
    "def debug_function(x):\n",
    "    tf.print(\"Debug info:\", tf.shape(x))\n",
    "    return tf.reduce_mean(x)\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_debug_code, tensorflow_debug_code, \"Debugging Workflows\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Profiling\n",
    "\n",
    "Understanding where your model spends time and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE PROFILING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyTorch profiling\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"\\nüî• PyTorch Profiling:\")\n",
    "    \n",
    "    def pytorch_profiling_example():\n",
    "        # Create a model for profiling\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(100, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "        \n",
    "        x = torch.randn(32, 100)\n",
    "        target = torch.randint(0, 10, (32,))\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        \n",
    "        # Simple timing\n",
    "        print(\"1. Simple Timing:\")\n",
    "        \n",
    "        # Warm up\n",
    "        for _ in range(5):\n",
    "            prediction = model(x)\n",
    "            loss = criterion(prediction, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Time forward pass\n",
    "        start_time = time.time()\n",
    "        for _ in range(100):\n",
    "            prediction = model(x)\n",
    "        forward_time = (time.time() - start_time) / 100\n",
    "        \n",
    "        # Time backward pass\n",
    "        start_time = time.time()\n",
    "        for _ in range(100):\n",
    "            prediction = model(x)\n",
    "            loss = criterion(prediction, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "        backward_time = (time.time() - start_time) / 100 - forward_time\n",
    "        \n",
    "        print(f\"Forward pass: {forward_time*1000:.2f} ms\")\n",
    "        print(f\"Backward pass: {backward_time*1000:.2f} ms\")\n",
    "        \n",
    "        # Memory usage\n",
    "        print(\"\\n2. Memory Usage:\")\n",
    "        \n",
    "        def get_memory_usage():\n",
    "            if torch.cuda.is_available():\n",
    "                return torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "            else:\n",
    "                # Approximate CPU memory (not exact)\n",
    "                total_params = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "                return total_params / 1024**2\n",
    "        \n",
    "        memory_before = get_memory_usage()\n",
    "        prediction = model(x)\n",
    "        memory_after = get_memory_usage()\n",
    "        \n",
    "        print(f\"Memory before forward: {memory_before:.2f} MB\")\n",
    "        print(f\"Memory after forward: {memory_after:.2f} MB\")\n",
    "        print(f\"Memory increase: {memory_after - memory_before:.2f} MB\")\n",
    "        \n",
    "        # Model size analysis\n",
    "        print(\"\\n3. Model Analysis:\")\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2\n",
    "        \n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"Model size: {model_size:.2f} MB\")\n",
    "        \n",
    "        # FLOPs estimation (rough)\n",
    "        def estimate_flops():\n",
    "            flops = 0\n",
    "            for layer in model:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    # Matrix multiplication: 2 * input_size * output_size * batch_size\n",
    "                    flops += 2 * layer.in_features * layer.out_features * x.size(0)\n",
    "            return flops\n",
    "        \n",
    "        estimated_flops = estimate_flops()\n",
    "        print(f\"Estimated FLOPs per forward pass: {estimated_flops:,}\")\n",
    "        print(f\"Estimated FLOP/s: {estimated_flops / forward_time / 1e9:.2f} GFLOP/s\")\n",
    "    \n",
    "    pytorch_profiling_example()\n",
    "\n",
    "# TensorFlow profiling\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\nüü† TensorFlow Profiling:\")\n",
    "    \n",
    "    def tensorflow_profiling_example():\n",
    "        # Create a model for profiling\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(200, activation='relu', input_shape=(100,)),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        x = tf.random.normal((32, 100))\n",
    "        target = tf.random.uniform((32,), maxval=10, dtype=tf.int32)\n",
    "        target_onehot = tf.one_hot(target, 10)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "        \n",
    "        # Compile for better performance\n",
    "        @tf.function\n",
    "        def train_step(x, y):\n",
    "            with tf.GradientTape() as tape:\n",
    "                prediction = model(x, training=True)\n",
    "                loss = loss_fn(y, prediction)\n",
    "            \n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "        \n",
    "        # Warm up\n",
    "        for _ in range(5):\n",
    "            train_step(x, target_onehot)\n",
    "        \n",
    "        print(\"1. Simple Timing:\")\n",
    "        \n",
    "        # Time forward pass\n",
    "        start_time = time.time()\n",
    "        for _ in range(100):\n",
    "            prediction = model(x, training=False)\n",
    "        forward_time = (time.time() - start_time) / 100\n",
    "        \n",
    "        # Time full training step\n",
    "        start_time = time.time()\n",
    "        for _ in range(100):\n",
    "            train_step(x, target_onehot)\n",
    "        train_time = (time.time() - start_time) / 100\n",
    "        \n",
    "        print(f\"Forward pass: {forward_time*1000:.2f} ms\")\n",
    "        print(f\"Full training step: {train_time*1000:.2f} ms\")\n",
    "        print(f\"Backward pass (estimated): {(train_time - forward_time)*1000:.2f} ms\")\n",
    "        \n",
    "        # Model analysis\n",
    "        print(\"\\n2. Model Analysis:\")\n",
    "        \n",
    "        total_params = model.count_params()\n",
    "        model_size = sum([tf.size(var).numpy() * 4 for var in model.trainable_variables]) / 1024**2  # Assume float32\n",
    "        \n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Model size: {model_size:.2f} MB\")\n",
    "        \n",
    "        # Layer-wise analysis\n",
    "        print(\"\\n3. Layer-wise Analysis:\")\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if hasattr(layer, 'count_params'):\n",
    "                layer_params = layer.count_params()\n",
    "                print(f\"Layer {i} ({layer.name}): {layer_params:,} parameters\")\n",
    "    \n",
    "    tensorflow_profiling_example()\n",
    "\n",
    "# Profiling best practices\n",
    "print(\"\\nüìã Profiling Best Practices:\")\n",
    "profiling_tips = [\n",
    "    \"üî• Always warm up before timing (JIT compilation, caching)\",\n",
    "    \"üìä Profile both forward and backward passes separately\",\n",
    "    \"üíæ Monitor memory usage, especially for large models\",\n",
    "    \"üéØ Profile on target hardware (CPU vs GPU)\",\n",
    "    \"üìà Use built-in profilers for detailed analysis\",\n",
    "    \"‚ö° Compare eager vs compiled execution (TensorFlow)\",\n",
    "    \"üîç Profile individual layers to find bottlenecks\",\n",
    "    \"üìè Measure actual throughput (samples/second)\",\n",
    "    \"üéõÔ∏è Profile different batch sizes\",\n",
    "    \"üîÑ Profile data loading pipeline separately\"\n",
    "]\n",
    "\n",
    "for tip in profiling_tips:\n",
    "    print(f\"  {tip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Systematic Debugging Approach\n",
    "\n",
    "A step-by-step methodology for debugging deep learning issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SYSTEMATIC DEBUGGING APPROACH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "üîç SYSTEMATIC DEBUGGING METHODOLOGY\n",
    "\n",
    "1. üìã REPRODUCE THE ISSUE\n",
    "   ‚Ä¢ Create minimal reproducible example\n",
    "   ‚Ä¢ Fix random seeds for consistency\n",
    "   ‚Ä¢ Document exact environment and versions\n",
    "\n",
    "2. üéØ ISOLATE THE PROBLEM\n",
    "   ‚Ä¢ Test individual components\n",
    "   ‚Ä¢ Use simple synthetic data\n",
    "   ‚Ä¢ Start with smallest possible model\n",
    "\n",
    "3. üìä GATHER INFORMATION\n",
    "   ‚Ä¢ Check tensor shapes and dtypes\n",
    "   ‚Ä¢ Monitor loss curves\n",
    "   ‚Ä¢ Inspect gradients and activations\n",
    "   ‚Ä¢ Profile performance\n",
    "\n",
    "4. üî¨ FORM HYPOTHESES\n",
    "   ‚Ä¢ Based on symptoms, what could be wrong?\n",
    "   ‚Ä¢ List possible causes in order of likelihood\n",
    "   ‚Ä¢ Consider common issues first\n",
    "\n",
    "5. üß™ TEST HYPOTHESES\n",
    "   ‚Ä¢ Test one hypothesis at a time\n",
    "   ‚Ä¢ Make minimal changes\n",
    "   ‚Ä¢ Document what works and what doesn't\n",
    "\n",
    "6. ‚úÖ VERIFY THE FIX\n",
    "   ‚Ä¢ Test on original problem\n",
    "   ‚Ä¢ Ensure no regression\n",
    "   ‚Ä¢ Add tests to prevent future issues\n",
    "\"\"\")\n",
    "\n",
    "# Debugging checklist\n",
    "print(\"\\nüìù DEBUGGING CHECKLIST:\")\n",
    "\n",
    "debugging_checklist = {\n",
    "    \"Data Issues\": [\n",
    "        \"‚úì Check data shapes and types\",\n",
    "        \"‚úì Verify data preprocessing\",\n",
    "        \"‚úì Look for NaN/Inf in inputs\",\n",
    "        \"‚úì Check data distribution\",\n",
    "        \"‚úì Verify labels are correct\"\n",
    "    ],\n",
    "    \"Model Issues\": [\n",
    "        \"‚úì Verify model architecture\",\n",
    "        \"‚úì Check parameter initialization\",\n",
    "        \"‚úì Ensure proper activation functions\",\n",
    "        \"‚úì Verify loss function choice\",\n",
    "        \"‚úì Check for parameter updates\"\n",
    "    ],\n",
    "    \"Training Issues\": [\n",
    "        \"‚úì Check learning rate\",\n",
    "        \"‚úì Verify optimizer settings\",\n",
    "        \"‚úì Monitor gradient magnitudes\",\n",
    "        \"‚úì Check for gradient clipping\",\n",
    "        \"‚úì Verify batch size effects\"\n",
    "    ],\n",
    "    \"Implementation Issues\": [\n",
    "        \"‚úì Check tensor operations\",\n",
    "        \"‚úì Verify device placement\",\n",
    "        \"‚úì Check memory usage\",\n",
    "        \"‚úì Verify random seed setting\",\n",
    "        \"‚úì Check framework versions\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in debugging_checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "# Common solutions\n",
    "print(\"\\nüí° COMMON SOLUTIONS:\")\n",
    "\n",
    "common_solutions = {\n",
    "    \"Loss not decreasing\": [\n",
    "        \"‚Ä¢ Check learning rate (try 1e-3, 1e-4)\",\n",
    "        \"‚Ä¢ Verify data preprocessing\",\n",
    "        \"‚Ä¢ Check model capacity\",\n",
    "        \"‚Ä¢ Try different optimizer\",\n",
    "        \"‚Ä¢ Check for label issues\"\n",
    "    ],\n",
    "    \"Loss becomes NaN\": [\n",
    "        \"‚Ä¢ Reduce learning rate\",\n",
    "        \"‚Ä¢ Add gradient clipping\",\n",
    "        \"‚Ä¢ Check for division by zero\",\n",
    "        \"‚Ä¢ Use more stable loss function\",\n",
    "        \"‚Ä¢ Check input data for NaN/Inf\"\n",
    "    ],\n",
    "    \"Training too slow\": [\n",
    "        \"‚Ä¢ Increase batch size\",\n",
    "        \"‚Ä¢ Use GPU acceleration\",\n",
    "        \"‚Ä¢ Optimize data loading\",\n",
    "        \"‚Ä¢ Use mixed precision training\",\n",
    "        \"‚Ä¢ Profile and optimize bottlenecks\"\n",
    "    ],\n",
    "    \"Memory issues\": [\n",
    "        \"‚Ä¢ Reduce batch size\",\n",
    "        \"‚Ä¢ Use gradient accumulation\",\n",
    "        \"‚Ä¢ Clear unused variables\",\n",
    "        \"‚Ä¢ Use gradient checkpointing\",\n",
    "        \"‚Ä¢ Optimize model architecture\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for problem, solutions in common_solutions.items():\n",
    "    print(f\"\\n{problem}:\")\n",
    "    for solution in solutions:\n",
    "        print(f\"  {solution}\")\n",
    "\n",
    "# Framework-specific debugging commands\n",
    "print(\"\\nüõ†Ô∏è QUICK DEBUGGING COMMANDS:\")\n",
    "\n",
    "pytorch_debug_commands = \"\"\"\n",
    "# PyTorch Quick Debug Commands\n",
    "\n",
    "# Check tensor properties\n",
    "print(f\"Shape: {tensor.shape}, dtype: {tensor.dtype}\")\n",
    "print(f\"Device: {tensor.device}, requires_grad: {tensor.requires_grad}\")\n",
    "print(f\"NaN: {torch.isnan(tensor).any()}, Inf: {torch.isinf(tensor).any()}\")\n",
    "\n",
    "# Check gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: grad_norm={param.grad.norm():.6f}\")\n",
    "\n",
    "# Memory debugging\n",
    "print(f\"CUDA memory: {torch.cuda.memory_allocated()/1e6:.1f}MB\")\n",
    "\n",
    "# Gradient checking\n",
    "torch.autograd.gradcheck(model, input_tensor)\n",
    "\n",
    "# Hook for intermediate values\n",
    "def debug_hook(module, input, output):\n",
    "    print(f\"{module.__class__.__name__}: {output.shape}\")\n",
    "model.register_forward_hook(debug_hook)\n",
    "\"\"\"\n",
    "\n",
    "tensorflow_debug_commands = \"\"\"\n",
    "# TensorFlow Quick Debug Commands\n",
    "\n",
    "# Check tensor properties\n",
    "print(f\"Shape: {tensor.shape}, dtype: {tensor.dtype}\")\n",
    "print(f\"Device: {tensor.device}\")\n",
    "print(f\"NaN: {tf.reduce_any(tf.math.is_nan(tensor))}\")\n",
    "print(f\"Inf: {tf.reduce_any(tf.math.is_inf(tensor))}\")\n",
    "\n",
    "# Debugging assertions\n",
    "tf.debugging.check_numerics(tensor, \"Tensor check\")\n",
    "tf.debugging.assert_all_finite(tensor, \"Finite check\")\n",
    "\n",
    "# Print in graph mode\n",
    "tf.print(\"Debug:\", tensor)\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Check gradients\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = model(x)\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "for i, grad in enumerate(grads):\n",
    "    tf.print(f\"Grad {i} norm:\", tf.norm(grad))\n",
    "\"\"\"\n",
    "\n",
    "print(create_side_by_side_comparison(\n",
    "    pytorch_debug_commands, tensorflow_debug_commands, \"Quick Debug Commands\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "**What we've learned:**\n",
    "\n",
    "1. **Common Issues**: Exploding/vanishing gradients, dead neurons, slow convergence, overfitting\n",
    "2. **Framework Tools**: PyTorch's dynamic debugging vs TensorFlow's assertion system\n",
    "3. **Profiling**: Performance analysis and bottleneck identification\n",
    "4. **Systematic Approach**: Methodical debugging workflow\n",
    "5. **Best Practices**: Checklists and common solutions\n",
    "\n",
    "**Framework-Specific Debugging:**\n",
    "\n",
    "| Aspect | PyTorch | TensorFlow |\n",
    "|--------|---------|------------|\n",
    "| **Tensor Inspection** | `.shape`, `.dtype`, `torch.isnan()` | `.shape`, `.dtype`, `tf.math.is_nan()` |\n",
    "| **Model Inspection** | `.named_parameters()`, hooks | `.summary()`, `.trainable_variables` |\n",
    "| **Gradient Checking** | `torch.autograd.gradcheck()` | Manual numerical checking |\n",
    "| **Assertions** | Python `assert` | `tf.debugging.assert_*()` |\n",
    "| **Print Debugging** | Standard `print()` | `tf.print()` for graphs |\n",
    "| **Profiling** | Manual timing, hooks | `tf.profiler`, manual timing |\n",
    "\n",
    "**Debugging Workflow:**\n",
    "\n",
    "1. **Reproduce** ‚Üí Create minimal example\n",
    "2. **Isolate** ‚Üí Test components separately\n",
    "3. **Gather** ‚Üí Collect diagnostic information\n",
    "4. **Hypothesize** ‚Üí Form theories about the issue\n",
    "5. **Test** ‚Üí Verify hypotheses systematically\n",
    "6. **Verify** ‚Üí Ensure the fix works\n",
    "\n",
    "**Common Issue Patterns:**\n",
    "\n",
    "**Exploding Gradients:**\n",
    "- Symptoms: NaN loss, very large gradient norms\n",
    "- Solutions: Gradient clipping, lower learning rate, better initialization\n",
    "\n",
    "**Vanishing Gradients:**\n",
    "- Symptoms: Loss plateaus, small gradients in early layers\n",
    "- Solutions: ReLU activations, residual connections, proper initialization\n",
    "\n",
    "**Dead Neurons:**\n",
    "- Symptoms: Many ReLU outputs are zero, zero gradients\n",
    "- Solutions: Lower learning rate, Leaky ReLU, better initialization\n",
    "\n",
    "**Performance Issues:**\n",
    "- Symptoms: Slow training, high memory usage\n",
    "- Solutions: Profiling, batch size optimization, GPU utilization\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "**Prevention:**\n",
    "- Use proper initialization (Xavier, He)\n",
    "- Choose appropriate learning rates\n",
    "- Monitor training metrics continuously\n",
    "- Use gradient clipping for RNNs\n",
    "- Validate data preprocessing\n",
    "\n",
    "**Debugging:**\n",
    "- Start with simple cases\n",
    "- Check one thing at a time\n",
    "- Use version control for experiments\n",
    "- Document findings and solutions\n",
    "- Create reproducible examples\n",
    "\n",
    "**Tools and Resources:**\n",
    "\n",
    "**PyTorch:**\n",
    "- TensorBoard for visualization\n",
    "- `torch.autograd.gradcheck()` for gradient verification\n",
    "- Hooks for intermediate inspection\n",
    "- `torch.profiler` for detailed profiling\n",
    "\n",
    "**TensorFlow:**\n",
    "- TensorBoard for visualization\n",
    "- `tf.debugging.*` for assertions\n",
    "- `tf.profiler` for performance analysis\n",
    "- Eager execution for easier debugging\n",
    "\n",
    "**Next Steps:**\n",
    "- Practice debugging on real projects\n",
    "- Learn advanced profiling techniques\n",
    "- Study framework-specific optimization\n",
    "- Explore distributed training debugging\n",
    "\n",
    "Effective debugging is a crucial skill for deep learning practitioners. Both PyTorch and TensorFlow provide powerful tools, but the key is developing a systematic approach to problem-solving and knowing which tools to use when."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
