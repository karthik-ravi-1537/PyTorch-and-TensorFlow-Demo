{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy Essentials for Machine Learning\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master NumPy arrays, operations, and broadcasting for ML workflows\n",
    "- Understand how NumPy serves as the foundation for both PyTorch and TensorFlow\n",
    "- Learn essential linear algebra operations used in deep learning\n",
    "- Practice data manipulation patterns common in ML preprocessing\n",
    "\n",
    "**Prerequisites:** Basic Python knowledge\n",
    "\n",
    "**Estimated Time:** 30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "NumPy is the foundational library that both PyTorch and TensorFlow build upon. Understanding NumPy is crucial because:\n",
    "- Both frameworks can seamlessly convert to/from NumPy arrays\n",
    "- Many preprocessing operations are done in NumPy\n",
    "- The concepts translate directly to tensor operations\n",
    "- Debugging often involves converting tensors to NumPy for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Add src to path for our utilities\n",
    "sys.path.append(os.path.join('..', '..', 'src'))\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Array Creation and Basic Properties\n",
    "\n",
    "Understanding how to create and inspect arrays is fundamental to ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways to create arrays (common in ML)\n",
    "\n",
    "# From lists (loading data)\n",
    "data_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "arr_from_list = np.array(data_list)\n",
    "print(\"From list:\")\n",
    "print(arr_from_list)\n",
    "print(f\"Shape: {arr_from_list.shape}, Dtype: {arr_from_list.dtype}\\n\")\n",
    "\n",
    "# Zeros (weight initialization)\n",
    "weights = np.zeros((3, 4))\n",
    "print(\"Zeros (weight initialization):\")\n",
    "print(weights)\n",
    "print(f\"Shape: {weights.shape}\\n\")\n",
    "\n",
    "# Random arrays (data generation, weight initialization)\n",
    "random_data = np.random.randn(100, 5)  # 100 samples, 5 features\n",
    "print(\"Random data (first 5 rows):\")\n",
    "print(random_data[:5])\n",
    "print(f\"Shape: {random_data.shape}\\n\")\n",
    "\n",
    "# Identity matrix (useful for regularization)\n",
    "identity = np.eye(3)\n",
    "print(\"Identity matrix:\")\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array properties essential for ML\n",
    "sample_array = np.random.randn(32, 10, 8)  # Batch size 32, sequence length 10, features 8\n",
    "\n",
    "print(\"Array Properties (typical ML batch):\")\n",
    "print(f\"Shape: {sample_array.shape} (batch_size, seq_len, features)\")\n",
    "print(f\"Number of dimensions: {sample_array.ndim}\")\n",
    "print(f\"Total elements: {sample_array.size}\")\n",
    "print(f\"Data type: {sample_array.dtype}\")\n",
    "print(f\"Memory usage: {sample_array.nbytes} bytes\")\n",
    "print(f\"Memory usage: {sample_array.nbytes / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Array Indexing and Slicing\n",
    "\n",
    "Critical for data manipulation, batch processing, and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data representing a batch of images\n",
    "# Shape: (batch_size, height, width, channels)\n",
    "batch_images = np.random.randint(0, 256, size=(8, 28, 28, 3))\n",
    "\n",
    "print(\"Batch of images shape:\", batch_images.shape)\n",
    "print(\"\\nIndexing and Slicing Examples:\")\n",
    "\n",
    "# Get first image\n",
    "first_image = batch_images[0]\n",
    "print(f\"First image shape: {first_image.shape}\")\n",
    "\n",
    "# Get first 4 images (mini-batch)\n",
    "mini_batch = batch_images[:4]\n",
    "print(f\"Mini-batch shape: {mini_batch.shape}\")\n",
    "\n",
    "# Get red channel from all images\n",
    "red_channel = batch_images[:, :, :, 0]\n",
    "print(f\"Red channel shape: {red_channel.shape}\")\n",
    "\n",
    "# Get center crop (common preprocessing)\n",
    "center_crop = batch_images[:, 7:21, 7:21, :]\n",
    "print(f\"Center crop shape: {center_crop.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing (filtering data)\n",
    "scores = np.array([85, 92, 78, 96, 88, 73, 91, 82])\n",
    "names = np.array(['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry'])\n",
    "\n",
    "print(\"Original scores:\", scores)\n",
    "print(\"Names:\", names)\n",
    "\n",
    "# Filter high performers (score > 85)\n",
    "high_performers = scores > 85\n",
    "print(f\"\\nHigh performers mask: {high_performers}\")\n",
    "print(f\"High performer scores: {scores[high_performers]}\")\n",
    "print(f\"High performer names: {names[high_performers]}\")\n",
    "\n",
    "# Multiple conditions\n",
    "good_range = (scores >= 80) & (scores <= 90)\n",
    "print(f\"\\nScores in 80-90 range: {scores[good_range]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Array Operations and Broadcasting\n",
    "\n",
    "Broadcasting is crucial for efficient ML computations and is used extensively in both PyTorch and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise operations (fundamental to neural networks)\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([[2, 2, 2], [3, 3, 3]])\n",
    "\n",
    "print(\"Array a:\")\n",
    "print(a)\n",
    "print(\"\\nArray b:\")\n",
    "print(b)\n",
    "\n",
    "print(\"\\nElement-wise operations:\")\n",
    "print(\"Addition (a + b):\")\n",
    "print(a + b)\n",
    "\n",
    "print(\"\\nMultiplication (a * b):\")\n",
    "print(a * b)\n",
    "\n",
    "print(\"\\nSquare (a**2):\")\n",
    "print(a**2)\n",
    "\n",
    "# Activation functions\n",
    "print(\"\\nCommon activation functions:\")\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"ReLU (max(0, x)): {np.maximum(0, x)}\")\n",
    "print(f\"Sigmoid: {1 / (1 + np.exp(-x))}\")\n",
    "print(f\"Tanh: {np.tanh(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting examples (very important for ML)\n",
    "print(\"Broadcasting Examples:\")\n",
    "\n",
    "# Example 1: Adding bias to all samples\n",
    "features = np.random.randn(100, 5)  # 100 samples, 5 features\n",
    "bias = np.array([0.1, -0.2, 0.3, -0.1, 0.2])  # bias for each feature\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Bias shape: {bias.shape}\")\n",
    "\n",
    "# Broadcasting adds bias to each sample\n",
    "features_with_bias = features + bias\n",
    "print(f\"Result shape: {features_with_bias.shape}\")\n",
    "print(f\"First sample before: {features[0]}\")\n",
    "print(f\"First sample after: {features_with_bias[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Example 2: Normalizing features (mean centering)\n",
    "data = np.random.randn(1000, 3) * 10 + 5  # Add some offset and scale\n",
    "print(f\"\\nOriginal data shape: {data.shape}\")\n",
    "print(f\"Original means: {np.mean(data, axis=0)}\")\n",
    "print(f\"Original stds: {np.std(data, axis=0)}\")\n",
    "\n",
    "# Normalize (broadcasting)\n",
    "mean = np.mean(data, axis=0)  # Shape: (3,)\n",
    "std = np.std(data, axis=0)    # Shape: (3,)\n",
    "normalized_data = (data - mean) / std  # Broadcasting!\n",
    "\n",
    "print(f\"\\nNormalized means: {np.mean(normalized_data, axis=0)}\")\n",
    "print(f\"Normalized stds: {np.std(normalized_data, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting rules visualization\n",
    "print(\"Broadcasting Rules Examples:\")\n",
    "\n",
    "# Rule: Arrays are aligned from the rightmost dimension\n",
    "examples = [\n",
    "    ((3, 4), (4,)),      # (3,4) + (4,) -> (3,4)\n",
    "    ((2, 3, 4), (4,)),   # (2,3,4) + (4,) -> (2,3,4)\n",
    "    ((2, 3, 4), (3, 4)), # (2,3,4) + (3,4) -> (2,3,4)\n",
    "    ((2, 1, 4), (3, 4)), # (2,1,4) + (3,4) -> (2,3,4)\n",
    "]\n",
    "\n",
    "for shape1, shape2 in examples:\n",
    "    a = np.ones(shape1)\n",
    "    b = np.ones(shape2)\n",
    "    result = a + b\n",
    "    print(f\"\\n{shape1} + {shape2} -> {result.shape}\")\n",
    "    print(\"\\nArray A:\")\n",
    "    print(a)\n",
    "    print(\"\\nArray B:\")\n",
    "    print(b)\n",
    "    print(\"\\nResult:\")\n",
    "    print(result)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Algebra Operations\n",
    "\n",
    "Essential for understanding neural network computations, matrix multiplications, and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication (core of neural networks)\n",
    "print(\"Matrix Multiplication Examples:\")\n",
    "\n",
    "# Simulate a simple neural network layer\n",
    "# Input: batch_size=32, input_features=10\n",
    "# Layer: input_features=10, output_features=5\n",
    "batch_size, input_features, output_features = 32, 10, 5\n",
    "\n",
    "X = np.random.randn(batch_size, input_features)  # Input data\n",
    "W = np.random.randn(input_features, output_features)  # Weights\n",
    "b = np.random.randn(output_features)  # Bias\n",
    "\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "print(f\"Weights W shape: {W.shape}\")\n",
    "print(f\"Bias b shape: {b.shape}\")\n",
    "\n",
    "# Forward pass: Y = XW + b\n",
    "Y = np.dot(X, W) + b  # or X @ W + b\n",
    "print(f\"Output Y shape: {Y.shape}\")\n",
    "\n",
    "print(f\"\\nFirst sample input: {X[0][:5]}...\")  # Show first 5 features\n",
    "print(f\"First sample output: {Y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different matrix operations\n",
    "A = np.random.randn(3, 4)\n",
    "B = np.random.randn(4, 2)\n",
    "\n",
    "print(\"Matrix Operations:\")\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "C = A @ B  # Same as np.dot(A, B)\n",
    "print(f\"A @ B shape: {C.shape}\")\n",
    "\n",
    "# Transpose (very common in ML)\n",
    "A_T = A.T\n",
    "print(f\"A transpose shape: {A_T.shape}\")\n",
    "\n",
    "# Element-wise vs matrix multiplication\n",
    "square_matrix = np.random.randn(3, 3)\n",
    "print(f\"\\nSquare matrix shape: {square_matrix.shape}\")\n",
    "print(f\"Element-wise square: {(square_matrix * square_matrix).shape}\")\n",
    "print(f\"Matrix multiplication: {(square_matrix @ square_matrix).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced linear algebra (useful for understanding ML algorithms)\n",
    "print(\"Advanced Linear Algebra:\")\n",
    "\n",
    "# Create a symmetric matrix (common in optimization)\n",
    "A = np.random.randn(4, 4)\n",
    "symmetric_A = A + A.T\n",
    "\n",
    "print(f\"Matrix A shape: {symmetric_A.shape}\")\n",
    "\n",
    "# Eigenvalues and eigenvectors (PCA, optimization)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(symmetric_A)\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors shape: {eigenvectors.shape}\")\n",
    "\n",
    "# Matrix norms (regularization)\n",
    "print(\"\\nMatrix norms:\")\n",
    "print(f\"Frobenius norm: {np.linalg.norm(A, 'fro'):.4f}\")\n",
    "print(f\"L2 norm: {np.linalg.norm(A, 2):.4f}\")\n",
    "\n",
    "# Determinant and inverse\n",
    "det_A = np.linalg.det(symmetric_A)\n",
    "print(f\"\\nDeterminant: {det_A:.4f}\")\n",
    "\n",
    "if abs(det_A) > 1e-10:  # Check if invertible\n",
    "    inv_A = np.linalg.inv(symmetric_A)\n",
    "    print(f\"Inverse exists, shape: {inv_A.shape}\")\n",
    "    # Verify: A @ A^(-1) should be identity\n",
    "    identity_check = symmetric_A @ inv_A\n",
    "    print(f\"A @ A^(-1) close to identity: {np.allclose(identity_check, np.eye(4))}\")\n",
    "else:\n",
    "    print(\"Matrix is singular (not invertible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Operations and Aggregations\n",
    "\n",
    "Critical for data analysis, loss computation, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical operations along different axes\n",
    "# Simulate prediction scores for classification\n",
    "# Shape: (batch_size, num_classes)\n",
    "predictions = np.random.randn(100, 5)  # 100 samples, 5 classes\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(\"\\nStatistical Operations:\")\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"Overall mean: {np.mean(predictions):.4f}\")\n",
    "print(f\"Overall std: {np.std(predictions):.4f}\")\n",
    "print(f\"Min value: {np.min(predictions):.4f}\")\n",
    "print(f\"Max value: {np.max(predictions):.4f}\")\n",
    "\n",
    "# Statistics along axes\n",
    "print(f\"\\nMean per class (axis=0): {np.mean(predictions, axis=0)}\")\n",
    "print(f\"Mean per sample (axis=1) shape: {np.mean(predictions, axis=1).shape}\")\n",
    "\n",
    "# Useful for softmax and classification\n",
    "max_per_sample = np.max(predictions, axis=1, keepdims=True)\n",
    "print(f\"\\nMax per sample shape (keepdims=True): {max_per_sample.shape}\")\n",
    "print(f\"Max per sample shape (keepdims=False): {np.max(predictions, axis=1).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical ML examples\n",
    "print(\"Practical ML Statistical Operations:\")\n",
    "\n",
    "# 1. Softmax implementation\n",
    "def softmax(x):\n",
    "    \"\"\"Numerically stable softmax\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "logits = np.random.randn(5, 3)  # 5 samples, 3 classes\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"Probabilities sum per sample: {np.sum(probabilities, axis=1)}\")\n",
    "print(f\"First sample probabilities: {probabilities[0]}\")\n",
    "\n",
    "# 2. Accuracy calculation\n",
    "true_labels = np.array([0, 1, 2, 1, 0])\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy = np.mean(true_labels == predicted_labels)\n",
    "print(f\"\\nTrue labels: {true_labels}\")\n",
    "print(f\"Predicted labels: {predicted_labels}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function implementations\n",
    "print(\"Common Loss Functions:\")\n",
    "\n",
    "# Mean Squared Error (regression)\n",
    "y_true = np.array([1.5, 2.3, 3.1, 4.2, 5.0])\n",
    "y_pred = np.array([1.2, 2.1, 3.4, 4.0, 5.2])\n",
    "\n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "print(f\"True values: {y_true}\")\n",
    "print(f\"Predicted values: {y_pred}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# Cross-entropy loss (classification)\n",
    "def cross_entropy_loss(y_true_labels, y_pred_probs):\n",
    "    \"\"\"Cross-entropy loss for classification\"\"\"\n",
    "    # Convert labels to one-hot if needed\n",
    "    n_classes = y_pred_probs.shape[1]\n",
    "    y_true_onehot = np.eye(n_classes)[y_true_labels]\n",
    "\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred_clipped = np.clip(y_pred_probs, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # Calculate cross-entropy\n",
    "    loss = -np.sum(y_true_onehot * np.log(y_pred_clipped)) / len(y_true_labels)\n",
    "    return loss\n",
    "\n",
    "ce_loss = cross_entropy_loss(true_labels, probabilities)\n",
    "print(f\"\\nCross-entropy loss: {ce_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Array Reshaping and Manipulation\n",
    "\n",
    "Essential for preparing data for neural networks and handling different tensor shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping operations (very common in deep learning)\n",
    "print(\"Array Reshaping:\")\n",
    "\n",
    "# Original data: flattened images\n",
    "flattened_images = np.random.randint(0, 256, size=(100, 784))  # 100 images, 28x28 pixels\n",
    "print(f\"Flattened images shape: {flattened_images.shape}\")\n",
    "\n",
    "# Reshape to image format\n",
    "images = flattened_images.reshape(100, 28, 28)\n",
    "print(f\"Reshaped to images: {images.shape}\")\n",
    "\n",
    "# Add channel dimension (for CNN)\n",
    "images_with_channel = images.reshape(100, 28, 28, 1)\n",
    "print(f\"With channel dimension: {images_with_channel.shape}\")\n",
    "\n",
    "# Or using -1 for automatic calculation\n",
    "auto_reshape = flattened_images.reshape(100, 28, 28, -1)\n",
    "print(f\"Auto reshape (-1): {auto_reshape.shape}\")\n",
    "\n",
    "# Flatten back\n",
    "flattened_again = images_with_channel.reshape(100, -1)\n",
    "print(f\"Flattened again: {flattened_again.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axis manipulation\n",
    "print(\"Axis Manipulation:\")\n",
    "\n",
    "# Sample data: batch of sequences\n",
    "sequences = np.random.randn(32, 50, 128)  # batch_size, seq_len, features\n",
    "print(f\"Original shape: {sequences.shape}\")\n",
    "\n",
    "# Transpose (swap axes)\n",
    "transposed = sequences.transpose(1, 0, 2)  # seq_len, batch_size, features\n",
    "print(f\"Transposed: {transposed.shape}\")\n",
    "\n",
    "# Add new axis\n",
    "with_new_axis = sequences[:, :, :, np.newaxis]\n",
    "print(f\"With new axis: {with_new_axis.shape}\")\n",
    "\n",
    "# Squeeze (remove dimensions of size 1)\n",
    "squeezed = np.squeeze(with_new_axis)\n",
    "print(f\"Squeezed: {squeezed.shape}\")\n",
    "\n",
    "# Expand dimensions\n",
    "expanded = np.expand_dims(sequences, axis=0)\n",
    "print(f\"Expanded (axis=0): {expanded.shape}\")\n",
    "\n",
    "expanded_last = np.expand_dims(sequences, axis=-1)\n",
    "print(f\"Expanded (axis=-1): {expanded_last.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation and stacking (combining data)\n",
    "print(\"Concatenation and Stacking:\")\n",
    "\n",
    "# Create sample batches\n",
    "batch1 = np.random.randn(16, 10)  # 16 samples, 10 features\n",
    "batch2 = np.random.randn(16, 10)  # 16 samples, 10 features\n",
    "batch3 = np.random.randn(16, 10)  # 16 samples, 10 features\n",
    "\n",
    "print(f\"Batch 1 shape: {batch1.shape}\")\n",
    "print(f\"Batch 2 shape: {batch2.shape}\")\n",
    "print(f\"Batch 3 shape: {batch3.shape}\")\n",
    "\n",
    "# Concatenate along batch dimension\n",
    "combined_batches = np.concatenate([batch1, batch2, batch3], axis=0)\n",
    "print(f\"Combined batches: {combined_batches.shape}\")\n",
    "\n",
    "# Stack (creates new dimension)\n",
    "stacked_batches = np.stack([batch1, batch2, batch3], axis=0)\n",
    "print(f\"Stacked batches: {stacked_batches.shape}\")\n",
    "\n",
    "# Horizontal stack (features)\n",
    "features1 = np.random.randn(100, 5)\n",
    "features2 = np.random.randn(100, 3)\n",
    "combined_features = np.hstack([features1, features2])\n",
    "print(f\"\\nFeatures 1: {features1.shape}\")\n",
    "print(f\"Features 2: {features2.shape}\")\n",
    "print(f\"Combined features: {combined_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance and Memory Considerations\n",
    "\n",
    "Understanding NumPy performance is crucial for efficient ML workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Vectorization vs loops\n",
    "print(\"Performance Comparison: Vectorization vs Loops\")\n",
    "\n",
    "# Create large arrays\n",
    "size = 1000000\n",
    "a = np.random.randn(size)\n",
    "b = np.random.randn(size)\n",
    "\n",
    "# Method 1: Pure Python loop (slow)\n",
    "start_time = time.time()\n",
    "result_loop = []\n",
    "for i in range(min(10000, size)):  # Only do 10k for speed\n",
    "    result_loop.append(a[i] * b[i])\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "# Method 2: NumPy vectorization (fast)\n",
    "start_time = time.time()\n",
    "result_vectorized = a * b\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop time (10k elements): {loop_time:.6f} seconds\")\n",
    "print(f\"Vectorized time ({size} elements): {vectorized_time:.6f} seconds\")\n",
    "print(f\"Speedup factor: {loop_time / vectorized_time * (size/10000):.1f}x\")\n",
    "\n",
    "# Memory usage\n",
    "print(\"\\nMemory usage:\")\n",
    "print(f\"Array 'a' memory: {a.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Array 'b' memory: {b.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Result memory: {result_vectorized.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory layout and views vs copies\n",
    "print(\"Memory Layout and Views:\")\n",
    "\n",
    "# Original array\n",
    "original = np.random.randn(1000, 1000)\n",
    "print(f\"Original array memory: {original.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# View (shares memory)\n",
    "view = original[::2, ::2]  # Every other element\n",
    "print(f\"View shares memory: {view.base is original}\")\n",
    "print(f\"View shape: {view.shape}\")\n",
    "\n",
    "# Copy (new memory)\n",
    "copy = original.copy()\n",
    "print(f\"Copy shares memory: {copy.base is original}\")\n",
    "print(f\"Copy memory: {copy.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Demonstrate view behavior\n",
    "original[0, 0] = 999\n",
    "print(\"\\nAfter modifying original[0,0] = 999:\")\n",
    "print(f\"View[0,0] = {view[0, 0]} (should be 999 if it's a view)\")\n",
    "print(f\"Copy[0,0] = {copy[0, 0]} (should be original value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Connection to PyTorch and TensorFlow\n",
    "\n",
    "Understanding how NumPy concepts translate to tensor operations in both frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate NumPy as the bridge between frameworks\n",
    "print(\"NumPy as the Bridge Between Frameworks:\")\n",
    "\n",
    "# Create sample data in NumPy\n",
    "numpy_data = np.random.randn(32, 10).astype(np.float32)\n",
    "numpy_labels = np.random.randint(0, 3, size=(32,))\n",
    "\n",
    "print(f\"NumPy data shape: {numpy_data.shape}\")\n",
    "print(f\"NumPy data type: {numpy_data.dtype}\")\n",
    "print(f\"NumPy labels shape: {numpy_labels.shape}\")\n",
    "\n",
    "# Show how this would convert to PyTorch (conceptually)\n",
    "print(\"\\nConversion to PyTorch (conceptual):\")\n",
    "print(\"torch_data = torch.from_numpy(numpy_data)\")\n",
    "print(\"torch_labels = torch.from_numpy(numpy_labels)\")\n",
    "\n",
    "# Show how this would convert to TensorFlow (conceptually)\n",
    "print(\"\\nConversion to TensorFlow (conceptual):\")\n",
    "print(\"tf_data = tf.constant(numpy_data)\")\n",
    "print(\"tf_labels = tf.constant(numpy_labels)\")\n",
    "\n",
    "# Common operations that work similarly\n",
    "print(\"\\nCommon operations (NumPy syntax):\")\n",
    "print(f\"Mean: {np.mean(numpy_data, axis=1).shape}\")\n",
    "print(f\"Max: {np.max(numpy_data, axis=1).shape}\")\n",
    "print(f\"Reshape: {numpy_data.reshape(32, 2, 5).shape}\")\n",
    "print(f\"Transpose: {numpy_data.T.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Data preprocessing pipeline\n",
    "print(\"Practical Data Preprocessing Pipeline:\")\n",
    "\n",
    "# Simulate loading data (this is where NumPy shines)\n",
    "raw_data = np.random.randn(1000, 20) * 5 + 10  # Some realistic data\n",
    "raw_labels = np.random.randint(0, 5, size=(1000,))\n",
    "\n",
    "print(f\"Raw data shape: {raw_data.shape}\")\n",
    "print(f\"Raw data range: [{np.min(raw_data):.2f}, {np.max(raw_data):.2f}]\")\n",
    "\n",
    "# Step 1: Normalize features\n",
    "mean = np.mean(raw_data, axis=0)\n",
    "std = np.std(raw_data, axis=0)\n",
    "normalized_data = (raw_data - mean) / std\n",
    "\n",
    "print(\"\\nAfter normalization:\")\n",
    "print(f\"Mean: {np.mean(normalized_data, axis=0)[:5]}...\")  # Should be ~0\n",
    "print(f\"Std: {np.std(normalized_data, axis=0)[:5]}...\")    # Should be ~1\n",
    "\n",
    "# Step 2: Train/validation split\n",
    "n_train = int(0.8 * len(normalized_data))\n",
    "indices = np.random.permutation(len(normalized_data))\n",
    "\n",
    "train_data = normalized_data[indices[:n_train]]\n",
    "val_data = normalized_data[indices[n_train:]]\n",
    "train_labels = raw_labels[indices[:n_train]]\n",
    "val_labels = raw_labels[indices[n_train:]]\n",
    "\n",
    "print(f\"\\nTrain set: {train_data.shape}\")\n",
    "print(f\"Validation set: {val_data.shape}\")\n",
    "\n",
    "# Step 3: Convert to appropriate format for frameworks\n",
    "print(\"\\nData ready for framework conversion:\")\n",
    "print(f\"Data type: {train_data.dtype}\")\n",
    "print(f\"Labels type: {train_labels.dtype}\")\n",
    "print(f\"No NaN values: {not np.any(np.isnan(train_data))}\")\n",
    "print(f\"Finite values: {np.all(np.isfinite(train_data))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "**What we've learned:**\n",
    "\n",
    "1. **Array Creation & Properties**: Understanding shapes, dtypes, and memory usage\n",
    "2. **Indexing & Slicing**: Essential for data manipulation and batch processing\n",
    "3. **Broadcasting**: Enables efficient operations without explicit loops\n",
    "4. **Linear Algebra**: Matrix operations that form the core of neural networks\n",
    "5. **Statistical Operations**: Computing metrics, losses, and aggregations\n",
    "6. **Reshaping**: Preparing data for different network architectures\n",
    "7. **Performance**: Vectorization and memory considerations\n",
    "8. **Framework Bridge**: How NumPy connects to PyTorch and TensorFlow\n",
    "\n",
    "**Key Patterns for ML:**\n",
    "- Use vectorized operations instead of loops\n",
    "- Understand broadcasting for efficient computations\n",
    "- Master axis-based operations for batch processing\n",
    "- Know when operations create views vs copies\n",
    "- Prepare data in NumPy before converting to framework tensors\n",
    "\n",
    "**Next Steps:**\n",
    "- Learn Pandas for structured data manipulation\n",
    "- Understand how these concepts translate to PyTorch tensors\n",
    "- See how TensorFlow operations mirror NumPy patterns\n",
    "\n",
    "NumPy is the foundation that makes both PyTorch and TensorFlow possible. Mastering these concepts will make learning either framework much easier!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
